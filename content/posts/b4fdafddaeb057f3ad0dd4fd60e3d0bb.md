---
title: Let's build the GPT Tokenizer
date: 2024-02-21
src_link: https://www.notion.so/Let-s-build-the-GPT-Tokenizer-YouTube-Andrej-Karpathy-7231e924f2d848258d93b1cd8a8b7780
src_date: '2024-02-21 13:37:00'
gold_link: https://www.youtube.com/watch?v=zduSFxRajkE
gold_link_hash: b4fdafddaeb057f3ad0dd4fd60e3d0bb
tags:
- '#host_www_youtube_com'
---

![](https://www.youtube.com/watch?v=zduSFxRajkE) 
# Description 
The Tokenizer is a necessary and pervasive component of Large Language Models (LLMs), where it translates between strings and tokens (text chunks). Tokenizers are a completely separate stage of the LLM pipeline: they have their own training sets, training algorithms (Byte Pair Encoding), and after training implement two fundamental functions: encode() from strings to tokens, and decode() back from tokens to strings. In this lecture we build from scratch the Tokenizer used in the GPT series from OpenAI. In the process, we will see that a lot of weird behaviors and problems of LLMs actually trace back to tokenization. We'll go through a number of these issues, discuss why tokenization is at fault, and why someone out there ideally finds a way to delete this stage entirely.

Chapters:
00:00:00 intro: Tokenization, GPT-2 paper, tokenization-related issues
00:05:50 tokenization by example in a Web UI (tiktokenizer)
00:14:56 strings in Python, Unicode code points
00:18:15 Unicode byte encodings, ASCII, UTF-8, UTF-16, UTF-32
00:22:47 daydreaming: deleting tokenization
00:23:50 Byte Pair Encoding (BPE) algorithm walkthrough
00:27:02 starting the implementation
00:28:35 counting consecutive pairs, finding most common pair
00:30:36 merging the most common pair
00:34:58 training the tokenizer: adding the while loop, compression ratio
00:39:20 tokenizer/LLM diagram: it is a completely separate stage
00:42:47 decoding tokens to strings
00:48:21 encoding strings to tokens
00:57:36 regex patterns to force splits across categories
01:11:38 tiktoken library intro, differences between GPT-2/GPT-4 regex
01:14:59 GPT-2 encoder.py released by OpenAI walkthrough
01:18:26 special tokens, tiktoken handling of, GPT-2/GPT-4 differences
01:25:28 minbpe exercise time! write your own GPT-4 tokenizer
01:28:42 sentencepiece library intro, used to train Llama 2 vocabulary
01:43:27 how to set vocabulary set? revisiting gpt.py transformer
01:48:11 training new tokens, example of prompt compression
01:49:58 multimodal [image, video, audio] tokenization with vector quantization
01:51:41 revisiting and explaining the quirks of LLM tokenization
02:10:20 final recommendations
02:12:50 ??? :)

Exercises:
- Advised flow: reference this document and try to implement the steps before I give away the partial solutions in the video. The full solutions if you're getting stuck are in the minbpe code https://github.com/karpathy/minbpe/blob/master/exercise.md

Links:
- Google colab for the video: https://colab.research.google.com/drive/1y0KnCFZvGVf_odSfcNAws6kcDD7HsI0L?usp=sharing
- GitHub repo for the video: minBPE https://github.com/karpathy/minbpe
- Playlist of the whole Zero to Hero series so far: https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ
- our Discord channel: https://discord.gg/3zy8kqD9Cp
- my Twitter: https://twitter.com/karpathy

Supplementary links:
- tiktokenizer https://tiktokenizer.vercel.app
- tiktoken from OpenAI: https://github.com/openai/tiktoken
- sentencepiece from Google https://github.com/google/sentencepiece
# en
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=0)~~  hi everyone so in this video I'd like us to cover the process of tokenization in large language models now you see here that I have a set face and that's because uh tokenization is my least favorite part of working with large language models but unfortunately it is necessary to understand in some detail because it it is fairly hairy gnarly and there's a lot of hidden foot guns to be aware of and a lot of oddness with large language models typically traces back to tokenization so what is tokenization now in my previous video Let's Build GPT from scratch uh we actually already did tokenization but we did a very naive simple version of tokenization so when you go to the Google colab for that video uh you see here that we loaded our training set and our training set was this uh Shakespeare uh data set now in the beginning the Shakespeare data set is just a large string in Python it's just text and so the question is how do we plug text into large language models and in this case here we created a vocabulary of 65 possible characters that we saw occur in 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=64)~~  this string these were the possible characters and we saw that there are 65 of them and then we created a a lookup table for converting from every possible character a little string piece into a token an integer so here for example we tokenized the string High there and we received this sequence of tokens and here we took the first 1,000 characters of our data set and we encoded it into tokens and because it is this is character level we received 1,000 tokens in a sequence so token 18 47 Etc now later we saw that the way we plug these tokens into the language model is by using an embedding table and so basically if we have 65 possible tokens then this embedding table is going to have 65 rows and roughly speaking we're taking the integer associated with every single sing Le token we're using that as a lookup into this table and we're plucking out the corresponding row and this row is a uh is trainable parameters that we're going to train using back propagation and this is the vector that then feeds into the Transformer um and 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=135)~~  that's how the Transformer Ser of perceives every single token so here we had a very naive tokenization process that was a character level tokenizer but in practice in state-ofthe-art uh language models people use a lot more complicated schemes unfortunately uh for constructing these uh token vocabularies so we're not dealing on the Character level we're dealing on chunk level and the way these um character chunks are constructed is using algorithms such as for example the bik pair in coding algorithm which we're going to go into in detail um and cover in this video I'd like to briefly show you the paper that introduced a bite level encoding as a mechanism for tokenization in the context of large language models and I would say that that's probably the gpt2 paper and if you scroll down here to the section input representation this is where they cover tokenization the kinds of properties that you'd like the tokenization to have and they conclude here that they're going to have a tokenizer where you have a vocabulary of 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=198)~~  50,2 57 possible tokens and the context size is going to be 1,24 tokens so in the in in the attention layer of the Transformer neural network every single token is attending to the previous tokens in the sequence and it's going to see up to 1,24 tokens so tokens are this like fundamental unit um the atom of uh large language models if you will and everything is in units of tokens everything is about tokens and tokenization is the process for translating strings or text into sequences of tokens and uh vice versa when you go into the Llama 2 paper as well I can show you that when you search token you're going to get get 63 hits um and that's because tokens are again pervasive so here they mentioned that they trained on two trillion tokens of data and so on so we're going to build our own tokenizer luckily the bite be encoding algorithm is not uh that super complicated and we can build it from scratch ourselves and we'll see exactly how this works before we dive into code I'd like to give you a brief Taste of some of the complexities that come from 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=264)~~  the tokenization because I just want to make sure that we motivate it sufficiently for why we are doing all this and why this is so gross so tokenization is at the heart of a lot of weirdness in large language models and I would advise that you do not brush it off a lot of the issues that may look like just issues with the new network architecture or the large language model itself are actually issues with the tokenization and fundamentally Trace uh back to it so if you've noticed any issues with large language models can't you know not able to do spelling tasks very easily that's usually due to tokenization simple string processing can be difficult for the large language model to perform natively uh non-english languages can work much worse and to a large extent this is due to tokenization sometimes llms are bad at simple arithmetic also can trace be traced to tokenization uh gbt2 specifically would have had quite a bit more issues with python than uh future versions of it due to tokenization there's a lot of other 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=324)~~  issues maybe you've seen weird warnings about a trailing whites space this is a tokenization issue um if you had asked GPT earlier about solid gold Magikarp and what it is you would see the llm go totally crazy and it would start going off about a completely unrelated tangent topic maybe you've been told to use yl over Json in structure data all of that has to do with tokenization so basically tokenization is at the heart of many issues I will look back around to these at the end of the video but for now let me just um skip over it a little bit and let's go to this web app um the Tik tokenizer bell.app so I have it loaded here and what I like about this web app is that tokenization is running a sort of live in your browser in JavaScript so you can just type here stuff hello world and the whole string rokenes so here what we see on uh the left is a string that you put in on the right we're currently using the gpt2 tokenizer we see that this string that I pasted here is currently tokenizing into 300 tokens and here they are sort of uh 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=391)~~  shown explicitly in different colors for every single token so for example uh this word tokenization became two tokens the token 1,634 the token um space is is token 318 so be careful on the bottom you can show white space and keep in mind that there are spaces and uh sln new line characters in here but you can hide them for clarity the token space at is token 379 the to the Token space the is 262 Etc so you notice here that the space is part of that uh token chunk now so this is kind of like how our English sentence broke up and that seems all well and good now now here I put in some arithmetic so we see that uh the token 127 Plus and then token six space 6 followed by 77 so what's happening here is that 127 is feeding in as a single token into the large language model but the um number 677 will actually feed in as two separate tokens and so the large language model has to sort of um take account of that and process it correctly in its Network and see here 804 will be broken up into two tokens and it's is all completely 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=478)~~  arbitrary and here I have another example of four-digit numbers and they break up in a way that they break up and it's totally arbitrary sometimes you have um multiple digits single token sometimes you have individual digits as many tokens and it's all kind of pretty arbitrary and coming out of the tokenizer here's another example we have the string egg and you see here that this became two tokens but for some reason when I say I have an egg you see when it's a space egg it's two token it's sorry it's a single token so just egg by itself in the beginning of a sentence is two tokens but here as a space egg is suddenly a single token uh for the exact same string okay here lowercase egg turns out to be a single token and in particular notice that the color is different so this is a different token so this is case sensitive and of course a capital egg would also be different tokens and again um this would be two tokens arbitrarily so so for the same concept egg depending on if it's in the beginning of a sentence at the end of a 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=544)~~  sentence lowercase uppercase or mixed all this will be uh basically very different tokens and different IDs and the language model has to learn from raw data from all the internet text that it's going to be training on that these are actually all the exact same concept and it has to sort of group them in the parameters of the neural network and understand just based on the data patterns that these are all very similar but maybe not almost exactly similar but but very very similar um after the EG demonstration here I have um an introduction from open a eyes chbt in Korean so manaso Pang uh Etc uh so this is in Korean and the reason I put this here is because you'll notice that um non-english languages work slightly worse in Chachi part of this is because of course the training data set for Chachi is much larger for English and for everything else but the same is true not just for the large language model itself but also for the tokenizer so when we train the tokenizer we're going to see that there's a training set 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=607)~~  as well and there's a lot more English than non-english and what ends up happening is that we're going to have a lot more longer tokens for English so how do I put this if you have a single sentence in English and you tokenize it you might see that it's 10 tokens or something like that but if you translate that sentence into say Korean or Japanese or something else you'll typically see that the number of tokens used is much larger and that's because the chunks here are a lot more broken up so we're using a lot more tokens for the exact same thing and what this does is it bloats up the sequence length of all the documents so you're using up more tokens and then in the attention of the Transformer when these tokens try to attend each other you are running out of context um in the maximum context length of that Transformer and so basically all the non-english text is stretched out from the perspective of the Transformer and this just has to do with the um trainings that used for the tokenizer and the tokenization itself so it will 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=670)~~  create a lot bigger tokens and a lot larger groups in English and it will have a lot of little boundaries for all the other non-english text um so if we translated this into English it would be significantly fewer tokens the final example I have here is a little snippet of python for doing FS buuz and what I'd like you to notice is look all these individual spaces are all separate tokens they are token 220 so uh 220 220 220 220 and then space if is a single token and so what's going on here is that when the Transformer is going to consume or try to uh create this text it needs to um handle all these spaces individually they all feed in one by one into the entire Transformer in the sequence and so this is being extremely wasteful tokenizing it in this way and so as a result of that gpt2 is not very good with python and it's not anything to do with coding or the language model itself it's just that if he use a lot of indentation using space in Python like we usually do uh you just end up bloating out all the text and it's separated across way too 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=739)~~  much of the sequence and we are running out of the context length in the sequence uh that's roughly speaking what's what's happening we're being way too wasteful we're taking up way too much token space now we can also scroll up here and we can change the tokenizer so note here that gpt2 tokenizer creates a token count of 300 for this string here we can change it to CL 100K base which is the GPT for tokenizer and we see that the token count drops to 185 so for the exact same string we are now roughly having the number of tokens and roughly speaking this is because uh the number of tokens in the GPT 4 tokenizer is roughly double that of the number of tokens in the gpt2 tokenizer so we went went from roughly 50k to roughly 100K now you can imagine that this is a good thing because the same text is now squished into half as many tokens so uh this is a lot denser input to the Transformer and in the Transformer every single token has a finite number of tokens before it that it's going to pay attention to and so what this is doing 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=800)~~  is we're roughly able to see twice as much text as a context for what token to predict next uh because of this change but of course just increasing the number of tokens is uh not strictly better infinitely uh because as you increase the number of tokens now your embedding table is um sort of getting a lot larger and also at the output we are trying to predict the next token and there's the soft Max there and that grows as well we're going to go into more detail later on this but there's some kind of a Sweet Spot somewhere where you have a just right number of tokens in your vocabulary where everything is appropriately dense and still fairly efficient now one thing I would like you to note specifically for the gp4 tokenizer is that the handling of the white space for python has improved a lot you see that here these four spaces are represented as one single token for the three spaces here and then the token SPF and here seven spaces were all grouped into a single token so we're being a lot more efficient in how we 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=860)~~  represent Python and this was a deliberate Choice made by open aai when they designed the gp4 tokenizer and they group a lot more space into a single character what this does is this densifies Python and therefore we can attend to more code before it when we're trying to predict the next token in the sequence and so the Improvement in the python coding ability from gbt2 to gp4 is not just a matter of the language model and the architecture and the details of the optimization but a lot of the Improvement here is also coming from the design of the tokenizer and how it groups characters into tokens okay so let's now start writing some code so remember what we want to do we want to take strings and feed them into language models for that we need to somehow tokenize strings into some integers in some fixed vocabulary and then we will use those integers to make a look up into a lookup table of vectors and feed those vectors into the Transformer as an input now the reason this gets a little bit tricky of course is that we don't just want to support 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=924)~~  the simple English alphabet we want to support different kinds of languages so this is anango in Korean which is hello and we also want to support many kinds of special characters that we might find on the internet for example Emoji so how do we feed this text into uh Transformers well how's the what is this text anyway in Python so if you go to the documentation of a string in Python you can see that strings are immutable sequences of Unicode code points okay what are Unicode code points we can go to PDF so Unicode code points are defined by the Unicode Consortium as part of the Unicode standard and what this is really is that it's just a definition of roughly 150,000 characters right now and roughly speaking what they look like and what integers um represent those characters so it says 150,000 characters across 161 scripts as of right now so if you scroll down here you can see that the standard is very much alive the latest standard 15.1 in September 2023 and basically this is just a way to define lots of types of 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=997)~~  characters like for example all these characters across different scripts so the way we can access the unic code code Point given Single Character is by using the or function in Python so for example I can pass in Ord of H and I can see that for the Single Character H the unic code code point is 104 okay um but this can be arbitr complicated so we can take for example our Emoji here and we can see that the code point for this one is 128,000 or we can take un and this is 50,000 now keep in mind you can't plug in strings here because you uh this doesn't have a single code point it only takes a single uni code code Point character and tells you its integer so in this way we can look up all the um characters of this specific string and their code points so or of X forx in this string and we get this encoding here now see here we've already turned the raw code points already have integers so why can't we simply just use these integers and not have any tokenization at all why can't we just use this natively as is and just 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1071)~~  use the code Point well one reason for that of course is that the vocabulary in that case would be quite long so in this case for Unicode the this is a vocabulary of 150,000 different code points but more worryingly than that I think the Unicode standard is very much alive and it keeps changing and so it's not kind of a stable representation necessarily that we may want to use directly so for those reasons we need something a bit better so to find something better we turn to encodings so if we go to the Wikipedia page here we see that the Unicode consortion defines three types of encodings utf8 UTF 16 and UTF 32 these encoding are the way by which we can take Unicode text and translate it into binary data or by streams utf8 is by far the most common uh so this is the utf8 page now this Wikipedia page is actually quite long but what's important for our purposes is that utf8 takes every single Cod point and it translates it to a by stream and this by stream is between one to four bytes so it's a variable length encoding so depending on the Unicode 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1136)~~  Point according to the schema you're going to end up with between 1 to four bytes for each code point on top of that there's utf8 uh utf16 and UTF 32 UTF 32 is nice because it is fixed length instead of variable length but it has many other downsides as well so the full kind of spectrum of pros and cons of all these different three encodings are beyond the scope of this video I just like to point out that I enjoyed this block post and this block post at the end of it also has a number of references that can be quite useful uh one of them is uh utf8 everywhere Manifesto um and this Manifesto describes the reason why utf8 is significantly preferred and a lot nicer than the other encodings and why it is used a lot more prominently um on the internet one of the major advantages just just to give you a sense is that utf8 is the only one of these that is backwards compatible to the much simpler asky encoding of text um but I'm not going to go into the full detail in this video so suffice to say that we like the utf8 encoding and uh let's try to take 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1204)~~  the string and see what we get if we encoded into utf8 the string class in Python actually has do encode and you can give it the encoding which is say utf8 now we get out of this is not very nice because this is the bytes is a bytes object and it's not very nice in the way that it's printed so I personally like to take it through list because then we actually get the raw B of this uh encoding so this is the raw byes that represent this string according to the utf8 en coding we can also look at utf16 we get a slightly different by stream and we here we start to see one of the disadvantages of utf16 you see how we have zero Z something Z something Z something we're starting to get a sense that this is a bit of a wasteful encoding and indeed for simple asky characters or English characters here uh we just have the structure of 0 something Z something and it's not exactly nice same for UTF 32 when we expand this we can start to get a sense of the wastefulness of this encoding for our purposes you see a lot of zeros followed by 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1271)~~  something and so uh this is not desirable so suffice it to say that we would like to stick with utf8 for our purposes however if we just use utf8 naively these are by streams so that would imply a vocabulary length of only 256 possible tokens uh but this this vocabulary size is very very small what this is going to do if we just were to use it naively is that all of our text would be stretched out over very very long sequences of bytes and so um what what this does is that certainly the embeding table is going to be tiny and the prediction at the top at the final layer is going to be very tiny but our sequences are very long and remember that we have pretty finite um context length and the attention that we can support in a transformer for computational reasons and so we only have as much context length but now we have very very long sequences and this is just inefficient and it's not going to allow us to attend to sufficiently long text uh before us for the purposes of the next token prediction task so we don't want to use the raw bytes of the 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1342)~~  utf8 encoding we want to be able to support larger vocabulary size that we can tune as a hyper but we want to stick with the utf8 encoding of these strings so what do we do well the answer of course is we turn to the bite pair encoding algorithm which will allow us to compress these bite sequences um to a variable amount so we'll get to that in a bit but I just want to briefly speak to the fact that I would love nothing more than to be able to feed raw bite sequences into uh language models in fact there's a paper about how this could potentially be done uh from Summer last last year now the problem is you actually have to go in and you have to modify the Transformer architecture because as I mentioned you're going to have a problem where the attention will start to become extremely expensive because the sequences are so long and so in this paper they propose kind of a hierarchical structuring of the Transformer that could allow you to just feed in raw bites and so at the end they say together these results establish the viability of tokenization 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1404)~~  free autor regressive sequence modeling at scale so tokenization free would indeed be amazing we would just feed B streams directly into our models but unfortunately I don't know that this has really been proven out yet by sufficiently many groups and a sufficient scale uh but something like this at one point would be amazing and I hope someone comes up with it but for now we have to come back and we can't feed this directly into language models and we have to compress it using the B paare encoding algorithm so let's see how that works so as I mentioned the B paare encoding algorithm is not all that complicated and the Wikipedia page is actually quite instructive as far as the basic idea goes go what we're doing is we have some kind of a input sequence uh like for example here we have only four elements in our vocabulary a b c and d and we have a sequence of them so instead of bytes let's say we just have four a vocab size of four the sequence is too long and we'd like to compress it so what we do is that we iteratively find the pair of uh 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1460)~~  tokens that occur the most frequently and then once we've identified that pair we repl replace that pair with just a single new token that we append to our vocabulary so for example here the bite pair AA occurs most often so we mint a new token let's call it capital Z and we replace every single occurrence of AA by Z so now we have two Z's here so here we took a sequence of 11 characters with vocabulary size four and we've converted it to a um sequence of only nine tokens but now with a vocabulary of five because we have a fifth vocabulary element that we just created and it's Z standing for concatination of AA and we can again repeat this process so we again look at the sequence and identify the pair of tokens that are most frequent let's say that that is now AB well we are going to replace AB with a new token that we meant call Y so y becomes ab and then every single occurrence of ab is now replaced with y so we end up with this so now we only have 1 2 3 4 5 6 seven characters in our sequence but we have not just um four 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1540)~~  vocabulary elements or five but now we have six and for the final round we again look through the sequence find that the phrase zy or the pair zy is most common and replace it one more time with another um character let's say x so X is z y and we replace all curses of zy and we get this following sequence so basically after we have gone through this process instead of having a um sequence of 11 uh tokens with a vocabulary length of four we now have a sequence of 1 2 3 four five tokens but our vocabulary length now is seven and so in this way we can iteratively compress our sequence I we Mint new tokens so in the in the exact same way we start we start out with bite sequences so we have 256 vocabulary size but we're now going to go through these and find the bite pairs that occur the most and we're going to iteratively start minting new tokens appending them to our vocabulary and replacing things and in this way we're going to end up with a compressed training data set and also an algorithm for taking any arbitrary sequence and 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1615)~~  encoding it using this uh vocabul and also decoding it back to Strings so let's now Implement all that so here's what I did I went to this block post that I enjoyed and I took the first paragraph and I copy pasted it here into text so this is one very long line here now to get the tokens as I mentioned we just take our text and we encode it into utf8 the tokens here at this point will be a raw bites single stream of bytes and just so that it's easier to work with instead of just a bytes object I'm going to convert all those bytes to integers and then create a list of it just so it's easier for us to manipulate and work with in Python and visualize and here I'm printing all of that so this is the original um this is the original paragraph and its length is 533 uh code points and then here are the bytes encoded in ut utf8 and we see that this has a length of 616 bytes at this point or 616 tokens and the reason this is more is because a lot of these simple asky characters or simple characters they just become a single bite but a lot 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1686)~~  of these Unicode more complex characters become multiple bytes up to four and so we are expanding that size so now what we'd like to do as a first step of the algorithm is we'd like to iterate over here and find the pair of bites that occur most frequently because we're then going to merge it so if you are working long on a notebook on a side then I encourage you to basically click on the link find this notebook and try to write that function yourself otherwise I'm going to come here and Implement first the function that finds the most common pair okay so here's what I came up with there are many different ways to implement this but I'm calling the function get stats it expects a list of integers I'm using a dictionary to keep track of basically the counts and then this is a pythonic way to iterate consecutive elements of this list uh which we covered in the previous video and then here I'm just keeping track of just incrementing by one um for all the pairs so if I call this on all the tokens here then the stats comes out 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1743)~~  here so this is the dictionary the keys are these topples of consecutive elements and this is the count so just to uh print it in a slightly better way this is one way that I like to do that where you it's a little bit compound here so you can pause if you like but we iterate all all the items the items called on dictionary returns pairs of key value and instead I create a list here of value key because if it's a value key list then I can call sort on it and by default python will uh use the first element which in this case will be value to sort by if it's given tles and then reverse so it's descending and print that so basically it looks like 101 comma 32 was the most commonly occurring consecutive pair and it occurred 20 times we can double check that that makes reasonable sense so if I just search 10132 then you see that these are the 20 occurrences of that um pair and if we'd like to take a look at what exactly that pair is we can use Char which is the opposite of or in Python so we give it a um unic code Cod point so 101 and of 32 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1822)~~  and we see that this is e and space so basically there's a lot of E space here meaning that a lot of these words seem to end with e so here's eace as an example so there's a lot of that going on here and this is the most common pair so now that we've identified the most common pair we would like to iterate over this sequence we're going to Mint a new token with the ID of 256 right because these tokens currently go from Z to 255 so when we create a new token it will have an ID of 256 and we're going to iterate over this entire um list and every every time we see 101 comma 32 we're going to swap that out for 256 so let's Implement that now and feel free to uh do that yourself as well so first I commented uh this just so we don't pollute uh the notebook too much this is a nice way of in Python obtaining the highest ranking pair so we're basically calling the Max on this dictionary stats and this will return the maximum key and then the question is how does it rank keys so you can provide it with a function that ranks keys and that 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1895)~~  function is just stats. getet uh stats. getet would basically return the value and so we're ranking by the value and getting the maximum key so it's 101 comma 32 as we saw now to actually merge 10132 um this is the function that I wrote but again there are many different versions of it so we're going to take a list of IDs and the the pair that we want to replace and that pair will be replaced with the new index idx so iterating through IDs if we find the pair swap it out for idx so we create this new list and then we start at zero and then we go through this entire list sequentially from left to right and here we are checking for equality at the current position with the pair um so here we are checking that the pair matches now here is a bit of a tricky condition that you have to append if you're trying to be careful and that is that um you don't want this here to be out of Bounds at the very last position when you're on the rightmost element of this list otherwise this would uh give you an autof bounds error so we have to make sure that we're not 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1961)~~  at the very very last element so uh this would be false for that so if we find a match we append to this new list that replacement index and we increment the position by two so we skip over that entire pair but otherwise if we we haven't found a matching pair we just sort of copy over the um element at that position and increment by one then return this so here's a very small toy example if we have a list 566 791 and we want to replace the occurrences of 67 with 99 then calling this on that will give us what we're asking for so here the 67 is replaced with 99 so now I'm going to uncomment this for our actual use case where we want to take our tokens we want to take the top pair here and replace it with 256 to get tokens to if we run this we get the following so recall that previously we had a length 616 in this list and now we have a length 596 right so this decreased by 20 which makes sense because there are 20 occurrences moreover we can try to find 256 here and we see plenty of occurrences on off it and moreover just double check there 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2040)~~  should be no occurrence of 10132 so this is the original array plenty of them and in the second array there are no occurrences of 1032 so we've successfully merged this single pair and now we just uh iterate this so we are going to go over the sequence again find the most common pair and replace it so let me now write a y Loop that uses these functions to do this um sort of iteratively and how many times do we do it four well that's totally up to us as a hyper parameter the more um steps we take the larger will be our vocabulary and the shorter will be our sequence and there is some sweet spot that we usually find works the best in practice and so this is kind of a hyperparameter and we tune it and we find good vocabulary sizes as an example gp4 currently uses roughly 100,000 tokens and um bpark that those are reasonable numbers currently instead the are large language models so let me now write uh putting putting it all together and uh iterating these steps okay now before we dive into the Y loop I wanted to add one more cell here where 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2103)~~  I went to the block post and instead of grabbing just the first paragraph or two I took the entire block post and I stretched it out in a single line and basically just using longer text will allow us to have more representative statistics for the bite Pairs and we'll just get a more sensible results out of it because it's longer text um so here we have the raw text we encode it into bytes using the utf8 encoding and then here as before we are just changing it into a list of integers in Python just so it's easier to work with instead of the raw byes objects and then this is the code that I came up with uh to actually do the merging in Loop these two functions here are identical to what we had above I only included them here just so that you have the point of reference here so uh these two are identical and then this is the new code that I added so the first first thing we want to do is we want to decide on the final vocabulary size that we want our tokenizer to have and as I mentioned this is a hyper parameter and you set it 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2165)~~  in some way depending on your best performance so let's say for us we're going to use 276 because that way we're going to be doing exactly 20 merges and uh 20 merges because we already have 256 tokens for the raw bytes and to reach 276 we have to do 20 merges uh to add 20 new tokens here uh this is uh one way in Python to just create a copy of a list so I'm taking the tokens list and by wrapping it in a list python will construct a new list of all the individual elements so this is just a copy operation then here I'm creating a merges uh dictionary so this merges dictionary is going to maintain basically the child one child two mapping to a new uh token and so what we're going to be building up here is a binary tree of merges but actually it's not exactly a tree because a tree would have a single root node with a bunch of leaves for us we're starting with the leaves on the bottom which are the individual bites those are the starting 256 tokens and then we're starting to like merge two of them at a time and so it's not a tree it's more like a forest 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2235)~~  um uh as we merge these elements so for 20 merges we're going to find the most commonly occurring pair we're going to Mint a new token integer for it so I here will start at zero so we'll going to start at 256 we're going to print that we're merging it and we're going to replace all of the occurrences of that pair with the new new lied token and we're going to record that this pair of integers merged into this new integer so running this gives us the following output so we did 20 merges and for example the first merge was exactly as before the 10132 um tokens merging into a new token 2556 now keep in mind that the individual uh tokens 101 and 32 can still occur in the sequence after merging it's only when they occur exactly consecutively that that becomes 256 now um and in particular the other thing to notice here is that the token 256 which is the newly minted token is also eligible for merging so here on the bottom the 20th merge was a merge of 25 and 259 becoming 275 so every time we replace these tokens they become eligible for merging 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2314)~~  in the next round of data ration so that's why we're building up a small sort of binary Forest instead of a single individual tree one thing we can take a look at as well is we can take a look at the compression ratio that we've achieved so in particular we started off with this tokens list um so we started off with 24,000 bytes and after merging 20 times uh we now have only 19,000 um tokens and so therefore the compression ratio simply just dividing the two is roughly 1.27 so that's the amount of compression we were able to achieve of this text with only 20 merges um and of course the more vocabulary elements you add uh the greater the compression ratio here would be finally so that's kind of like um the training of the tokenizer if you will now 1 Point I wanted to make is that and maybe this is a diagram that can help um kind of illustrate is that tokenizer is a completely separate object from the large language model itself so everything in this lecture we're not really touching the llm itself uh we're just training the tokenizer this is a 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2382)~~  completely separate pre-processing stage usually so the tokenizer will have its own training set just like a large language model has a potentially different training set so the tokenizer has a training set of documents on which you're going to train the tokenizer and then and um we're performing The Bite pair encoding algorithm as we saw above to train the vocabulary of this tokenizer so it has its own training set it is a pre-processing stage that you would run a single time in the beginning um and the tokenizer is trained using bipar coding algorithm once you have the tokenizer once it's trained and you have the vocabulary and you have the merges uh we can do both encoding and decoding so these two arrows here so the tokenizer is a translation layer between raw text which is as we saw the sequence of Unicode code points it can take raw text and turn it into a token sequence and vice versa it can take a token sequence and translate it back into raw text so now that we have trained uh tokenizer and we have these merges we 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2446)~~  are going to turn to how we can do the encoding and the decoding step if you give me text here are the tokens and vice versa if you give me tokens here's the text once we have that we can translate between these two Realms and then the language model is going to be trained as a step two afterwards and typically in a in a sort of a state-of-the-art application you might take all of your training data for the language model and you might run it through the tokenizer and sort of translate everything into a massive token sequence and then you can throw away the raw text you're just left with the tokens themselves and those are stored on disk and that is what the large language model is actually reading when it's training on them so this one approach that you can take as a single massive pre-processing step a stage um so yeah basically I think the most important thing I want to get across is that this is completely separate stage it usually has its own entire uh training set you may want to have those training sets be different 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2498)~~  between the tokenizer and the logge language model so for example when you're training the tokenizer as I mentioned we don't just care about the performance of English text we care about uh multi many different languages and we also care about code or not code so you may want to look into different kinds of mixtures of different kinds of languages and different amounts of code and things like that because the amount of different language that you have in your tokenizer training set will determine how many merges of it there will be and therefore that determines the density with which uh this type of data is um sort of has in the token space and so roughly speaking intuitively if you add some amount of data like say you have a ton of Japanese data in your uh tokenizer training set then that means that more Japanese tokens will get merged and therefore Japanese will have shorter sequences uh and that's going to be beneficial for the large language model which has a finite context length on which it can work on in in the token 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2557)~~  space uh so hopefully that makes sense so we're now going to turn to encoding and decoding now that we have trained a tokenizer so we have our merges and now how do we do encoding and decoding okay so let's begin with decoding which is this Arrow over here so given a token sequence let's go through the tokenizer to get back a python string object so the raw text so this is the function that we' like to implement um we're given the list of integers and we want to return a python string if you'd like uh try to implement this function yourself it's a fun exercise otherwise I'm going to start uh pasting in my own solution so there are many different ways to do it um here's one way I will create an uh kind of pre-processing variable that I will call vocab and vocab is a mapping or a dictionary in Python for from the token uh ID to the bytes object for that token so we begin with the raw bytes for tokens from 0 to 255 and then we go in order of all the merges and we sort of uh populate this vocab list by doing an addition here so this is the basically 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2626)~~  the bytes representation of the first child followed by the second one and remember these are bytes objects so this addition here is an addition of two bytes objects just concatenation so that's what we get here one tricky thing to be careful with by the way is that I'm iterating a dictionary in Python using a DOT items and uh it really matters that this runs in the order in which we inserted items into the merous dictionary luckily starting with python 3.7 this is guaranteed to be the case but before python 3.7 this iteration may have been out of order with respect to how we inserted elements into merges and this may not have worked but we are using an um modern python so we're okay and then here uh given the IDS the first thing we're going to do is get the tokens so the way I implemented this here is I'm taking I'm iterating over all the IDS I'm using vocap to look up their bytes and then here this is one way in Python to concatenate all these bytes together to create our tokens and then these tokens here at this point are 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2692)~~  raw bytes so I have to decode using UTF F now back into python strings so previously we called that encode on a string object to get the bytes and now we're doing it Opposite we're taking the bytes and calling a decode on the bytes object to get a string in Python and then we can return text so um this is how we can do it now this actually has a um issue um in the way I implemented it and this could actually throw an error so try to think figure out why this code could actually result in an error if we plug in um uh some sequence of IDs that is unlucky so let me demonstrate the issue when I try to decode just something like 97 I am going to get letter A here back so nothing too crazy happening but when I try to decode 128 as a single element the token 128 is what in string or in Python object uni Cod decoder utfa can't Decode by um 0x8 which is this in HEX in position zero invalid start bite what does that mean well to understand what this means we have to go back to our utf8 page uh that I briefly showed earlier and this is Wikipedia utf8 and 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2771)~~  basically there's a specific schema that utfa bytes take so in particular if you have a multi-te object for some of the Unicode characters they have to have this special sort of envelope in how the encoding works and so what's happening here is that invalid start pite that's because 128 the binary representation of it is one followed by all zeros so we have one and then all zero and we see here that that doesn't conform to the format because one followed by all zero just doesn't fit any of these rules so to speak so it's an invalid start bite which is byte one this one must have a one following it and then a zero following it and then the content of your uni codee in x here so basically we don't um exactly follow the utf8 standard and this cannot be decoded and so the way to fix this um is to use this errors equals in bytes. decode function of python and by default errors is strict so we will throw an error if um it's not valid utf8 bytes encoding but there are many different things that you could put here on error handling 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2844)~~  this is the full list of all the errors that you can use and in particular instead of strict let's change it to replace and that will replace uh with this special marker this replacement character so errors equals replace and now we just get that character back so basically not every single by sequence is valid utf8 and if it happens that your large language model for example predicts your tokens in a bad manner then they might not fall into valid utf8 and then we won't be able to decode them so the standard practice is to basically uh use errors equals replace and this is what you will also find in the openai um code that they released as well but basically whenever you see um this kind of a character in your output in that case uh something went wrong and the LM output not was not valid uh sort of sequence of tokens okay and now we're going to go the other way so we are going to implement this Arrow right here where we are going to be given a string and we want to encode it into tokens so this is the signature of the 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2914)~~  function that we're interested in and um this should basically print a list of integers of the tokens so again uh try to maybe implement this yourself if you'd like a fun exercise uh and pause here otherwise I'm going to start putting in my solution so again there are many ways to do this so um this is one of the ways that sort of I came came up with so the first thing we're going to do is we are going to uh take our text encode it into utf8 to get the raw bytes and then as before we're going to call list on the bytes object to get a list of integers of those bytes so those are the starting tokens those are the raw bytes of our sequence but now of course according to the merges dictionary above and recall this was the merges some of the bytes may be merged according to this lookup in addition to that remember that the merges was built from top to bottom and this is sort of the order in which we inserted stuff into merges and so we prefer to do all these merges in the beginning before we do these merges later because um for 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2979)~~  example this merge over here relies on the 256 which got merged here so we have to go in the order from top to bottom sort of if we are going to be merging anything now we expect to be doing a few merges so we're going to be doing W true um and now we want to find a pair of byes that is consecutive that we are allowed to merge according to this in order to reuse some of the functionality that we've already written I'm going to reuse the function uh get stats so recall that get stats uh will give us the we'll basically count up how many times every single pair occurs in our sequence of tokens and return that as a dictionary and the dictionary was a mapping from all the different uh by pairs to the number of times that they occur right um at this point we don't actually care how many times they occur in the sequence we only care what the raw pairs are in that sequence and so I'm only going to be using basically the keys of the dictionary I only care about the set of possible merge candidates if that makes sense now we want to identify the pair 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3046)~~  that we're going to be merging at this stage of the loop so what do we want we want to find the pair or like the a key inside stats that has the lowest index in the merges uh dictionary because we want to do all the early merges before we work our way to the late merges so again there are many different ways to implement this but I'm going to do something a little bit fancy here so I'm going to be using the Min over an iterator in Python when you call Min on an iterator and stats here as a dictionary we're going to be iterating the keys of this dictionary in Python so we're looking at all the pairs inside stats um which are all the consecutive Pairs and we're going to be taking the consecutive pair inside tokens that has the minimum what the Min takes a key which gives us the function that is going to return a value over which we're going to do the Min and the one we care about is we're we care about taking merges and basically getting um that pairs index so basically for any pair inside stats we are going to be looking into 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3120)~~  merges at what index it has and we want to get the pair with the Min number so as an example if there's a pair 101 and 32 we definitely want to get that pair uh we want to identify it here and return it and pair would become 10132 if it occurs and the reason that I'm putting a float INF here as a fall back is that in the get function when we call uh when we basically consider a pair that doesn't occur in the merges then that pair is not eligible to be merged right so if in the token sequence there's some pair that is not a merging pair it cannot be merged then uh it doesn't actually occur here and it doesn't have an index and uh it cannot be merged which we will denote as float INF and the reason Infinity is nice here is because for sure we're guaranteed that it's not going to participate in the list of candidates when we do the men so uh so this is one way to do it so B basically long story short this Returns the most eligible merging candidate pair uh that occurs in the tokens now one thing to be careful with here is this uh function here might 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3187)~~  fail in the following way if there's nothing to merge then uh uh then there's nothing in merges um that satisfi that is satisfied anymore there's nothing to merge everything just returns float imps and then the pair I think will just become the very first element of stats um but this pair is not actually a mergeable pair it just becomes the first pair inside stats arbitrarily because all of these pairs evaluate to float in for the merging Criterion so basically it could be that this this doesn't look succeed because there's no more merging pairs so if this pair is not in merges that was returned then this is a signal for us that actually there was nothing to merge no single pair can be merged anymore in that case we will break merged you may come up with a different implementation by the way this is kind of like really trying hard in Python um but really we're just trying to find a pair that can be merged with the lowest index here now if we did find a pair that is inside merges with the lowest index then so we're going to look into the merger 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3262)~~  dictionary for that pair to look up the index and we're going to now merge that into that index so we're going to do tokens equals and we're going to replace the original tokens we're going to be replacing the pair pair and we're going to be replacing it with index idx and this returns a new list of tokens where every occurrence of pair is replaced with idx so we're doing a merge and we're going to be continuing this until eventually nothing can be merged we'll come out here and we'll break out and here we just return tokens and so that that's the implementation I think so hopefully this runs okay cool um yeah and this looks uh reasonable so for example 32 is a space in asky so that's here um so this looks like it worked great okay so let's wrap up this section of the video at least I wanted to point out that this is not quite the right implementation just yet because we are leaving out a special case so in particular if uh we try to do this this would give us an error and the issue is that um if we only have a 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3326)~~  single character or an empty string then stats is empty and that causes an issue inside Min so one way to fight this is if L of tokens is at least two because if it's less than two it's just a single token or no tokens then let's just uh there's nothing to merge so we just return so that would fix uh that case Okay and then second I have a few test cases here for us as well so first let's make sure uh about or let's note the following if we take a string and we try to encode it and then decode it back you'd expect to get the same string back strings so I think uh so here it is the case and I think in general this is probably the case um but notice that going backwards is not is not you're not going to have an identity going backwards because as I mentioned us not all token sequences are valid utf8 uh sort of by streams and so so therefore you're some of them can't even be decodable um so this only goes in One Direction but for that one direction we can check uh here if we take the training text which is the text that we 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3396)~~  train to tokenizer around we can make sure that when we encode and decode we get the same thing back which is true and here I took some validation data so I went to I think this web page and I grabbed some text so this is text that the tokenizer has not seen and we can make sure that this also works um okay so that gives us some confidence that this was correctly implemented so those are the basics of the bite pair encoding algorithm we saw how we can uh take some training set train a tokenizer the parameters of this tokenizer really are just this dictionary of merges and that basically creates the little binary Forest on top of raw bites once we have this the merges table we can both encode and decode between raw text and token sequences so that's the the simplest setting of The tokenizer what we're going to do now though is we're going to look at some of the St the art lar language models and the kinds of tokenizers that they use and we're going to see that this picture complexifies very quickly so we're going 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3452)~~  to go through the details of this comp complexification one at a time so let's kick things off by looking at the GPD Series so in particular I have the gpt2 paper here um and this paper is from 2019 or so so 5 years ago and let's scroll down to input representation this is where they talk about the tokenizer that they're using for gpd2 now this is all fairly readable so I encourage you to pause and um read this yourself but this is where they motivate the use of the bite pair encoding algorithm on the bite level representation of utf8 encoding so this is where they motivate it and they talk about the vocabulary sizes and everything now everything here is exactly as we've covered it so far but things start to depart around here so what they mention is that they don't just apply the naive algorithm as we have done it and in particular here's a example suppose that you have common words like dog what will happen is that dog of course occurs very frequently in the text and it occurs right next to all kinds of punctuation as an example so 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3516)~~  doc dot dog exclamation mark dog question mark Etc and naively you might imagine that the BP algorithm could merge these to be single tokens and then you end up with lots of tokens that are just like dog with a slightly different punctuation and so it feels like you're clustering things that shouldn't be clustered you're combining kind of semantics with uation and this uh feels suboptimal and indeed they also say that this is suboptimal according to some of the experiments so what they want to do is they want to top down in a manual way enforce that some types of um characters should never be merged together um so they want to enforce these merging rules on top of the bite PA encoding algorithm so let's take a look um at their code and see how they actually enforce this and what kinds of mergy they actually do perform so I have to to tab open here for gpt2 under open AI on GitHub and when we go to Source there is an encoder thatp now I don't personally love that they call it encoder dopy because this is the tokenizer and the tokenizer can do both 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3579)~~  encode and decode uh so it feels kind of awkward to me that it's called encoder but that is the tokenizer and there's a lot going on here and we're going to step through it in detail at one point for now I just want to focus on this part here the create a rigix pattern here that looks very complicated and we're going to go through it in a bit uh but this is the core part that allows them to enforce rules uh for what parts of the text Will Never Be merged for sure now notice that re. compile here is a little bit misleading because we're not just doing import re which is the python re module we're doing import reex as re and reex is a python package that you can install P install r x and it's basically an extension of re so it's a bit more powerful re um so let's take a look at this pattern and what it's doing and why this is actually doing the separation that they are looking for okay so I've copy pasted the pattern here to our jupit notebook where we left off and let's take this pattern for a spin so in the exact same way that 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3642)~~  their code does we're going to call an re. findall for this pattern on any arbitrary string that we are interested so this is the string that we want to encode into tokens um to feed into n llm like gpt2 so what exactly is this doing well re. findall will take this pattern and try to match it against a string um the way this works is that you are going from left to right in the string and you're trying to match the pattern and R.F find all will get all the occurrences and organize them into a list now when you look at the um when you look at this pattern first of all notice that this is a raw string um and then these are three double quotes just to start the string so really the string itself this is the pattern itself right and notice that it's made up of a lot of ores so see these vertical bars those are ores in reg X and so you go from left to right in this pattern and try to match it against the string wherever you are so we have hello and we're going to try to match it well it's not apostrophe s it's not apostrophe t 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3711)~~  or any of these but it is an optional space followed by- P of uh sorry SL P of L one or more times what is/ P of L it is coming to some documentation that I found um there might be other sources as well uh SLP is a letter any kind of letter from any language and hello is made up of letters h e l Etc so optional space followed by a bunch of letters one or more letters is going to match hello but then the match ends because a white space is not a letter so from there on begins a new sort of attempt to match against the string again and starting in here we're going to skip over all of these again until we get to the exact same Point again and we see that there's an optional space this is the optional space followed by a bunch of letters one or more of them and so that matches so when we run this we get a list of two elements hello and then space world so how are you if we add more letters we would just get them like this now what is this doing and why is this important we are taking our string and instead of directly encoding it um for 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3789)~~  tokenization we are first splitting it up and when you actually step through the code and we'll do that in a bit more detail what really is doing on a high level is that it first splits your text into a list of texts just like this one and all these elements of this list are processed independently by the tokenizer and all of the results of that processing are simply concatenated so hello world oh I I missed how hello world how are you we have five elements of list all of these will independent independently go from text to a token sequence and then that token sequence is going to be concatenated it's all going to be joined up and roughly speaking what that does is you're only ever finding merges between the elements of this list so you can only ever consider merges within every one of these elements in individually and um after you've done all the possible merging for all of these elements individually the results of all that will be joined um by concatenation and so you are basically what what you're doing effectively is 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3858)~~  you are never going to be merging this e with this space because they are now parts of the separate elements of this list and so you are saying we are never going to merge eace um because we're breaking it up in this way so basically using this regx pattern to Chunk Up the text is just one way of enforcing that some merges are not to happen and we're going to go into more of this text and we'll see that what this is trying to do on a high level is we're trying to not merge across letters across numbers across punctuation and so on so let's see in more detail how that works so let's continue now we have/ P ofn if you go to the documentation SLP of n is any kind of numeric character in any script so it's numbers so we have an optional space followed by numbers and those would be separated out so letters and numbers are being separated so if I do Hello World 123 how are you then world will stop matching here because one is not a letter anymore but one is a number so this group will match for that and uh let's see how these apostrophes work 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3928)~~  so here if we have um uh Slash V or I mean apostrophe V as an example then apostrophe here is not a letter or a number so hello will stop matching and then we will exactly match this with that so that will come out as a separate thing so why are they doing the apostrophes here honestly I think that these are just like very common apostrophes p uh that are used um typically I don't love that they've done this because uh let me show you what happens when you have uh some Unicode apostrophes like for example you can have if you have house then this will be separated out because of this matching but if you use the Unicode apostrophe like this then suddenly this does not work and so this apostrophe will actually become its own thing now and so so um it's basically hardcoded for this specific kind of apostrophe and uh otherwise they become completely separate tokens in addition to this you can go to the gpt2 docs and here when they Define the pattern they say should have added re. ignore case so BP merges can happen for capitalized versions of 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4005)~~  contractions so what they're pointing out is that you see how this is apostrophe and then lowercase letters well because they didn't do re. ignore case then then um these rules will not separate out the apostrophes if it's uppercase so house would be like this but if I did house if I'm uppercase then notice suddenly the apostrophe comes by itself so the tokenization will work differently in uppercase and lower case inconsistently separating out these apostrophes so it feels extremely gnarly and slightly gross um but that's that's how that works okay so let's come back after trying to match a bunch of apostrophe Expressions by the way the other issue here is that these are quite language specific probably so I don't know that all the languages for example use or don't use apostrophes but that would be inconsistently tokenized as a result then we try to match letters then we try to match numbers and then if that doesn't work we fall back to here and what this is saying is again optional space followed by something that is not 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4071)~~  a letter number or a space in one or more of that so what this is doing effectively is this is trying to match punctuation roughly speaking not letters and not numbers so this group will try to trigger for that so if I do something like this then these parts here are not letters or numbers but they will actually they are uh they will actually get caught here and so they become its own group so we've separated out the punctuation and finally this um this is also a little bit confusing so this is matching white space but this is using a negative look ahead assertion in regex so what this is doing is it's matching wh space up to but not including the last Whit space character why is this important um this is pretty subtle I think so you see how the white space is always included at the beginning of the word so um space r space u Etc suppose we have a lot of spaces here what's going to happen here is that these spaces up to not including the last character will get caught by this and what that will do is it will separate out the spaces up to but not 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4142)~~  including the last character so that the last character can come here and join with the um space you and the reason that's nice is because space you is the common token so if I didn't have these Extra Spaces here you would just have space you and if I add tokens if I add spaces we still have a space view but now we have all this extra white space so basically the GB to tokenizer really likes to have a space letters or numbers um and it it preens these spaces and this is just something that it is consistent about so that's what that is for and then finally we have all the the last fallback is um whites space characters uh so um that would be just um if that doesn't get caught then this thing will catch any trailing spaces and so on I wanted to show one more real world example here so if we have this string which is a piece of python code and then we try to split it up then this is the kind of output we get so you'll notice that the list has many elements here and that's because we are splitting up fairly often uh every 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4205)~~  time sort of a category changes um so there will never be any merges Within These elements and um that's what you are seeing here now you might think that in order to train the tokenizer uh open AI has used this to split up text into chunks and then run just a BP algorithm within all the chunks but that is not exactly what happened and the reason is the following notice that we have the spaces here uh those Spaces end up being entire elements but these spaces never actually end up being merged by by open Ai and the way you can tell is that if you copy paste the exact same chunk here into Tik token U Tik tokenizer you see that all the spaces are kept independent and they're all token 220 so I think opena at some point Point en Force some rule that these spaces would never be merged and so um there's some additional rules on top of just chunking and bpe that open ey is not uh clear about now the training code for the gpt2 tokenizer was never released so all we have is uh the code that I've already shown you but this code here 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4273)~~  that they've released is only the inference code for the tokens so this is not the training code you can't give it a piece of text and training tokenizer this is just the inference code which Tak takes the merges that we have up above and applies them to a new piece of text and so we don't know exactly how opening ey trained um train the tokenizer but it wasn't as simple as chunk it up and BP it uh whatever it was next I wanted to introduce you to the Tik token library from openai which is the official library for tokenization from openai so this is Tik token bip install P to Tik token and then um you can do the tokenization in inference this is again not training code this is only inference code for tokenization um I wanted to show you how you would use it quite simple and running this just gives us the gpt2 tokens or the GPT 4 tokens so this is the tokenizer use for GPT 4 and so in particular we see that the Whit space in gpt2 remains unmerged but in GPT 4 uh these Whit spaces merge as we also saw in this one where here they're all 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4339)~~  unmerged but if we go down to GPT 4 uh they become merged um now in the gp4 uh tokenizer they changed the regular expression that they use to Chunk Up text so the way to see this is that if you come to your the Tik token uh library and then you go to this file Tik token X openi public this is where sort of like the definition of all these different tokenizers that openi maintains is and so uh necessarily to do the inference they had to publish some of the details about the strings so this is the string that we already saw for gpt2 it is slightly different but it is actually equivalent uh to what we discussed here so this pattern that we discussed is equivalent to this pattern this one just executes a little bit faster so here you see a little bit of a slightly different definition but otherwise it's the same we're going to go into special tokens in a bit and then if you scroll down to CL 100k this is the GPT 4 tokenizer you see that the pattern has changed um and this is kind of like the main the major change in 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4406)~~  addition to a bunch of other special tokens which I'll go into in a bit again now some I'm not going to actually go into the full detail of the pattern change because honestly this is my numbing uh I would just advise that you pull out chat GPT and the regex documentation and just step through it but really the major changes are number one you see this eye here that means that the um case sensitivity this is case insensitive match and so the comment that we saw earlier on oh we should have used re. uppercase uh basically we're now going to be matching these apostrophe s apostrophe D apostrophe M Etc uh we're going to be matching them both in lowercase and in uppercase so that's fixed there's a bunch of different like handling of the whites space that I'm not going to go into the full details of and then one more thing here is you will notice that when they match the numbers they only match one to three numbers so so they will never merge numbers that are in low in more than three digits only up to three digits of 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4471)~~  numbers will ever be merged and uh that's one change that they made as well to prevent uh tokens that are very very long number sequences uh but again we don't really know why they do any of this stuff uh because none of this is documented and uh it's just we just get the pattern so um yeah it is what it is but those are some of the changes that gp4 has made and of course the vocabulary size went from roughly 50k to roughly 100K the next thing I would like to do very briefly is to take you through the gpt2 encoder dopy that openi has released uh this is the file that I already mentioned to you briefly now this file is uh fairly short and should be relatively understandable to you at this point um starting at the bottom here they are loading two files encoder Json and vocab bpe and they do some light processing on it and then they call this encoder object which is the tokenizer now if you'd like to inspect these two files which together constitute their saved tokenizer then you can do that with a piece of code like 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4537)~~  this um this is where you can download these two files and you can inspect them if you'd like and what you will find is that this encoder as they call it in their code is exactly equivalent to our vocab so remember here where we have this vocab object which allowed us us to decode very efficiently and basically it took us from the integer to the byes uh for that integer so our vocab is exactly their encoder and then their vocab bpe confusingly is actually are merges so their BP merges which is based on the data inside vocab bpe ends up being equivalent to our merges so uh basically they are saving and loading the two uh variables that for us are also critical the merges variable and the vocab variable using just these two variables you can represent a tokenizer and you can both do encoding and decoding once you've trained this tokenizer now the only thing that um is actually slightly confusing inside what opening ey does here is that in addition to this encoder and a decoder they also have something called a bite encoder and 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4609)~~  a bite decoder and this is actually unfortunately just kind of a spirous implementation detail and isn't actually deep or interesting in any way so I'm going to skip the discussion of it but what opening ey does here for reasons that I don't fully understand is that not only have they this tokenizer which can encode and decode but they have a whole separate layer here in addition that is used serially with the tokenizer and so you first do um bite encode and then encode and then you do decode and then bite decode so that's the loop and they are just stacked serial on top of each other and and it's not that interesting so I won't cover it and you can step through it if you'd like otherwise this file if you ignore the bite encoder and the bite decoder will be algorithmically very familiar with you and the meat of it here is the what they call bpe function and you should recognize this Loop here which is very similar to our own y Loop where they're trying to identify the Byram uh a pair that they should be merging next and then here just like we 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4669)~~  had they have a for Loop trying to merge this pair uh so they will go over all of the sequence and they will merge the pair whenever they find it and they keep repeating that until they run out of possible merges in the in the text so that's the meat of this file and uh there's an encode and a decode function just like we have implemented it so long story short what I want you to take away at this point is that unfortunately it's a little bit of a messy code that they have but algorithmically it is identical to what we've built up above and what we've built up above if you understand it is algorithmically what is necessary to actually build a BP to organizer train it and then both encode and decode the next topic I would like to turn to is that of special tokens so in addition to tokens that are coming from you know raw bytes and the BP merges we can insert all kinds of tokens that we are going to use to delimit different parts of the data or introduced to create a special structure of the token streams so in uh if you look at this encoder 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4727)~~  object from open AIS gpd2 right here we mentioned this is very similar to our vocab you'll notice that the length of 50257 and as I mentioned it's mapping uh and it's inverted from the mapping of our vocab our vocab goes from integer to string and they go the other way around for no amazing reason um but the thing to note here is that this the mapping table here is 50257 where does that number come from where what are the tokens as I mentioned there are 256 raw bite token tokens and then opena actually did 50,000 merges so those become the other tokens but this would have been 50256 so what is the 57th token and there is basically one special token and that one special token you can see is called end of text so this is a special token and it's the very last token and this token is used to delimit documents ments in the training set so when we're creating the training data we have all these documents and we tokenize them and we get a stream of tokens those tokens only range from Z to 50256 and then in between those 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4807)~~  documents we put special end of text token and we insert that token in between documents and we are using this as a signal to the language model that the document has ended and what follows is going to be unrelated to the document previously that said the language model has to learn this from data it it needs to learn that this token usually means that it should wipe its sort of memory of what came before and what came before this token is not actually informative to what comes next but we are expecting the language model to just like learn this but we're giving it the Special sort of the limiter of these documents we can go here to Tech tokenizer and um this the gpt2 tokenizer uh our code that we've been playing with before so we can add here right hello world world how are you and we're getting different tokens but now you can see what if what happens if I put end of text you see how until I finished it these are all different tokens end of text still set different tokens and now when I finish it suddenly we get token 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4873)~~  50256 and the reason this works is because this didn't actually go through the bpe merges instead the code that actually outposted tokens has special case instructions for handling special tokens um we did not see these special instructions for handling special tokens in the encoder dopy it's absent there but if you go to Tech token Library which is uh implemented in Rust you will find all kinds of special case handling for these special tokens that you can register uh create adds to the vocabulary and then it looks for them and it uh whenever it sees these special tokens like this it will actually come in and swap in that special token so these things are outside of the typical algorithm of uh B PA en coding so these special tokens are used pervasively uh not just in uh basically base language modeling of predicting the next token in the sequence but especially when it gets to later to the fine tuning stage and all of the chat uh gbt sort of aspects of it uh because we don't just want to Del limit documents we want to delimit entire conversations 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4939)~~  between an assistant and a user so if I refresh this sck tokenizer page the default example that they have here is using not sort of base model encoders but ftuned model uh sort of tokenizers um so for example using the GPT 3.5 turbo scheme these here are all special tokens I am start I end Etc uh this is short for Imaginary mcore start by the way but you can see here that there's a sort of start and end of every single message and there can be many other other tokens lots of tokens um in use to delimit these conversations and kind of keep track of the flow of the messages here now we can go back to the Tik token library and here when you scroll to the bottom they talk about how you can extend tick token and I can you can create basically you can Fork uh the um CL 100K base tokenizers in gp4 and for example you can extend it by adding more special tokens and these are totally up to you you can come up with any arbitrary tokens and add them with the new ID afterwards and the tikken library will uh correctly swap them out uh when 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5010)~~  it sees this in the strings now we can also go back to this file which we've looked at previously and I mentioned that the gpt2 in Tik toen open I.P we have the vocabulary we have the pattern for splitting and then here we are registering the single special token in gpd2 which was the end of text token and we saw that it has this ID in GPT 4 when they defy this here you see that the pattern has changed as we've discussed but also the special tokens have changed in this tokenizer so we of course have the end of text just like in gpd2 but we also see three sorry four additional tokens here Thim prefix middle and suffix what is fim fim is short for fill in the middle and if you'd like to learn more about this idea it comes from this paper um and I'm not going to go into detail in this video it's beyond this video and then there's one additional uh serve token here so that's that encoding as well so it's very common basically to train a language model and then if you'd like uh you can add special tokens now when you 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5078)~~  add special tokens you of course have to um do some model surgery to the Transformer and all the parameters involved in that Transformer because you are basically adding an integer and you want to make sure that for example your embedding Matrix for the vocabulary tokens has to be extended by adding a row and typically this row would be initialized uh with small random numbers or something like that because we need to have a vector that now stands for that token in addition to that you have to go to the final layer of the Transformer and you have to make sure that that projection at the very end into the classifier uh is extended by one as well so basically there's some model surgery involved that you have to couple with the tokenization changes if you are going to add special tokens but this is a very common operation that people do especially if they'd like to fine tune the model for example taking it from a base model to a chat model like chat GPT okay so at this point you should have everything you need in order to 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5131)~~  build your own gp4 tokenizer now in the process of developing this lecture I've done that and I published the code under this repository MBP so MBP looks like this right now as I'm recording but uh the MBP repository will probably change quite a bit because I intend to continue working on it um in addition to the MBP repository I've published the this uh exercise progression that you can follow so if you go to exercise. MD here uh this is sort of me breaking up the task ahead of you into four steps that sort of uh build up to what can be a gp4 tokenizer and so feel free to follow these steps exactly and follow a little bit of the guidance that I've laid out here and anytime you feel stuck just reference the MBP repository here so either the tests could be useful or the MBP repository itself I try to keep the code fairly clean and understandable and so um feel free to reference it whenever um you get stuck uh in addition to that basically once you write it you should be able to reproduce this behavior from Tech token 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5197)~~  so getting the gb4 tokenizer you can take uh you can encode the string and you should get these tokens and then you can encode and decode the exact same string to recover it and in addition to all that you should be able to implement your own train function uh which Tik token Library does not provide it's it's again only inference code but you could write your own train MBP does it as well and that will allow you to train your own token vocabularies so here are some of the code inside M be mean bpe uh shows the token vocabularies that you might obtain so on the left uh here we have the GPT 4 merges uh so the first 256 are raw individual bytes and then here I am visualizing the merges that gp4 performed during its training so the very first merge that gp4 did was merge two spaces into a single token for you know two spaces and that is a token 256 and so this is the order in which things merged during gb4 training and this is the merge order that um we obtain in MBP by training a tokenizer and in this case I trained it on a Wikipedia page of 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5263)~~  Taylor Swift uh not because I'm a Swifty but because that is one of the longest um Wikipedia Pages apparently that's available but she is pretty cool and um what was I going to say yeah so you can compare these two uh vocabularies and so as an example um here GPT for merged I in to become in and we've done the exact same thing on this token 259 here space t becomes space t and that happened for us a little bit later as well so the difference here is again to my understanding only a difference of the training set so as an example because I see a lot of white space I supect that gp4 probably had a lot of python code in its training set I'm not sure uh for the tokenizer and uh here we see much less of that of course in the Wikipedia page so roughly speaking they look the same and they look the same because they're running the same algorithm and when you train your own you're probably going to get something similar depending on what you train it on okay so we are now going to move on from tick token and the way that open AI tokenizes its strings and 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5328)~~  we're going to discuss one more very commonly used library for working with tokenization inlm and that is sentence piece so sentence piece is very commonly used in language models because unlike Tik token it can do both training and inference and is quite efficient at both it supports a number of algorithms for training uh vocabularies but one of them is the B pair en coding algorithm that we've been looking at so it supports it now sentence piece is used both by llama and mistal series and many other models as well it is on GitHub under Google sentence piece and the big difference with sentence piece and we're going to look at example because this is kind of hard and subtle to explain is that they think different about the order of operations here so in the case of Tik token we first take our code points in the string we encode them using mutf to bytes and then we're merging bytes it's fairly straightforward for sentence piece um it works directly on the level of the code points themselves so so it looks at whatever code points are available in 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5394)~~  your training set and then it starts merging those code points and um the bpe is running on the level of code points and if you happen to run out of code points so there are maybe some rare uh code points that just don't come up too often and the Rarity is determined by this character coverage hyper parameter then these uh code points will either get mapped to a special unknown token like ank or if you have the bite foldback option turned on then that will take those rare Cod points it will encode them using utf8 and then the individual bytes of that encoding will be translated into tokens and there are these special bite tokens that basically get added to the vocabulary so it uses BP on on the code points and then it falls back to bytes for rare Cod points um and so that's kind of like difference personally I find the Tik token we significantly cleaner uh but it's kind of like a subtle but pretty major difference between the way they approach tokenization let's work with with a concrete example because otherwise this 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5454)~~  is kind of hard to um to get your head around so let's work with a concrete example this is how we can import sentence piece and then here we're going to take I think I took like the description of sentence piece and I just created like a little toy data set it really likes to have a file so I created a toy. txt file with this content now what's kind of a little bit crazy about sentence piece is that there's a ton of options and configurations and the reason this is so is because sentence piece has been around I think for a while and it really tries to handle a large diversity of things and um because it's been around I think it has quite a bit of accumulated historical baggage uh as well and so in particular there's like a ton of configuration arguments this is not even all of it you can go to here to see all the training options um and uh there's also quite useful documentation when you look at the raw Proto buff uh that is used to represent the trainer spec and so on um many of these options are irrelevant to 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5515)~~  us so maybe to point out one example Das Das shrinking Factor uh this shrinking factor is not used in the B pair en coding algorithm so this is just an argument that is irrelevant to us um it algorithm now what I tried to do here is I tried to set up sentence piece in a way that is very very similar as far as I can tell to maybe identical hopefully to the way that llama 2 was strained so the way they trained their own um their own tokenizer and the way I did this was basically you can take the tokenizer model file that meta released and you can um open it using the Proto protuff uh sort of file that you can generate and then you can inspect all the options and I tried to copy over all the options that looked relevant so here we set up the input it's raw text in this file here's going to be the output so it's going to be for talk 400. model and vocab we're saying that we're going to use the BP algorithm and we want to Bap size of 400 then there's a ton of configurations for um for basically pre-processing and normalization rules as they're called 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5587)~~  normalization used to be very prevalent I would say before llms in natural language processing so in machine translation and uh text classification and so on you want to normalize and simplify the text and you want to turn it all lowercase and you want to remove all double whites space Etc and in language models we prefer not to do any of it or at least that is my preference as a deep learning person you want to not touch your data you want to keep the raw data as much as possible um in a raw form so you're basically trying to turn off a lot of this if you can the other thing that sentence piece does is that it has this concept of sentences so sentence piece it's back it's kind of like was developed I think early in the days where there was um an idea that they you're training a tokenizer on a bunch of independent sentences so it has a lot of like how many sentences you're going to train on what is the maximum sentence length um shuffling sentences and so for it sentences are kind of like the individual training examples but again 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5647)~~  in the context of llms I find that this is like a very spous and weird distinction like sentences are just like don't touch the raw data sentences happen to exist but in raw data sets there are a lot of like inet like what exactly is a sentence what isn't a sentence um and so I think like it's really hard to Define what an actual sentence is if you really like dig into it and there could be different concepts of it in different languages or something like that so why even introduce the concept it it doesn't honestly make sense to me I would just prefer to treat a file as a giant uh stream of bytes it has a lot of treatment around rare word characters and when I say word I mean code points we're going to come back to this in a second and it has a lot of other rules for um basically splitting digits splitting white space and numbers and how you deal with that so these are some kind of like merge rules so I think this is a little bit equivalent to tick token using the regular expression to split up categories there's like kind of 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5707)~~  equivalence of it if you squint T it in sentence piece where you can also for example split up split up the digits uh and uh so on there's a few more things here that I'll come back to in a bit and then there are some special tokens that you can indicate and it hardcodes the UN token the beginning of sentence end of sentence and a pad token um and the UN token must exist for my understanding and then some some things so we can train and when when I press train it's going to create this file talk 400. model and talk 400. wab I can then load the model file and I can inspect the vocabulary off it and so we trained vocab size 400 on this text here and these are the individual pieces the individual tokens that sentence piece will create so in the beginning we see that we have the an token uh with the ID zero then we have the beginning of sequence end of sequence one and two and then we said that the pad ID is negative 1 so we chose not to use it so there's no pad ID here then these are individual bite tokens so here we saw that bite fallback 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5780)~~  in llama was turned on so it's true so what follows are going to be the 256 bite IDs and then at the bottom after the bite tokens come the merges and these are the parent nodes in the merges so we're not seeing the children we're just seeing the parents and their ID and then after the merges comes eventually the individual tokens and their IDs and so these are the individual tokens so these are the individual code Point tokens if you will and they come at the end so that is the ordering with which sentence piece sort of like represents its vocabularies it starts with special tokens then the bike tokens then the merge tokens and then the individual codo tokens and all these raw codepoint to tokens are the ones that it encountered in the training set so those individual code points are all the the entire set of code points that occurred here so those all get put in there and then those that are extremely rare as determined by character coverage so if a code Point occurred only a single time out of like a million um sentences or 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5855)~~  something like that then it would be ignored and it would not be added to our uh vocabulary once we have a vocabulary we can encode into IDs and we can um sort of get a list and then here I am also decoding the indiv idual tokens back into little pieces as they call it so let's take a look at what happened here hello space on so these are the token IDs we got back and when we look here uh a few things sort of uh jump to mind number one take a look at these characters the Korean characters of course were not part of the training set so sentence piece is encountering code points that it has not seen during training time and those code points do not have a token associated with them so suddenly these are un tokens unknown tokens but because bite fall back as true instead sentence piece falls back to bytes and so it takes this it encodes it with utf8 and then it uses these tokens to represent uh those bytes and that's what we are getting sort of here this is the utf8 uh encoding and in this shifted by three uh because of these um special tokens here 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5936)~~  that have IDs earlier on so that's what happened here now one more thing that um well first before I go on with respect to the bitef back let me remove bite foldback if this is false what's going to happen let's retrain so the first thing that happened is all the bite tokens disappeared right and now we just have the merges and we have a lot more merges now because we have a lot more space because we're not taking up space in the wab size uh with all the bytes and now if we encode this we get a zero so this entire string here suddenly there's no bitef back so this is unknown and unknown is an and so this is zero because the an token is token zero and you have to keep in mind that this would feed into your uh language model so what is a language model supposed to do when all kinds of different things that are unrecognized because they're rare just end up mapping into Unk it's not exactly the property that you want so that's why I think llama correctly uh used by fallback true uh because we definitely want to feed 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6004)~~  these um unknown or rare code points into the model and some uh some manner the next thing I want to show you is the following notice here when we are decoding all the individual tokens you see how spaces uh space here ends up being this um bold underline I'm not 100% sure by the way why sentence piece switches whites space into these bold underscore characters maybe it's for visualization I'm not 100% sure why that happens uh but notice this why do we have an extra space in the front of hello um what where is this coming from well it's coming from this option here um add dummy prefix is true and when you go to the documentation add D whites space at the beginning of text in order to treat World in world and hello world in the exact same way so what this is trying to do is the following if we go back to our tick tokenizer world as uh token by itself has a different ID than space world so we have this is 1917 but this is 14 Etc so these are two different tokens for the language model and the language model has to learn from data that they 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6079)~~  are actually kind of like a very similar concept so to the language model in the Tik token World um basically words in the beginning of sentences and words in the middle of sentences actually look completely different um and it has to learned that they are roughly the same so this add dami prefix is trying to fight that a little bit and the way that works is that it basically uh adds a dummy prefix so for as a as a part of pre-processing it will take the string and it will add a space it will do this and that's done in an effort to make this world and that world the same they will both be space world so that's one other kind of pre-processing option that is turned on and llama 2 also uh uses this option and that's I think everything that I want to say for my preview of sentence piece and how it is different um maybe here what I've done is I just uh put in the Raw protocol buffer representation basically of the tokenizer the too trained so feel free to sort of Step through this and if you would like uh your tokenization to look 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6147)~~  identical to that of the meta uh llama 2 then you would be copy pasting these settings as I tried to do up above and uh yeah that's I think that's it for this section I think my summary for sentence piece from all of this is number one I think that there's a lot of historical baggage in sentence piece a lot of Concepts that I think are slightly confusing and I think potentially um contain foot guns like this concept of a sentence and it's maximum length and stuff like that um otherwise it is fairly commonly used in the industry um because it is efficient and can do both training and inference uh it has a few quirks like for example un token must exist and the way the bite fallbacks are done and so on I don't find particularly elegant and unfortunately I have to say it's not very well documented so it took me a lot of time working with this myself um and just visualizing things and trying to really understand what is happening here because uh the documentation unfortunately is in my opion not not super amazing but it is a very nice repo 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6205)~~  that is available to you if you'd like to train your own tokenizer right now okay let me now switch gears again as we're starting to slowly wrap up here I want to revisit this issue in a bit more detail of how we should set the vocap size and what are some of the considerations around it so for this I'd like to go back to the model architecture that we developed in the last video when we built the GPT from scratch so this here was uh the file that we built in the previous video and we defined the Transformer model and and let's specifically look at Bap size and where it appears in this file so here we Define the voap size uh at this time it was 65 or something like that extremely small number so this will grow much larger you'll see that Bap size doesn't come up too much in most of these layers the only place that it comes up to is in exactly these two places here so when we Define the language model there's the token embedding table which is this two-dimensional array where the vocap size is basically the number of rows and 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6261)~~  uh each vocabulary element each token has a vector that we're going to train using back propagation that Vector is of size and embed which is number of channels in the Transformer and basically as voap size increases this embedding table as I mentioned earlier is going to also grow we're going to be adding rows in addition to that at the end of the Transformer there's this LM head layer which is a linear layer and you'll notice that that layer is used at the very end to produce the logits uh which become the probabilities for the next token in sequence and so intuitively we're trying to produce a probability for every single token that might come next at every point in time of that Transformer and if we have more and more tokens we need to produce more and more probabilities so every single token is going to introduce an additional dot product that we have to do here in this linear layer for this final layer in a Transformer so why can't vocap size be infinite why can't we grow to Infinity well number one your token embedding 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6318)~~  table is going to grow uh your linear layer is going to grow so we're going to be doing a lot more computation here because this LM head layer will become more computational expensive number two because we have more parameters we could be worried that we are going to be under trining some of these parameters so intuitively if you have a very large vocabulary size say we have a million uh tokens then every one of these tokens is going to come up more and more rarely in the training data because there's a lot more other tokens all over the place and so we're going to be seeing fewer and fewer examples uh for each individual token and you might be worried that basically the vectors associated with every token will be undertrained as a result because they just don't come up too often and they don't participate in the forward backward pass in addition to that as your vocab size grows you're going to start shrinking your sequences a lot right and that's really nice because that means that we're going to be attending to more and more text so 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6372)~~  that's nice but also you might be worrying that two large of chunks are being squished into single tokens and so the model just doesn't have as much of time to think per sort of um some number of characters in the text or you can think about it that way right so basically we're squishing too much information into a single token and then the forward pass of the Transformer is not enough to actually process that information appropriately and so these are some of the considerations you're thinking about when you're designing the vocab size as I mentioned this is mostly an empirical hyperparameter and it seems like in state-of-the-art architectures today this is usually in the high 10,000 or somewhere around 100,000 today and the next consideration I want to briefly talk about is what if we want to take a pre-trained model and we want to extend the vocap size and this is done fairly commonly actually so for example when you're doing fine-tuning for cha GPT um a lot more new special tokens get introduced on top of the base model to 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6426)~~  maintain the metadata and all the structure of conversation objects between a user and an assistant so that takes a lot of special tokens you might also try to throw in more special tokens for example for using the browser or any other tool and so it's very tempting to add a lot of tokens for all kinds of special functionality so if you want to be adding a token that's totally possible Right all we have to do is we have to resize this embedding so we have to add rows we would initialize these uh parameters from scratch to be small random numbers and then we have to extend the weight inside this linear uh so we have to start making dot products um with the associated parameters as well to basically calculate the probabilities for these new tokens so both of these are just a resizing operation it's a very mild model surgery and can be done fairly easily and it's quite common that basically you would freeze the base model you introduce these new parameters and then you only train these new parameters to introduce new tokens into 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6481)~~  the architecture um and so you can freeze arbitrary parts of it or you can train arbitrary parts of it and that's totally up to you but basically minor surgery required if you'd like to introduce new tokens and finally I'd like to mention that actually there's an entire design space of applications in terms of introducing new tokens into a vocabulary that go Way Beyond just adding special tokens and special new functionality so just to give you a sense of the design space but this could be an entire video just by itself uh this is a paper on learning to compress prompts with what they called uh gist tokens and the rough idea is suppose that you're using language models in a setting that requires very long prompts while these long prompts just slow everything down because you have to encode them and then you have to use them and then you're tending over them and it's just um you know heavy to have very large prompts so instead what they do here in this paper is they introduce new tokens and um imagine basically having a few new tokens you put them in 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6536)~~  a sequence and then you train the model by distillation so you are keeping the entire model Frozen and you're only training the representations of the new tokens their embeddings and you're optimizing over the new tokens such that the behavior of the language model is identical uh to the model that has a very long prompt that works for you and so it's a compression technique of compressing that very long prompt into those few new gist tokens and so you can train this and then at test time you can discard your old prompt and just swap in those tokens and they sort of like uh stand in for that very long prompt and have an almost identical performance and so this is one um technique and a class of parameter efficient fine-tuning techniques where most of the model is basically fixed and there's no training of the model weights there's no training of Laura or anything like that of new parameters the the parameters that you're training are now just the uh token embeddings so that's just one example but this could again be like an 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6593)~~  entire video but just to give you a sense that there's a whole design space here that is potentially worth exploring in the future the next thing I want to briefly address is that I think recently there's a lot of momentum in how you actually could construct Transformers that can simultaneously process not just text as the input modality but a lot of other modalities so be it images videos audio Etc and how do you feed in all these modalities and potentially predict these modalities from a Transformer uh do you have to change the architecture in some fundamental way and I think what a lot of people are starting to converge towards is that you're not changing the architecture you stick with the Transformer you just kind of tokenize your input domains and then call the day and pretend it's just text tokens and just do everything else identical in an identical manner so here for example there was a early paper that has nice graphic for how you can take an image and you can chunc at it into integers um and these sometimes uh so 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6645)~~  these will basically become the tokens of images as an example and uh these tokens can be uh hard tokens where you force them to be integers they can also be soft tokens where you uh sort of don't require uh these to be discrete but you do Force these representations to go through bottlenecks like in Auto encoders uh also in this paper that came out from open a SORA which I think really um uh blew the mind of many people and inspired a lot of people in terms of what's possible they have a Graphic here and they talk briefly about how llms have text tokens Sora has visual patches so again they came up with a way to chunc a videos into basically tokens when they own vocabularies and then you can either process discrete tokens say with autog regressive models or even soft tokens with diffusion models and uh all of that is sort of uh being actively worked on designed on and is beyond the scope of this video but just something I wanted to mention briefly okay now that we have come quite deep into the tokenization algorithm and we understand a lot more 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6707)~~  about how it works let's loop back around to the beginning of this video and go through some of these bullet points and really see why they happen so first of all why can't my llm spell words very well or do other spell related tasks so fundamentally this is because as we saw these characters are chunked up into tokens and some of these tokens are actually fairly long so as an example I went to the gp4 vocabulary and I looked at uh one of the longer tokens so that default style turns out to be a single individual token so that's a lot of characters for a single token so my suspicion is that there's just too much crammed into this single token and my suspicion was that the model should not be very good at tasks related to spelling of this uh single token so I asked how many letters L are there in the word default style and of course my prompt is intentionally done that way and you see how default style will be a single token so this is what the model sees so my suspicion is that it wouldn't be very good at this and indeed it is 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6771)~~  not it doesn't actually know how many L's are in there it thinks there are three and actually there are four if I'm not getting this wrong myself so that didn't go extremely well let's look look at another kind of uh character level task so for example here I asked uh gp4 to reverse the string default style and they tried to use a code interpreter and I stopped it and I said just do it just try it and uh it gave me jumble so it doesn't actually really know how to reverse this string going from right to left uh so it gave a wrong result so again like working with this working hypothesis that maybe this is due to the tokenization I tried a different approach I said okay let's reverse the exact same string but take the following approach step one just print out every single character separated by spaces and then as a step two reverse that list and it again Tred to use a tool but when I stopped it it uh first uh produced all the characters and that was actually correct and then It reversed them and that was correct once it had this so 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6833)~~  somehow it can't reverse it directly but when you go just first uh you know listing it out in order it can do that somehow and then it can once it's uh broken up this way this becomes all these individual characters and so now this is much easier for it to see these individual tokens and reverse them and print them out so that is kind of interesting so let's continue now why are llms worse at uh non-english langu and I briefly covered this already but basically um it's not only that the language model sees less non-english data during training of the model parameters but also the tokenizer is not um is not sufficiently trained on non-english data and so here for example hello how are you is five tokens and its translation is 15 tokens so this is a three times blow up and so for example anang is uh just hello basically in Korean and that end up being three tokens I'm actually kind of surprised by that because that is a very common phrase there just the typical greeting of like hello and that ends up being three tokens whereas our hello is a 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6899)~~  single token and so basically everything is a lot more bloated and diffuse and this is I think partly the reason that the model Works worse on other languages uh coming back why is LM bad at simple arithmetic um that has to do with the tokenization of numbers and so um you'll notice that for example addition is very sort of like uh there's an algorithm that is like character level for doing addition so for example here we would first add the ones and then the tens and then the hundreds you have to refer to specific parts of these digits but uh these numbers are represented completely arbitrarily based on whatever happened to merge or not merge during the tokenization process there's an entire blog post about this that I think is quite good integer tokenization is insane and this person basically systematically explores the tokenization of numbers in I believe this is gpt2 and so they notice that for example for the for um four-digit numbers you can take a look at whether it is uh a single token or whether it is two tokens that is a 1 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6962)~~  three or a 2 two or a 31 combination and so all the different numbers are all the different combinations and you can imagine this is all completely arbitrarily so and the model unfortunately sometimes sees uh four um a token for for all four digits sometimes for three sometimes for two sometimes for one and it's in an arbitrary uh Manner and so this is definitely a headwind if you will for the language model and it's kind of incredible that it can kind of do it and deal with it but it's also kind of not ideal and so that's why for example we saw that meta when they train the Llama 2 algorithm and they use sentence piece they make sure to split up all the um all the digits as an example for uh llama 2 and this is partly to improve a simple arithmetic kind of performance and finally why is gpt2 not as good in Python again this is partly a modeling issue on in the architecture and the data set and the strength of the model but it's also partially tokenization because as we saw here with the simple python example the encoding 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7023)~~  efficiency of the tokenizer for handling spaces in Python is terrible and every single space is an individual token and this dramatically reduces the context length that the model can attend to cross so that's almost like a tokenization bug for gpd2 and that was later fixed with gp4 okay so here's another fun one my llm abruptly halts when it sees the string end of text so here's um here's a very strange Behavior print a string end of text is what I told jt4 and it says could you please specify the string and I'm I'm telling it give me end of text and it seems like there's an issue it's not seeing end of text and then I give it end of text is the string and then here's a string and then it just doesn't print it so obviously something is breaking here with respect to the handling of the special token and I don't actually know what open ey is doing under the hood here and whether they are potentially parsing this as an um as an actual token instead of this just being uh end of text um as like individual sort of pieces of it without the special token 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7086)~~  handling logic and so it might be that someone when they're calling do encode uh they are passing in the allowed special and they are allowing end of text as a special character in the user prompt but the user prompt of course is is a sort of um attacker controlled text so you would hope that they don't really parse or use special tokens or you know from that kind of input but it appears that there's something definitely going wrong here and um so your knowledge of these special tokens ends up being in a tax surface potentially and so if you'd like to confuse llms then just um try to give them some special tokens and see if you're breaking something by chance okay so this next one is a really fun one uh the trailing whites space issue so if you come to playground and uh we come here to GPT 3.5 turbo instruct so this is not a chat model this is a completion model so think of it more like it's a lot more closer to a base model it does completion it will continue the token sequence so here's a tagline for ice cream shop and we want to continue the 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7152)~~  sequence and so we can submit and get a bunch of tokens okay no problem but now suppose I do this but instead of pressing submit here I do here's a tagline for ice cream shop space so I have a space here before I click submit we get a warning your text ends in a trail Ling space which causes worse performance due to how API splits text into tokens so what's happening here it still gave us a uh sort of completion here but let's take a look at what's happening so here's a tagline for an ice cream shop and then what does this look like in the actual actual training data suppose you found the completion in the training document somewhere on the internet and the llm trained on this data so maybe it's something like oh yeah maybe that's the tagline that's a terrible tagline but notice here that when I create o you see that because there's the the space character is always a prefix to these tokens in GPT so it's not an O token it's a space o token the space is part of the O and together they are token 8840 that's that's space o so what's What's 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7222)~~  Happening Here is that when I just have it like this and I let it complete the next token it can sample the space o token but instead if I have this and I add my space then what I'm doing here when I incode this string is I have basically here's a t line for an ice cream uh shop and this space at the very end becomes a token 220 and so we've added token 220 and this token otherwise would be part of the tagline because if there actually is a tagline here so space o is the token and so this is suddenly a of distribution for the model because this space is part of the next token but we're putting it here like this and the model has seen very very little data of actual Space by itself and we're asking it to complete the sequence like add in more tokens but the problem is that we've sort of begun the first token and now it's been split up and now we're out of this distribution and now arbitrary bad things happen and it's just a very rare example for it to see something like that and uh that's why we get the warning so the fundamental issue here is 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7289)~~  of course that um the llm is on top of these tokens and these tokens are text chunks they're not characters in a way you and I would think of them they are these are the atoms of what the LM is seeing and there's a bunch of weird stuff that comes out of it let's go back to our default cell style I bet you that the model has never in its training set seen default cell sta without Le in there it's always seen this as a single group because uh this is some kind of a function in um I'm guess I don't actually know what this is part of this is some kind of API but I bet you that it's never seen this combination of tokens uh in its training data because or I think it would be extremely rare so I took this and I copy pasted it here and I had I tried to complete from it and the it immediately gave me a big error and it said the model predicted to completion that begins with a stop sequence resulting in no output consider adjusting your prompt or stop sequences so what happened here when I clicked submit is that immediately the model 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7350)~~  emitted and sort of like end of text token I think or something like that it basically predicted the stop sequence immediately so it had no completion and so this is why I'm getting a warning again because we're off the data distribution and the model is just uh predicting just totally arbitrary things it's just really confused basically this is uh this is giving it brain damage it's never seen this before it's shocked and it's predicting end of text or something I tried it again here and it in this case it completed it but then for some reason this request May violate our usage policies this was flagged um basically something just like goes wrong and there's something like Jank you can just feel the Jank because the model is like extremely unhappy with just this and it doesn't know how to complete it because it's never occurred in training set in a training set it always appears like this and becomes a single token so these kinds of issues where tokens are either you sort of like complete the first character of the next token or you 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7407)~~  are sort of you have long tokens that you then have just some of the characters off all of these are kind of like issues with partial tokens is how I would describe it and if you actually dig into the T token repository go to the rust code and search for unstable and you'll see um en code unstable native unstable token tokens and a lot of like special case handling none of this stuff about unstable tokens is documented anywhere but there's a ton of code dealing with unstable tokens and unstable tokens is exactly kind of like what I'm describing here what you would like out of a completion API is something a lot more fancy like if we're putting in default cell sta if we're asking for the next token sequence we're not actually trying to append the next token exactly after this list we're actually trying to append we're trying to consider lots of tokens um that if we were or I guess like we're trying to search over characters that if we retened would be of high probability if that makes sense um so that we can actually add a single individual 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7472)~~  character uh instead of just like adding the next full token that comes after this partial token list so I this is very tricky to describe and I invite you to maybe like look through this it ends up being extremely gnarly and hairy kind of topic it and it comes from tokenization fundamentally so um maybe I can even spend an entire video talking about unstable tokens sometime in the future okay and I'm really saving the best for last my favorite one by far is the solid gold Magikarp and it just okay so this comes from this blog post uh solid gold Magikarp and uh this is um internet famous now for those of us in llms and basically I I would advise you to uh read this block Post in full but basically what this person was doing is this person went to the um token embedding stable and clustered the tokens based on their embedding representation and this person noticed that there's a cluster of tokens that look really strange so there's a cluster here at rot e stream Fame solid gold Magikarp Signet message like really 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7536)~~  weird tokens in uh basically in this embedding cluster and so what are these tokens and where do they even come from like what is solid gold magikarpet makes no sense and then they found bunch of these tokens and then they notice that actually the plot thickens here because if you ask the model about these tokens like you ask it uh some very benign question like please can you repeat back to me the string sold gold Magikarp uh then you get a variety of basically totally broken llm Behavior so either you get evasion so I'm sorry I can't hear you or you get a bunch of hallucinations as a response um you can even get back like insults so you ask it uh about streamer bot it uh tells the and the model actually just calls you names uh or it kind of comes up with like weird humor like you're actually breaking the model by asking about these very simple strings like at Roth and sold gold Magikarp so like what the hell is happening and there's a variety of here documented behaviors uh there's a bunch of tokens not just so good 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7598)~~  Magikarp that have that kind of a behavior and so basically there's a bunch of like trigger words and if you ask the model about these trigger words or you just include them in your prompt the model goes haywire and has all kinds of uh really Strange Behaviors including sort of ones that violate typical safety guidelines uh and the alignment of the model like it's swearing back at you so what is happening here and how can this possibly be true well this again comes down to tokenization so what's happening here is that sold gold Magikarp if you actually dig into it is a Reddit user so there's a u Sol gold Magikarp and probably what happened here even though I I don't know that this has been like really definitively explored but what is thought to have happened is that the tokenization data set was very different from the training data set for the actual language model so in the tokenization data set there was a ton of redded data potentially where the user solid gold Magikarp was mentioned in the text because solid gold Magikarp was a 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7659)~~  very common um sort of uh person who would post a lot uh this would be a string that occurs many times in a tokenization data set because it occurs many times in a tokenization data set these tokens would end up getting merged to the single individual token for that single Reddit user sold gold Magikarp so they would have a dedicated token in a vocabulary of was it 50,000 tokens in gpd2 that is devoted to that Reddit user and then what happens is the tokenization data set has those strings but then later when you train the model the language model itself um this data from Reddit was not present and so therefore in the entire training set for the language model sold gold Magikarp never occurs that token never appears in the training set for the actual language model later so this token never gets activated it's initialized at random in the beginning of optimization then you have forward backward passes and updates to the model and this token is just never updated in the embedding table that row Vector never gets sampled it 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7720)~~  never gets used so it never gets trained and it's completely untrained it's kind of like unallocated memory in a typical binary program written in C or something like that that so it's unallocated memory and then at test time if you evoke this token then you're basically plucking out a row of the embedding table that is completely untrained and that feeds into a Transformer and creates undefined behavior and that's what we're seeing here this completely undefined never before seen in a training behavior and so any of these kind of like weird tokens would evoke this Behavior because fundamentally the model is um is uh uh out of sample out of distribution okay and the very last thing I wanted to just briefly mention point out although I think a lot of people are quite aware of this is that different kinds of formats and different representations and different languages and so on might be more or less efficient with GPD tokenizers uh or any tokenizers for any other L for that matter so for example Json is actually 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7774)~~  really dense in tokens and yaml is a lot more efficient in tokens um so for example this are these are the same in Json and in yaml the Json is 116 and the yaml is 99 so quite a bit of an Improvement and so in the token economy where we are paying uh per token in many ways and you are paying in the context length and you're paying in um dollar amount for uh the cost of processing all this kind of structured data when you have to um so prefer to use theal over Json and in general kind of like the tokenization density is something that you have to um sort of care about and worry about at all times and try to find efficient encoding schemes and spend a lot of time in tick tokenizer and measure the different token efficiencies of different formats and settings and so on okay so that concludes my fairly long video on tokenization I know it's a try I know it's annoying I know it's irritating I personally really dislike the stage what I do have to say at this point is don't brush it off there's a lot of foot guns sharp edges here security issues uh AI 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7838)~~  safety issues as we saw plugging in unallocated memory into uh language models so um it's worth understanding this stage um that said I will say that eternal glory goes to anyone who can get rid of it uh I showed you one possible paper that tried to uh do that and I think I hope a lot more can follow over time and my final recommendations for the application right now are if you can reuse the GPT 4 tokens and the vocabulary uh in your application then that's something you should consider and just use Tech token because it is very efficient and nice library for inference for bpe I also really like the bite level BP that uh Tik toen and openi uses uh if you for some reason want to train your own vocabulary from scratch um then I would use uh the bpe with sentence piece um oops as I mentioned I'm not a huge fan of sentence piece I don't like its uh bite fallback and I don't like that it's doing BP on unic code code points I think it's uh it also has like a million settings and I think there's a lot of foot gonss here and I think it's 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7900)~~  really easy to Mis calibrate them and you end up cropping your sentences or something like that uh because of some type of parameter that you don't fully understand so so be very careful with the settings try to copy paste exactly maybe where what meta did or basically spend a lot of time looking at all the hyper parameters and go through the code of sentence piece and make sure that you have this correct um but even if you have all the settings correct I still think that the algorithm is kind of inferior to what's happening here and maybe the best if you really need to train your vocabulary maybe the best thing is to just wait for M bpe to becomes as efficient as possible and uh that's something that maybe I hope to work on and at some point maybe we can be training basically really what we want is we want tick token but training code and that is the ideal thing that currently does not exist and MBP is um is in implementation of it but currently it's in Python so that's currently what I have to say for uh tokenization there 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7958)~~  might be an advanced video that has even drier and even more detailed in the future but for now I think we're going to leave things off here and uh I hope and uh they increase this contact size from gpt1 of 512 uh to 1024 and GPT 4 two the next okay next I would like us to briefly walk through the code from open ATP I'm sorry I'm gonna sneeze and then what's Happening Here is this is a spous layer that I will explain in a 
# ru
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=0)~~  Привет всем, в этом видео я хотел бы, чтобы мы рассмотрели процесс токенизации в больших языковых моделях. Теперь вы видите, что у меня есть определенное лицо, и это потому, что токенизация - моя наименее любимая часть работы с большими языковыми моделями, но, к сожалению, это так. необходимо разобраться в некоторых деталях, потому что это довольно сложно, и есть много скрытых пушек, о которых следует знать, и много странностей с большими языковыми моделями обычно связаны с токенизацией, так что же такое токенизация в моем предыдущем видео? Давайте  Создайте GPT с нуля. На самом деле мы уже провели токенизацию, но мы сделали очень наивную и простую версию токенизации, поэтому, когда вы зайдете в Google Colab для просмотра этого видео, вы видите здесь, что мы загрузили наш обучающий набор, и наш обучающий набор был таким, э-э, Шекспир, э-э  набор данных сейчас в начале набор данных Шекспира - это просто большая строка в Python, это просто текст, и поэтому вопрос в том, как нам подключить текст к большим языковым моделям, и в этом случае 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=58)~~  здесь мы создали словарь из 65 возможных символов, которые мы видели  в этой строке встречаются возможные символы, и мы увидели, что их 65, а затем мы создали таблицу поиска для преобразования каждого возможного символа небольшого фрагмента строки в токен целого числа, поэтому здесь, например, мы маркировали строку High там  и мы получили эту последовательность токенов, и здесь мы взяли первые 1000 символов нашего набора данных и закодировали их в токены, и поскольку это уровень символов, мы получили 1000 токенов в последовательности, поэтому токен 18 47 и т. д. теперь позже мы увидели, что  мы подключаем эти токены к языковой модели с помощью таблицы внедрения, и поэтому, по сути, если у нас есть 65 возможных токенов, тогда эта таблица внедрения будет иметь 65 строк, и, грубо говоря, мы берем целое число, связанное с каждым отдельным знаком Le  токен, который мы используем для поиска в этой таблице, и выбираем соответствующую строку, и эта строка представляет собой обучаемые параметры, которые мы собираемся обучить с помощью обратного 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=130)~~  распространения, и это вектор, который затем подается в  Transformer гм, и именно так Transformer Ser воспринимает каждый токен, поэтому здесь у нас был очень наивный процесс токенизации, который представлял собой токенизатор уровня символов, но на практике в современных языковых моделях люди используют гораздо более сложные схемы, к сожалению, для  создавая эти словари токенов, поэтому мы не имеем дело с уровнем символов, мы имеем дело с уровнем фрагментов, и способ построения этих фрагментов символов заключается в использовании таких алгоритмов, как, например, пара bik в алгоритме кодирования, который мы собираемся использовать.  подробнее обсудим это в этом видео. Я хотел бы кратко показать вам документ, в котором представлено кодирование на уровне битов как механизм токенизации в контексте больших языковых моделей, и я бы сказал, что это, вероятно, документ gpt2, и если вы прокручиваете здесь до входного представления раздела, здесь они описывают токенизацию тех типов свойств, которые вы хотите, чтобы токенизация имела, и здесь они приходят к выводу, 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=193)~~  что у них будет токенизатор, словарный запас которого будет равен 50,2.  57 возможных токенов, а размер контекста составит 1,24 токена, поэтому на входном уровне внимания нейронной сети Transformer каждый отдельный токен обслуживает предыдущие токены в последовательности, и он будет видеть до 1,24.  токены, так что токены - это как фундаментальная единица, гм, атом больших языковых моделей, если хотите, и все находится в единицах токенов, все дело в токенах, а токенизация - это процесс перевода строк или текста в последовательности токенов, и наоборот, когда вы  зайдите также в статью Llama 2, я могу показать вам, что когда вы ищете токен, вы получите 63 совпадения, и это потому, что токены снова широко распространены, поэтому здесь они упомянули, что они обучались на двух триллионах токенов данных и так далее.  Итак, мы собираемся создать наш собственный токенизатор, к счастью, алгоритм кодирования укусов не такой уж и сложный, и мы можем сами создать его с нуля, и мы посмотрим, как именно это работает, прежде чем мы углубимся в код, который 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=261)~~  я хотел бы дать  Вкратце о некоторых сложностях, связанных с токенизацией, потому что я просто хочу убедиться, что мы достаточно мотивируем это, почему мы все это делаем и почему это так грубо, поэтому токенизация лежит в основе многих странностей.  в больших языковых моделях, и я бы посоветовал вам не отмахиваться от многих проблем, которые могут выглядеть как проблемы с новой сетевой архитектурой или самой большой языковой моделью, на самом деле являются проблемами токенизации и, по сути, прослеживаются до нее.  поэтому, если вы заметили какие-либо проблемы с большими языковыми моделями, разве вы не знаете, что не можете очень легко выполнять задачи по правописанию, обычно это происходит из-за токенизации, простая обработка строк может быть затруднительна для выполнения большой языковой модели в исходном виде, э-э, неанглийские языки могут работают намного хуже, и в значительной степени это связано с токенизацией, иногда llms плохо справляется с простой арифметикой, а также можно отследить токенизацию, ну, в частности, у gbt2 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=318)~~  было бы немного больше проблем с Python, чем у будущих его версий, из- за токенизации есть  много других проблем, возможно, вы видели странные предупреждения о конце пробелов, это проблема токенизации, если бы вы раньше спросили GPT о Magikarp из чистого золота и о том, что это такое, вы бы увидели, что фильм сошел с ума и начал бы идти  не по поводу совершенно не связанной с этим темы, может быть, вам сказали использовать yl вместо Json в структурных данных, все это связано с токенизацией, так что, по сути, токенизация лежит в основе многих проблем, я вернусь к этим в конце  видео, но сейчас позвольте мне просто пропустить его немного и перейдем к этому веб-приложению, токенизатору Tik bell.app, так что я загрузил его здесь, и что мне нравится в этом веб-приложении, так это то, что токенизация выполняется своего рода жить в вашем браузере на JavaScript, так что вы можете просто набрать здесь hello world и всю строку rokenes, поэтому здесь то, что мы видим слева, это строка, которую вы вставляете справа, в настоящее время мы используем 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=382)~~  токенизатор gpt2, который мы видим  что эта строка, которую я вставил сюда, в настоящее время разбивается на 300 токенов, и здесь они как бы показаны явно разными цветами для каждого отдельного токена, так что, например, токенизация этого слова стала двумя токенами, токен 1634, пространство токенов составляет  токен 318, поэтому будьте осторожны, внизу вы можете показать пробелы, и имейте в виду, что здесь есть пробелы и символы новой строки, но вы можете скрыть их для ясности. 262 И т. д., вы заметили, что пробел теперь является частью этого фрагмента токена, так что это примерно похоже на то, как наше английское предложение разбилось, и теперь все кажется хорошо, теперь я ввожу некоторые арифметические действия, и мы видим, что токен 127 Плюс, а затем токен шесть, пробел 6, за которым следует 77, так что здесь происходит то, что 127 подается как один токен в большую языковую модель, но номер 677 на самом деле будет подавать как два отдельных токена, и поэтому большая языковая модель должен как бы принять это во внимание 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=471)~~  и правильно обработать в своей сети, и вот здесь 804 будет разбит на два токена, и все это совершенно произвольно, и вот у меня есть еще один пример четырехзначных чисел, и они разбиваются на  способ их распада, и это совершенно произвольно иногда у вас есть один токен из нескольких цифр иногда у вас есть отдельные цифры столько же токенов, и все это довольно произвольно и выходит из токенизатора, вот еще один пример, у нас есть строковое яйцо, и вы видите здесь  что это стало двумя токенами, но по какой-то причине, когда я говорю, что у меня есть яйцо, вы видите, когда это космическое яйцо, это два токена, извините, это один токен, поэтому просто яйцо в начале предложения представляет собой два токена, но здесь как  пространство яйцо внезапно становится одним токеном, э-э, для одной и той же строки, хорошо, здесь яйцо в нижнем регистре оказывается одним токеном, и, в частности, обратите внимание, что цвет другой, поэтому это другой токен, поэтому он чувствителен к регистру и, конечно, яйцо с заглавной буквы  также будут разные 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=535)~~  токены, и опять же, это будут два токена произвольно, поэтому для одного и того же концептуального яйца в зависимости от того, находится ли оно в начале предложения, в конце предложения, в нижнем регистре, в верхнем регистре или в смешанном виде, все это будет, ну, по сути, очень разные токены  и разные идентификаторы, и языковая модель должна учиться на необработанных данных из всего интернет-текста, на котором она будет обучаться, что на самом деле все это одна и та же концепция, и она должна как бы группировать их по параметрам нейронной сети и просто основываясь на шаблонах данных, я понимаю, что все они очень похожи, но, может быть, не почти совсем похожи, но очень очень похожи, гм, после демонстрации EG, у меня есть введение от «открой глаза», чбт на корейском, так что манасо Панг, э-э и т. д., так вот это  на корейском языке, и причина, по которой я разместил это здесь, заключается в том, что вы заметите, что неанглийские языки работают немного хуже в чачи, отчасти это связано с тем, что, конечно, набор обучающих данных 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=596)~~  для чачи намного больше для английского и для всего остального, кроме  то же самое верно не только для самой большой языковой модели, но и для токенизатора, поэтому, когда мы обучаем токенизатор, мы увидим, что есть и обучающий набор, и английского языка намного больше, чем неанглийского, и что в итоге получится происходит то, что у нас будет намного больше длинных токенов для английского языка, так как бы это сказать, если у вас есть одно предложение на английском языке и вы его токенизируете, вы можете увидеть, что это 10 токенов или что-то в этом роде, но если вы переведете это  Если перевести предложение, скажем, на корейский , японский или что-то еще, вы обычно увидите, что количество используемых токенов намного больше, и это потому, что фрагменты здесь намного более разбиты, поэтому мы используем гораздо больше токенов для одного и того же, и  это приводит к увеличению длины последовательности всех документов, поэтому вы используете больше токенов, а затем в центре внимания Преобразователя, когда эти токены пытаются 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=650)~~  присоединиться друг к другу, у вас заканчивается контекст, гм, при максимальной длине контекста. этого Трансформера, и поэтому в основном весь неанглоязычный текст растянут с точки зрения Трансформера, и это просто связано с тренировками, которые использовались для токенизатора, и самой токенизацией, поэтому он создаст гораздо большие токены и  намного больше групп на английском языке, и у него будет много маленьких границ для всего остального неанглийского текста, поэтому, если мы переведем это на английский, то будет значительно меньше токенов. Последний пример, который у меня есть, — это небольшой фрагмент Python для выполнения  FS buuz, и я хотел бы, чтобы вы заметили, что все эти отдельные пробелы являются отдельными токенами, они являются токенами 220, так что 220 220 220 220, а затем пробел, если это один токен, и вот что здесь происходит, когда Трансформер собирается использовать или попытаться создать этот текст, ему нужно обработать все эти пробелы индивидуально, они все по одному вводятся во весь Трансформер в последовательности, и поэтому это 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=719)~~  чрезвычайно расточительно маркировать его таким образом, и поэтому, как  В результате gpt2 не очень хорошо работает с Python, и это не имеет ничего общего с кодированием или самой языковой моделью, просто если он использует много отступов с использованием пространства в Python, как мы обычно делаем, вы просто в конечном итоге раздуваете все текст, и он разделен на слишком большую часть последовательности, и у нас заканчивается длина контекста в последовательности, ну, грубо говоря, что происходит, мы слишком расточительны, мы занимаем слишком много места для токенов, теперь мы можем  также прокрутите здесь вверх, и мы можем изменить токенизатор, поэтому обратите внимание, что токенизатор gpt2 создает количество токенов 300 для этой строки. Здесь мы можем изменить его на базу CL 100K, которая является GPT для токенизатора, и мы видим, что количество токенов падает до 185.  Итак, для одной и той же строки мы теперь примерно имеем количество токенов, и, грубо говоря, это потому, что количество токенов в токенизаторе GPT 4 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=774)~~  примерно вдвое превышает количество токенов в токенизаторе gpt2, поэтому мы пошли примерно от  От 50 000 до примерно 100 000, теперь вы можете себе представить, что это хорошо, потому что один и тот же текст теперь разделен на половину меньшего количества токенов, так что это намного более плотный ввод в Transformer, а в Transformer каждый отдельный токен имеет конечное количество токенов.  перед этим он будет обращать внимание, и поэтому мы можем видеть примерно в два раза больше текста, чем контекста для того, какой токен предсказать следующим, э-э, из-за этого изменения, но, конечно, просто увеличивая количество токенов  не является ли это строго лучшим бесконечно, потому что по мере того, как вы увеличиваете количество токенов, ваша таблица встраивания становится намного больше, а также на выходе мы пытаемся предсказать следующий токен, и там есть Soft Max, и он растет по мере того, как  ну, мы собираемся более подробно рассказать об этом позже, но где-то есть своего рода золотая середина, где у вас есть правильное количество токенов в вашем 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=832)~~  словаре, где все достаточно плотно и по-прежнему достаточно эффективно, а теперь я хотел бы, чтобы вы особенно для токенизатора gp4 следует отметить, что обработка пробелов для Python значительно улучшилась. Вы видите, что здесь эти четыре пробела представлены как один токен для трех пробелов здесь, а затем SPF токена, и здесь все семь пробелов были сгруппированы в один токен, поэтому мы гораздо более эффективно представляем Python, и это был осознанный выбор, сделанный open aai, когда они разработали токенизатор gp4, и они группируют гораздо больше места в один символ, что это делает:  это уплотняет Python, и поэтому мы можем уделять больше внимания большему количеству кода перед ним, когда пытаемся предсказать следующий токен в последовательности, и поэтому улучшение возможностей кодирования Python с gbt2 на gp4 — это не просто вопрос языковой модели и  архитектура и детали оптимизации, но многие улучшения здесь также связаны с дизайном токенизатора и тем, как он группирует символы в токены, хорошо, давайте теперь начнем писать код, 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=899)~~  так что помните, что мы хотим сделать, мы хотим взять строки  и передать их в языковые модели, для этого нам нужно каким-то образом разбить строки на некоторые целые числа в некотором фиксированном словаре, а затем мы будем использовать эти целые числа для поиска в справочной таблице векторов и передать эти векторы в преобразователь в качестве входных данных сейчас.  причина, по которой это становится немного сложнее, конечно, заключается в том, что мы не просто хотим поддерживать простой английский алфавит, мы хотим поддерживать различные виды языков, поэтому это ананго на корейском языке, и это привет, и мы также хотим поддерживать множество видов языков. специальные символы, которые мы можем найти в Интернете, например, Emoji, так как же нам передать этот текст в Emoji, так как же нам передать этот текст в Трансформеры, ну, а что это за текст в Python, поэтому, если вы перейдете к документации по строке в Python, вы увидите, что строки  неизменяемые последовательности кодовых точек Unicode, хорошо, что такое кодовые точки Unicode, 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=958)~~  мы можем перейти к PDF, чтобы кодовые точки Unicode были определены Консорциумом Unicode как часть стандарта Unicode, и на самом деле это просто определение примерно из 150 000 символов прямо сейчас  и, грубо говоря, как они выглядят и какие целые числа обозначают эти символы, так что на данный момент там написано 150 000 символов в 161 сценарии, поэтому, если вы прокрутите здесь вниз, вы увидите, что стандарт очень жив, последний стандарт 15.1 в сентябре 2023 года и  по сути, это просто способ определить множество типов символов, таких как, например, все эти символы в разных сценариях, поэтому мы можем получить доступ к точке кода уникального кода, заданной одним символом, с помощью функции или в Python, поэтому, например, я могу передать  в порядке H, и я вижу, что для одиночного символа H кодовая точка уникального кода равна 104, окей, но это может быть сложно, поэтому мы можем взять, например, наш Emoji здесь, и мы видим, что кодовая точка для этого: 128 000 или мы можем взять un, и это 50 000, теперь имейте в виду, что 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1034)~~  вы не можете вставлять сюда строки, потому что у вас нет ни одной кодовой точки, он принимает только один символ точки кода uni-кода и сообщает вам его целое число, поэтому  таким образом мы можем найти все символы um этой конкретной строки и их кодовые точки, или X forx в этой строке, и мы получаем эту кодировку здесь, теперь смотрите здесь, мы уже повернули, необработанные кодовые точки уже имеют целые числа, так почему  не можем ли мы просто использовать эти целые числа и вообще не иметь никакой токенизации, почему мы не можем просто использовать это изначально как есть и просто использовать код? Одна из причин этого, конечно, заключается в том, что словарь в этом случае будет довольно  долго, в данном случае для Unicode это словарь из 150 000 различных кодовых точек, но что еще более тревожно, я думаю, что стандарт Unicode очень жив и постоянно меняется, поэтому это не обязательно является своего рода стабильным представлением, которое мы можем захотеть  использовать напрямую, поэтому по этим причинам нам нужно что-то немного лучше, 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1096)~~  поэтому, чтобы найти что-то лучше, мы обращаемся к кодировкам, поэтому, если мы перейдем на страницу Википедии здесь, мы увидим, что консорциум Unicode определяет три типа кодировок utf8 UTF 16 и UTF 32, эти кодировки  способ, с помощью которого мы можем взять текст Unicode и преобразовать его в двоичные данные или с помощью потоков utf8, безусловно, является наиболее распространенным, так что это страница utf8, теперь эта страница Википедии на самом деле довольно длинная, но для наших целей важно то, что utf8 занимает  каждую точку Cod, и он преобразует ее в поток, и этот поток имеет размер от одного до четырех байтов, так что это кодировка переменной длины, поэтому в зависимости от точки Unicode в соответствии со схемой вы получите от 1 до четырех байтов для каждой кодовой точки, кроме того, есть utf8, uh utf16 и UTF 32. UTF 32 хорош, потому что имеет фиксированную длину, а не переменную длину, но у него также есть много других недостатков, поэтому полный спектр плюсов и минусов всех этих три разные кодировки выходят за рамки 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1160)~~  этого видео. Я просто хотел бы отметить, что мне понравился этот блок-пост, и этот блок- пост в конце также содержит ряд ссылок, которые могут быть весьма полезны, э-э, одна из них - э-э, везде utf8 Манифест, хм, и этот Манифест описывает причину, по которой utf8 значительно предпочтительнее и намного лучше, чем другие кодировки, и почему он используется гораздо более широко, хм, в Интернете. Одно из основных преимуществ, просто чтобы дать вам представление, заключается в том, что utf8  единственный из них, который обратно совместим с гораздо более простой кодировкой текста asky, но я не буду вдаваться в подробности в этом видео, поэтому достаточно сказать, что нам нравится кодировка utf8, и давайте попробуем взять string и посмотрим, что мы получим, если мы закодируем в utf8, класс строк в Python на самом деле имеет do encode, и вы можете дать ему кодировку, скажем, utf8, теперь мы выходим из этого, не очень приятно, потому что это байты - это объект байтов  и это не очень приятно в том виде, в каком оно печатается, поэтому лично мне нравится проходить его 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1225)~~  через список, потому что тогда мы фактически получаем необработанный B этой кодировки, так что это необработанные прощания, которые представляют эту строку в соответствии с кодировкой utf8, которую мы можем также посмотрите на utf16, мы получаем немного разные потоки, и здесь мы начинаем видеть один из недостатков utf16, вы видите, что у нас ноль Z что-то Z что-то Z что-то, мы начинаем понимать, что это немного расточительная кодировка, и действительно для простых символов asky или английских символов здесь, у нас просто есть структура 0 что-то Z что-то, и это не совсем хорошо, то же самое для UTF 32, когда мы расширяем это, мы можем начать понимать расточительность этой кодировки  для наших целей вы видите много нулей, за которыми следует что-то, и поэтому это нежелательно, поэтому достаточно сказать, что мы хотели бы придерживаться utf8 для наших целей, однако, если мы просто будем использовать utf8 наивно, это будут потоки, так что это будет означать  длина словаря всего 256 возможных токенов, но этот размер словаря очень-очень мал, 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1295)~~  если бы мы просто использовали его наивно, то весь наш текст растянулся бы на очень-очень длинные последовательности байтов и  Итак, что это значит, так это то, что, безусловно, таблица внедрения будет крошечной, а прогноз наверху на последнем слое будет очень маленьким, но наши последовательности очень длинные, и помните, что у нас довольно конечная длина контекста и  внимание, которое мы можем поддерживать в преобразователе по вычислительным причинам, и поэтому у нас есть только такая же длина контекста, но теперь у нас очень-очень длинные последовательности, и это просто неэффективно и не позволит нам уделить внимание достаточно длинному тексту перед нами  для целей следующей задачи прогнозирования токена, поэтому мы не хотим использовать необработанные байты кодировки utf8, мы хотим иметь возможность поддерживать больший размер словаря, который мы можем настроить как гипер, но мы хотим придерживаться кодировки utf8  из этих строк, так что же мы делаем хорошо, ответ, конечно, заключается в том, что мы обращаемся 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1355)~~  к алгоритму кодирования битовых пар, который позволит нам сжимать эти битовые последовательности до переменной величины, так что мы доберемся до этого немного позже, но я просто хочу  вкратце скажу о том факте, что мне бы не хотелось ничего больше, чем иметь возможность вводить необработанные последовательности фрагментов в э-э, языковые модели, на самом деле есть статья о том, как это потенциально можно сделать, э-э, летом прошлого года, проблема в том, что у вас действительно есть  Чтобы войти, вам придется изменить архитектуру Трансформера, потому что, как я уже упоминал, у вас возникнет проблема, когда внимание начнет становиться чрезвычайно дорогим, потому что последовательности очень длинные, и поэтому в этой статье они предлагают своего рода иерархическую структуризацию. Трансформера, который мог бы позволить вам просто подавать необработанные фрагменты, и поэтому в конце они говорят, что вместе эти результаты доказывают жизнеспособность авторегрессионного моделирования последовательностей без токенизации 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1405)~~  в масштабе, поэтому отсутствие токенизации действительно было бы потрясающе, мы бы просто подали потоки B непосредственно в  наши модели, но, к сожалению, я не знаю, действительно ли это уже было доказано достаточно большим количеством групп и в достаточном масштабе, ну, но что-то подобное в какой-то момент было бы потрясающе, и я надеюсь, что кто-то это придумает, но сейчас нам придется  вернитесь, и мы не можем передать это непосредственно в языковые модели, и нам приходится сжимать его, используя алгоритм кодирования B paare, поэтому давайте посмотрим, как это работает, и, как я уже упоминал, алгоритм кодирования B paare не так уж и сложен, и страница Википедии на самом деле весьма поучительно, поскольку основная идея заключается в том, что мы делаем, это некая входная последовательность, например, здесь у нас есть только четыре элемента в нашем словаре a b c и d, и у нас есть их последовательность, поэтому вместо этого у нас есть только четыре элемента в нашем словаре a b c и d  байтов, скажем, у нас всего 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1450)~~  четыре, размер словаря равен четырем, последовательность слишком длинная, и мы хотели бы сжать ее, поэтому мы итеративно находим пару токенов, которые встречаются наиболее часто, а затем, как только мы  Мы определили эту пару, и мы заменили эту пару только одним новым токеном, который мы добавили в наш словарь, поэтому, например, здесь чаще всего встречается битовая пара AA, поэтому мы создаем новый токен, назовем его заглавной Z, и мы заменяем каждое отдельное вхождение  AA через Z, теперь у нас есть две Z, поэтому мы взяли последовательность из 11 символов с размером словаря четыре и преобразовали ее в последовательность из девяти токенов, но теперь со словарем из пяти, потому что у нас есть пятый словарь. элемент, который мы только что создали, и это Z означает объединение AA, и мы можем снова повторить этот процесс, чтобы снова посмотреть на последовательность и определить пару токенов, которые встречаются наиболее часто, скажем, что теперь это AB, что ж, мы собираемся заменить  AB с новым токеном, который мы имели в виду, вызовите Y, чтобы y 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1524)~~  стал ab, а затем каждое вхождение ab теперь заменяется на y, и в итоге мы получаем это, так что теперь у нас есть только 1 2 3 4 5 6 семь символов в нашей последовательности, но мы  у нас не просто четыре или пять словарных элементов, а теперь у нас их шесть, и в последнем раунде мы снова просматриваем последовательность, находим, что фраза zy или пара zy наиболее распространены, и заменяем ее еще раз другим символом um, скажем, x  Итак, X — это z y, и мы заменяем все проклятия на zy, и мы получаем следующую последовательность, так что, по сути, после того, как мы прошли этот процесс, вместо того, чтобы иметь последовательность um из 11 токенов uh с длиной словаря, равной четырем, у нас теперь есть последовательность 1 2  3 четыре пять токенов, но длина нашего словаря теперь равна семи, и таким образом мы можем итеративно сжимать нашу последовательность. сейчас мы пройдемся по ним и найдем пары битов, которые встречаются чаще всего, и начнем итеративно создавать новые токены, добавляя их в наш словарь и заменяя элементы, и таким образом мы в конечном итоге 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1609)~~  получим  сжатый набор обучающих данных, а также алгоритм для взятия любой произвольной последовательности и ее кодирования с использованием этого словаря, а также декодирования ее обратно в строки, так что давайте теперь реализуем все это, и вот что я сделал. Я перешел к этому блок-посту, который мне понравился, и взял  первый абзац, и я скопировал и вставил его сюда в текст, так что это одна очень длинная строка, чтобы получить токены, как я уже говорил, мы просто берем наш текст и кодируем его в utf8, токены здесь на этом этапе будут одиночными необработанными фрагментами поток байтов, и просто для того, чтобы с ним было легче работать, а не просто с объектом байтов, я собираюсь преобразовать все эти байты в целые числа, а затем создать их список, чтобы нам было легче манипулировать и работать с ним в Python и  визуализируйте, и вот я печатаю все это, так что это оригинал, гм, это исходный абзац, и его длина составляет 533 э-э, кодовые точки, а затем вот байты, закодированные в ut utf8, и мы видим, что это имеет длину 616 байт  на данный 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1676)~~  момент или 616 токенов, и причина, по которой это больше, заключается в том, что многие из этих простых символов asky или простых символов просто становятся одним байтом, но многие из этих более сложных символов Unicode становятся несколькими байтами до четырех, и поэтому мы расширяем  такого размера, поэтому теперь в качестве первого шага алгоритма мы хотели бы выполнить итерацию и найти пару фрагментов, которые встречаются наиболее часто, потому что затем мы собираемся объединить их, поэтому, если вы  долго работая над блокнотом на стороне, тогда я советую вам просто щелкнуть ссылку, найти этот блокнот и попытаться написать эту функцию самостоятельно, иначе я приду сюда и сначала реализую функцию, которая находит наиболее распространенную пару, хорошо, так что вот  то, что я придумал, есть много разных способов реализовать это, но я вызываю функцию get stats, она ожидает список целых чисел, я использую словарь, чтобы отслеживать в основном подсчеты, а затем это питонический способ итерации последовательные элементы этого списка, 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1731)~~  которые мы рассмотрели в предыдущем видео, а здесь я просто отслеживаю увеличение на одну единицу для всех пар, поэтому, если я вызову это для всех токенов здесь, статистика выйдет здесь, так что это  это словарь, ключи - это опрокидывание последовательных элементов, а это счетчик, так что просто чтобы распечатать его немного лучше, это один из способов, который мне нравится делать, где вы здесь немного составные, поэтому вы можете сделать паузу, если  вам нравится, но мы перебираем все элементы, которые элементы, вызываемые в словаре, возвращают пары значений ключа, и вместо этого я создаю здесь список ключей значений, потому что, если это список ключей значений, я могу вызвать сортировку по нему, и по умолчанию Python будет использовать первый элемент, который в этом случае будет значением для сортировки, если ему заданы, а затем обратный, так что он убывает и печатает, так что в основном это выглядит как 101 запятая 32 была наиболее часто встречающейся последовательной парой, и это произошло 20 раз, мы можем дважды проверить 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1798)~~  это имеет разумный смысл, поэтому, если я просто найду 10132, вы увидите, что это 20 вхождений этой пары ум, и если мы хотим взглянуть на то, что именно представляет собой эта пара, мы можем использовать Char, который является противоположностью или  в Python, поэтому мы присваиваем ему уникальный код Cod, то есть 101 и 32, и мы видим, что это e и пробел, так что в основном здесь много пробелов E, что означает, что многие из этих слов, похоже, заканчиваются на e, так что вот eace  в качестве примера, здесь происходит много всего, и это самая распространенная пара, и теперь, когда мы определили наиболее распространенную пару, которую мы хотели бы перебрать по этой последовательности, мы собираемся выпустить новый токен с идентификатором  из 256, потому что эти токены в настоящее время изменяются от Z до 255, поэтому, когда мы создаем новый токен, он будет иметь идентификатор 256, и мы собираемся перебирать весь этот список, и каждый раз, когда мы видим 101 запятую 32, мы  собираюсь заменить это на 256, так что давайте реализуем это сейчас и 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1867)~~  не стесняйтесь делать это сами, поэтому сначала я прокомментировал это просто для того, чтобы мы не слишком сильно загрязняли блокнот, это хороший способ в Python получить  пара с самым высоким рейтингом, поэтому мы в основном вызываем Max в этой статистике словаря, и это вернет максимальный ключ, а затем вопрос в том, как он ранжирует ключи, чтобы вы могли предоставить ему функцию, которая ранжирует ключи, и эта функция - это просто статистика.  получите статистику. getet в основном возвращает значение, поэтому мы ранжируем по значению и получаем максимальный ключ, поэтому это 101 запятая 32, как мы видели сейчас, чтобы фактически объединить 10132 гм, это функция, которую я написал, но, опять же, существует много разных ее версий  поэтому мы возьмем список идентификаторов и пару, которую мы хотим заменить, и эта пара будет заменена новым индексом idx, поэтому, перебирая идентификаторы, если мы найдем пару, замените ее на idx, чтобы мы создали этот новый  список, а затем мы начинаем с нуля, а затем последовательно проходим весь этот список слева 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1935)~~  направо, и здесь мы проверяем равенство в текущей позиции с равенство в текущей позиции с парой, гм, так что здесь мы проверяем, что пара совпадает, теперь вот это немного сложное условие, которое вам нужно добавить, если вы пытаетесь быть осторожным, и оно заключается в том, что вы не хотите, чтобы это здесь выходило за пределы в самой последней позиции, когда вы находитесь в самом правом элементе этого списка, в противном случае  это приведет к ошибке autof borders, поэтому мы должны убедиться, что мы не находимся на самом последнем элементе, так что это будет неверно, поэтому, если мы найдем совпадение, мы добавим в этот новый список этот индекс замены и  мы увеличиваем позицию на два, поэтому пропускаем всю эту пару, но в противном случае, если мы не нашли подходящую пару, мы просто копируем элемент um в этой позиции и увеличиваем на единицу, а затем возвращаем это, так что вот очень маленькая игрушка например, если у нас есть список 566 791, и мы хотим заменить вхождения 67 на 99, то вызов этого даст нам то, что мы просим, ​​поэтому здесь 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=1999)~~  67 заменяется на 99, так что теперь я собираюсь раскомментировать это для нашего фактического варианта использования, когда мы хотим взять наши токены, мы хотим взять здесь верхнюю пару и заменить ее на 256, чтобы получить токены. Если мы запустим это, мы получим следующее, поэтому напомним, что ранее у нас была длина 616 в этом списке и  теперь у нас есть длина 596, так что она уменьшилась на 20, что имеет смысл, потому что есть 20 вхождений, более того, мы можем попытаться найти 256 здесь, и мы видим много вхождений вне этого, и, кроме того, просто дважды проверьте, не должно быть вхождений 10132, поэтому  это исходный массив, их много, а во втором массиве нет вхождений 1032, поэтому мы успешно объединили эту единственную пару, и теперь мы просто повторяем это, поэтому мы собираемся снова просмотреть последовательность и найти наиболее распространенную пару  и замените его, так что позвольте мне теперь написать цикл y, который использует эти функции для выполнения этого типа итеративно, и сколько раз мы делаем 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2064)~~  это четыре раза, это полностью зависит от нас как гиперпараметр, чем больше шагов мы предпримем, тем больше будет  наш словарный запас, и чем короче будет наша последовательность, и есть некоторая золотая середина, которая, как мы обычно находим, лучше всего работает на практике, и это своего рода гиперпараметр, и мы настраиваем его и находим хорошие размеры словаря, например, gp4 в настоящее время использует примерно 100 000  токены и хм bpark, что на данный момент это разумные числа, а не большие языковые модели, поэтому позвольте мне теперь написать, э-э, собрать все это вместе и э-э повторить эти шаги, хорошо, теперь, прежде чем мы углубимся в цикл Y, я хотел добавить сюда еще одну ячейку, где Я подошел к блок-посту и вместо того, чтобы взять только первый абзац или два, я взял весь блок-пост и растянул его в одну строку, и, по сути, простое использование более длинного текста позволит нам получить более репрезентативную статистику для пар фрагментов и  мы просто получим от этого более разумные результаты, 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2118)~~  потому что это более длинный текст, поэтому здесь у нас есть необработанный текст, мы кодируем его в байты, используя кодировку utf8, а затем здесь, как и раньше, мы просто меняем его на список целых чисел в Python  просто для того, чтобы с ними было легче работать, а не с необработанными объектами byes, а затем это код, который я придумал, чтобы фактически выполнить слияние в цикле, эти две функции здесь идентичны тем, что у нас было выше, я включил их сюда только так  что у вас есть точка отсчета, так что эти два идентичны, а затем это новый код, который я добавил, поэтому первое, что мы хотим сделать, это определиться с окончательным размером словаря, который мы хотим, чтобы наш токенизатор имел  и, как я уже упоминал, это гиперпараметр, и вы устанавливаете его каким-то образом в зависимости от вашей лучшей производительности, поэтому скажем, что мы собираемся использовать 276, потому что таким образом мы собираемся выполнить ровно 20 слияний и 20 слияний.  потому что у нас уже есть 256 токенов для необработанных байтов, и чтобы 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2181)~~  достичь 276, нам нужно сделать 20 слияний, э-э, чтобы добавить сюда 20 новых токенов, э-э, это один из способов в Python просто создать копию списка, поэтому я беру токены  list и, обернув его в список, Python создаст новый список всех отдельных элементов, так что это просто отдельных элементов, так что это просто операция копирования, тогда здесь я создаю словарь слияния, так что этот словарь слияния будет поддерживать в основном дочерний элемент, один дочерний элемент, два сопоставление с новым токеном uh, и поэтому здесь мы собираемся построить двоичное дерево слияний, но на самом деле это не совсем дерево, потому что дерево будет иметь один корневой узел с кучей листьев, для нас мы  начиная с листьев внизу, которые представляют собой отдельные укусы, это стартовые 256 жетонов, а затем мы начинаем объединять два из них за раз, и так это не дерево, а больше похоже на лес, хм, когда мы объединяем их  элементов, поэтому для 20 слияний мы найдем наиболее часто встречающуюся пару, мы собираемся создать для нее новое целое число токена, поэтому я 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2248)~~  начну с нуля, поэтому мы начнем с 256, мы собираемся напечатать что мы объединяем его и собираемся заменить все вхождения этой пары новым новым лживым токеном, и мы собираемся записать, что эта пара целых чисел объединилась в это новое целое число, поэтому запуск этого дает нам следующее целое число, поэтому запуск этого дает нам следующее вывод, поэтому мы сделали 20 слияний, и, например, первое слияние было точно таким же, как и до того, как 10132 токенов um слились в новый токен 2556, теперь имейте в виду, что отдельные токены uh 101 и 32 все еще могут встречаться в последовательности после слияния, только когда они  происходит точно последовательно, и теперь это становится 256, и, в частности, здесь следует отметить еще одну вещь: токен 256, который является вновь созданным токеном, также имеет право на слияние, поэтому здесь внизу 20-е слияние было слиянием 25 и 259, становящимся 275, поэтому каждый раз, когда мы заменяем эти токены, они получают право на объединение в следующем раунде распределения данных, 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2316)~~  поэтому мы создаем небольшой своего рода двоичный лес вместо одного отдельного дерева, на что мы также можем обратить внимание. мы можем взглянуть на степень сжатия, которую мы достигли, поэтому, в частности, мы начали с этого списка токенов, поэтому мы начали с 24 000 байт, и после слияния 20 раз у нас теперь есть только 19 000 токенов um, и поэтому степень сжатия, просто разделив эти два значения, составляет примерно 1,27, так что именно такой степени сжатия мы смогли достичь этого текста всего за 20 слияний, гм, и, конечно, чем больше словарных элементов вы добавите, тем больше будет степень сжатия здесь, наконец, так что это  что-то вроде обучения токенизатора, если хотите, 1 момент, который я хотел отметить, и, возможно, это диаграмма, которая может помочь, гм, как бы проиллюстрировать, что токенизатор - это совершенно отдельный объект от самой большой языковой модели, так что все  в этой лекции мы на самом деле не касаемся самого фильма, мы просто обучаем токенизатор, обычно это совершенно отдельный этап предварительной обработки, 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2384)~~  поэтому токенизатор будет иметь свой собственный обучающий набор, точно так же, как большая языковая модель имеет потенциально другое обучение  установите так, чтобы у токенизатора был обучающий набор документов, на котором вы собираетесь обучать токенизатор, а затем мы выполняем алгоритм кодирования пары битов, как мы видели выше, чтобы обучить словарь этого токенизатора, чтобы он имел собственное обучение  установите, что это этап предварительной обработки, который вы запускаете один раз в начале, и токенизатор обучается с использованием алгоритма двупарного кодирования, как только у вас есть токенизатор, как только он будет обучен, и у вас есть словарный запас, и у вас есть слияния, мы можем сделать  и кодирование, и декодирование, поэтому здесь эти две стрелки, поэтому токенизатор представляет собой уровень перевода между необработанным текстом, который, как мы видели, представляет собой последовательность кодовых точек Unicode, он может брать необработанный текст и превращать его в последовательность токенов, 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2435)~~  и наоборот, он может принимать токен. последовательность и перевести ее обратно в необработанный текст, так что теперь, когда мы обучили токенизатор и у нас есть эти слияния, мы собираемся перейти к тому, как мы можем выполнить этап кодирования и декодирования, если вы дадите мне текст, вот токены, и наоборот, если  вы даете мне жетоны, вот текст, как только он у нас будет, который мы можем перевести между этими двумя областями, а затем языковая модель будет обучена на втором этапе и, как правило, в своего рода современном приложении  вы можете взять все свои обучающие данные для языковой модели, пропустить их через токенизатор и как бы перевести все в массивную последовательность токенов, а затем вы можете выбросить необработанный текст, который у вас только что остался, с самими токенами и теми хранятся на диске, и именно это на самом деле читает большая языковая модель во время обучения на них, так что это единственный подход, который вы можете использовать как один массивный шаг предварительной обработки на 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2487)~~  этапе, так что да, в основном я думаю, что это самое важное, что я хочу  Чтобы понять, что это совершенно отдельный этап, он обычно имеет свой собственный полный обучающий набор, вы можете захотеть, чтобы эти обучающие наборы были разными между токенизатором и языковой моделью журнала, например, когда вы обучаете токенизатор, как я уже упоминал  нас заботит не только производительность текста на английском языке, мы заботимся о множестве разных языков, а также о коде или некоде, поэтому вы можете рассмотреть различные виды смесей разных языков и разное количество кода. и тому подобное, потому что количество разных языков, которые есть в вашем обучающем наборе токенизатора, будет определять, сколько его слияний будет, и, следовательно, это определяет плотность, с которой этот тип данных, гм, в некотором роде имеет в пространстве токенов  и, грубо говоря, интуитивно, если вы добавите некоторое количество данных, например, у вас есть тонна японских данных в вашем обучающем наборе токенизатора, это означает, что больше японских 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2545)~~  токенов будет объединено, и, следовательно, японские токены будут иметь более короткие последовательности, э-э, и это будет  Это полезно для модели большого языка, которая имеет конечную длину контекста, с которым она может работать в пространстве токенов. Надеюсь, это имеет смысл, поэтому теперь мы собираемся перейти к кодированию и декодированию, поскольку мы обучили токенизатор, поэтому мы  у нас есть слияния, и теперь, как нам выполнять кодирование и декодирование, хорошо, давайте начнем с декодирования, что представляет собой эта стрелка, так что, учитывая последовательность токенов, давайте пройдем через токенизатор, чтобы вернуть строковый объект Python, поэтому необработанный текст, так что это функция что мы хотим реализовать, нам дан список целых чисел, и мы хотим вернуть строку Python, если вы хотите, попробуйте реализовать эту функцию самостоятельно, это забавное упражнение, иначе я начну вставлять в свой  собственное решение, поэтому есть много разных способов сделать это, вот один из способов, я 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2597)~~  создам своего рода переменную предварительной обработки, которую я назову vocab, а vocab - это отображение или словарь в Python для перехода от токена, ну, идентификатора к байтам  для этого токена, поэтому мы начинаем с необработанных байтов для токенов от 0 до 255, а затем идем по порядку всех слияний и как бы заполняем этот словарный список, делая здесь дополнение, так что это, по сути, байтовое представление  за первым дочерним элементом следует второй, и помните, что это байтовые объекты, поэтому это добавление здесь представляет собой добавление двух байтовых объектов, просто конкатенацию, вот что мы здесь получаем. Кстати, одна сложная вещь, с которой следует быть осторожным, это то, что я повторяю словарь в Python с использованием элементов DOT, и действительно важно, чтобы это выполнялось в том порядке, в котором мы вставляли элементы в словарь merous, к счастью, начиная с Python 3.7, это гарантированно так, но до Python 3.7 эта итерация могла быть исключена  порядок в отношении того, как мы вставляли элементы в слияния, и это, 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2663)~~  возможно, не сработало, но мы используем современный Python, так что у нас все в порядке, а затем, учитывая IDS, первое, что мы собираемся сделать, это получить токены, поэтому  способ, которым я это реализовал здесь, я беру, я перебираю все IDS, я использую vocap для поиска их байтов, а затем вот это один из способов в Python объединить все эти байты вместе, чтобы создать наши токены, а затем  эти токены здесь на данный момент представляют собой необработанные байты, поэтому мне нужно декодировать с помощью UTF F теперь обратно в строки Python, поэтому ранее мы вызывали это кодирование для строкового объекта, чтобы получить байты, и теперь мы делаем это. Напротив, мы берем байты  и вызываем декодирование объекта байтов, чтобы получить строку в Python, а затем мы можем вернуть текст, так что вот как мы можем это сделать сейчас, на самом деле есть проблема с тем, как я это реализовал, и это может фактически вызвать ошибку  так что попробуйте подумать и выяснить, почему этот код на самом деле может привести к ошибке, если мы подключим 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2730)~~  какую-то последовательность идентификаторов, которая неудачна, поэтому позвольте мне продемонстрировать проблему, когда я пытаюсь декодировать что-то вроде 97, я собираюсь получить букву A  здесь, так что ничего особенного не происходит, но когда я пытаюсь декодировать 128 как один элемент, токен 128 - это то, что в строке или в объекте Python uni Cod decoder utfa не может декодировать по um 0x8, что находится в HEX в нулевой позиции, недопустимое начало  укусите, что это значит, чтобы хорошо понять, что это значит, нам нужно вернуться на нашу страницу utf8, которую я кратко показал ранее, и это Википедия utf8, и по сути есть определенная схема, которую принимают байты utfa, особенно если у вас есть несколько  te для некоторых символов Юникода они должны иметь этот особый вид конверта в том, как работает кодировка, и поэтому здесь происходит недопустимый начальный элемент, потому что 128 его двоичное представление - это единица, за которой следуют все нули, поэтому у нас есть один а затем все нули, и здесь мы видим, что 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2800)~~  это не соответствует формату, потому что за единицей следуют все нули, просто не соответствует ни одному из этих правил, так сказать, так что это недопустимый начальный бит, который является байтом один, этот должен иметь один после него, а затем ноль после него, а затем содержимое вашего uni-кода в x здесь, так что в основном мы не совсем следуем стандарту utf8, и это не может быть декодировано, и поэтому способ исправить это - использовать эти ошибки  равно в байтах. функция декодирования Python, и по умолчанию ошибки являются строгими, поэтому мы выдадим ошибку, если это недопустимая кодировка байтов utf8, но есть много разных вещей, которые вы могли бы разместить здесь для обработки ошибок, это полный список всех ошибок, которые вы можете  используйте и, в частности, вместо строгого, давайте изменим его на замену, и это заменит этот специальный маркер этим символом замены, поэтому ошибки равны замене, и теперь мы просто возвращаем этот символ, так что в основном не каждая последовательность действительна utf8, и если это произойдет  что ваша большая 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2871)~~  языковая модель, например, плохо предсказывает ваши токены, тогда они могут не попасть в действительный utf8, и тогда мы не сможем их декодировать, поэтому стандартная практика состоит в том, чтобы в основном использовать ошибки, равные замене, и это то, что вы  также найдет в коде openai um, который они также выпустили, но в основном всякий раз, когда вы видите такой символ в своем выводе, в этом случае что-то пошло не так, и вывод LM не был действительным, э-э, своего рода последовательность токенов, хорошо и  теперь мы собираемся пойти другим путем, поэтому мы собираемся реализовать эту стрелку прямо здесь, где нам будет предоставлена ​​строка, и мы хотим закодировать ее в токены, так что это подпись интересующей нас функции.  и, по сути, это должно напечатать список целых чисел токенов, так что еще раз, ну, попробуйте, может быть, реализовать это самостоятельно, если вы хотите веселое упражнение, ну, и сделайте паузу здесь, иначе я собираюсь начать вставлять свое решение, так что снова есть  много способов сделать это, так что это один из способов, 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2934)~~  которые я придумал, поэтому первое, что мы собираемся сделать, это первое, что мы собираемся сделать, это взять наш текст, закодировать его в utf8, чтобы получить необработанные байты и  затем, как и раньше, мы собираемся вызвать list объекта байтов, чтобы получить список целых чисел этих байтов, так что это начальные токены, это необработанные байты нашей последовательности, но теперь, конечно, в соответствии со словарем слияний, приведенным выше, и вспомните это  были слияния, некоторые байты могут быть объединены в соответствии с этим поиском, в дополнение к этому помните, что слияния были построены сверху вниз, и это своего рода порядок, в котором мы вставляли данные в слияния, и поэтому мы предпочитаем делать все это  слияния в начале, прежде чем мы сделаем эти слияния позже, потому что, например, это слияние здесь основано на 256, которые были слиты здесь, поэтому нам нужно идти в порядке сверху вниз, например, если мы собираемся что -то объединять, сейчас мы  ожидаем, что нам придется выполнить несколько 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=2991)~~  слияний, поэтому мы собираемся сделать W true um, и теперь мы хотим найти пару последовательных байтов, которые нам разрешено объединять в соответствии с этим, чтобы повторно использовать некоторые функциональные возможности, которые мы  Я уже написал, что собираюсь повторно использовать функцию uh get stats, так что напомним, что get stats uh даст нам, по сути, мы подсчитаем, сколько раз каждая отдельная пара встречается в нашей последовательности токенов, и вернем это как словарь и  словарь представлял собой сопоставление всех различных пар с количеством раз, когда они встречаются, прямо сейчас, нас на самом деле не волнует, сколько раз они встречаются в последовательности, нас волнует только то, какие необработанные пары находятся в этой последовательности  и поэтому я собираюсь использовать только ключи словаря. Меня волнует только набор возможных кандидатов на слияние, если это имеет смысл. Теперь мы хотим идентифицировать пару, которую мы собираемся объединить на этом этапе  цикл, так что мы хотим, мы хотим найти пару или что-то вроде ключа 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3053)~~  внутри статистики, который имеет самый низкий индекс в словаре слияний, э-э, потому что мы хотим выполнить все ранние слияния, прежде чем мы доберемся до поздних слияний, так что еще раз  Есть много разных способов реализовать это, но я собираюсь сделать здесь что-то немного необычное, поэтому я буду использовать Min вместо итератора в Python, когда вы вызываете Min на итераторе, и статистику здесь в качестве словаря, который мы  Мы собираемся перебирать ключи этого словаря в Python, поэтому мы смотрим на все пары внутри статистики, которые являются последовательными парами, и мы собираемся взять последовательную пару внутри токенов, которая имеет минимум, равный Min  берет ключ, который дает нам функцию, которая будет возвращать значение, для которого мы собираемся выполнить Min, и нас волнует то, что мы заботимся о слияниях и, по сути, о получении слияниях и, по сути, о получении индекса пар, так что в основном для  любая пара внутри статистики, мы собираемся изучить слияния по ее индексу, и мы хотим получить пару с минимальным номером, 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3126)~~  например, если есть пара 101 и 32, мы определенно хотим получить эту пару, ну, мы хотим  идентифицируйте его здесь и верните, и пара станет 10132, если это произойдет, и причина, по которой я помещаю здесь INF с плавающей запятой в качестве запасного варианта, заключается в том, что в функции get, когда мы вызываем, когда мы в основном рассматриваем пару, которая не происходят при слияниях, тогда эта пара не может быть объединена правильно, поэтому, если в последовательности токенов есть какая-то пара, которая не является сливающейся парой, ее нельзя объединить, тогда на самом деле это не происходит здесь и у нее нет индекса  и его нельзя объединить, что мы обозначим как float INF, и причина, по которой Infinity здесь хороша, заключается в том, что мы точно гарантируем, что он не будет участвовать в списке кандидатов, когда мы будем выбирать мужчин, так что это один способ сделать это так B, вкратце, это возвращает наиболее подходящую пару кандидатов на слияние, которая встречается в токенах, теперь есть одна вещь, с которой следует быть осторожным, вот 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3184)~~  эта функция uh может потерпеть неудачу следующим образом, если нечего объединять, тогда uh  хм, тогда в слияниях нет ничего хм, что удовлетворительно, что удовлетворено, больше нечего объединять, все просто возвращает плавающие импы, а затем пара, я думаю, просто станет самым первым элементом статистики хм, но эта пара на самом деле не является объединяемой парой, она просто становится  первая пара внутри статистики произвольно, потому что все эти пары оцениваются как плавающие для критерия слияния, поэтому в принципе может быть так, что это не выглядит успешным, потому что больше нет сливающихся пар, поэтому, если эта пара не находится в слияниях, которые были возвращены, тогда  это для нас сигнал, что на самом деле объединять нечего, ни одну пару больше нельзя объединить, в этом случае мы объединить нельзя, вы можете придумать другую реализацию, кстати, это вроде как на самом деле  очень стараемся в Python, но на самом деле мы просто пытаемся найти пару, которую можно объединить с наименьшим индексом, здесь и сейчас, если мы нашли пару, которая находится 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3254)~~  внутри слияния с наименьшим индексом, тогда так что мы собираемся  заглянуть в словарь слияния для этой пары, чтобы найти индекс, и теперь мы собираемся объединить его с этим индексом, чтобы сделать токены равными и заменить исходные токены, которые мы собираемся  заменяем пару пар, и мы собираемся заменить ее индексом idx, и это возвращает новый список токенов, в котором каждое вхождение пары заменяется на idx, поэтому мы выполняем слияние и собираемся продолжить это пока, в конце концов, ничего не сможет быть объединено, мы придем сюда и вырвемся, и здесь мы просто вернем токены, и это реализация, я думаю, так что надеюсь, что это работает нормально, круто, хм, да, и это выглядит э-э-э, разумно, например, 32  пробел в asky, вот он, хм, похоже, все сработало отлично, ладно, давайте завершим этот раздел видео, по крайней мере, я хотел отметить, что это пока не совсем правильная реализация, потому что мы упускаем особый случай  так что, в частности, если мы попытаемся это сделать, это даст нам ошибку, и проблема в том, что если у нас есть только 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3326)~~  один символ или пустая строка, тогда статистика пуста, и это вызывает проблему внутри Min, поэтому один из способов борьбы с этим  это если L токенов равно как минимум двум, потому что если оно меньше двух, это просто один токен или нет токенов, тогда давайте просто, ну, объединять нечего, поэтому мы просто возвращаемся, чтобы это исправило этот случай. Хорошо, а затем во-вторых, у меня есть несколько тестовые примеры здесь также для нас, поэтому сначала давайте удостоверимся, или давайте отметим следующее: если мы возьмем строку и попытаемся ее закодировать, а затем декодировать ее обратно, вы ожидаете получить ту же самую строку строки, так что я думаю, ну, вот это тот случай, и я думаю, что в целом это, вероятно, так, но заметьте, что движение назад - это не так, вы не будете иметь идентичность, идущую назад, потому что, как я уже упоминал, не все мы  последовательности токенов действительны utf8, ну, вроде как по потокам, и так, поэтому некоторые из них даже не могут быть декодированы, так что это происходит только в одном 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3390)~~  направлении, но для этого одного направления мы можем проверить, э-э, здесь, если мы возьмем обучающий текст  это текст, который мы обучаем токенизатору, мы можем быть уверены, что когда мы кодируем и декодируем, мы получаем одно и то же, что верно, и здесь я взял некоторые данные проверки, поэтому я зашел, кажется, на эту веб-страницу и взял немного текста  так что это текст, который токенизатор не видел, и мы можем убедиться, что это также работает, хм, хорошо, так что это дает нам некоторую уверенность в том, что это было правильно реализовано, так что это основы алгоритма кодирования пары битов, которые мы видели, как мы можем взять  некоторый обучающий набор обучает токенизатор, параметры этого токенизатора на самом деле представляют собой просто словарь слияний, который по сути создает небольшой двоичный лес поверх необработанных фрагментов. Как только у нас есть таблица слияний, мы можем как кодировать, так и декодировать между необработанным текстом и последовательностями токенов  так что это самая простая настройка токенизатора, но сейчас 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3443)~~  мы собираемся рассмотреть некоторые из современных языковых моделей и типы токенизаторов, которые они используют, и мы увидим  что эта картина усложняется очень быстро, поэтому мы собираемся рассмотреть детали этого усложнения компа по одной, поэтому давайте начнем с рассмотрения серии GPD, так что, в частности, у меня есть статья по gpt2, а эта статья из Примерно в 2019 году, 5 лет назад, и давайте прокрутим вниз до входного представления, здесь они говорят о токенизаторе, который они используют для gpd2, теперь все это довольно читабельно, поэтому я советую вам сделать паузу и прочитать это самостоятельно, но именно здесь  они мотивируют использование алгоритма кодирования битовой пары в представлении кодировки utf8 на уровне битов, так что именно здесь они мотивируют это и говорят о размерах словаря и обо всем, теперь все здесь точно так, как мы это рассмотрели до сих пор, но все начинается  уйти отсюда, поэтому они упоминают, что они не просто применяют наивный алгоритм, как это сделали мы, и, в частности, вот 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3505)~~  пример, предположим, что у вас есть общие слова, такие как собака, что произойдет, так это то, что собака, конечно, встречается очень часто в текст, и он встречается рядом со всеми видами знаков препинания в качестве примера, поэтому doc точка собака восклицательный знак собака вопросительный знак и т. д., и вы наивно можете подумать, что алгоритм BP может объединить их в отдельные токены, и тогда вы получите множество токенов  это похоже на собаку с немного другой пунктуацией, и поэтому создается впечатление, что вы группируете вещи, которые не должны быть кластеризованы, вы комбинируете семантику с определением, и это кажется неоптимальным, и действительно, они также говорят, что это неоптимально в соответствии с  к некоторым экспериментам, поэтому они хотят вручную вручную обеспечить, чтобы некоторые типы символов um никогда не объединялись вместе, um, поэтому они хотят обеспечить соблюдение этих правил слияния поверх алгоритма кодирования bit PA. Итак, давайте посмотрим на их код и посмотрим, как они на самом деле обеспечивают это 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3561)~~  и какие виды слияний они на самом деле выполняют, поэтому мне нужно открыть вкладку здесь для gpt2 в открытом AI на GitHub, и когда мы переходим к исходному коду, там есть кодировщик  вот мне лично не нравится, что они называют это кодировщиком, потому что это токенизатор, а токенизатор может выполнять как кодирование, так и декодирование, так что мне немного неловко, что он называется кодировщиком, но это токенизатор, и там много чего  что происходит здесь, и мы собираемся подробно рассмотреть это в какой-то момент, я просто хочу сосредоточиться на этой части здесь, создайте здесь шаблон rigix, который выглядит очень сложным, и мы собираемся пройти через это немного позже.  хм, но это основная часть, которая позволяет им обеспечивать соблюдение правил, хм, для того, какие части текста никогда не будут объединены наверняка, теперь обратите внимание на это.  компиляция здесь немного вводит в заблуждение, потому что мы не просто выполняем import re, который представляет собой модуль re для python, мы делаем import reex, 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3615)~~  поскольку re и reex — это пакет Python, который вы можете установить P install r x, и это по сути расширение re  так что это немного более мощный ответ, так что давайте посмотрим на этот шаблон, на то, что он делает и почему он на самом деле обеспечивает разделение, которое они ищут, хорошо, поэтому я скопировал шаблон здесь в наш блокнот Jupit, где мы остановились  off, и давайте попробуем этот шаблон, так что точно так же, как и их код, мы собираемся вызвать re.  findall для этого шаблона в любой произвольной строке, которая нас интересует, так что это строка, которую мы хотим закодировать в токены um для подачи в n llm, например gpt2, так что именно это хорошо работает.  findall возьмет этот шаблон и попытается сопоставить его со строкой. Это работает так: вы идете по строке слева направо и пытаетесь сопоставить шаблон, а RF find all получит все вхождения и организует  их в список теперь, когда вы смотрите на um, когда вы смотрите на этот шаблон, прежде всего обратите внимание, что это необработанная строка um, а 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3684)~~  затем это три двойные кавычки, просто чтобы начать строку, так что на самом деле сама строка - это сам шаблон направо и обратите внимание, что он состоит из большого количества руд, поэтому посмотрите на эти вертикальные полосы, это руды в строке X, и поэтому вы идете слева направо по этому шаблону и пытаетесь сопоставить его со строкой, где бы вы ни находились, поэтому у нас привет и мы постараемся сопоставить его хорошо, это не апостроф s, это не апостроф t или что-то из этого, но это необязательный пробел, за которым следует - P извиняюсь SL P of L один или несколько раз то, что есть / P of L это приходит к какой-то документации, которую я нашел, возможно, есть и другие источники, э-э, SLP - это буква, любая буква с любого языка, а привет состоит из букв h e l и т. д., поэтому необязательное пространство, за которым следует группа букв, одна или несколько букв  будет соответствовать приветствию, но затем совпадение заканчивается, потому что пробел не является буквой, поэтому с этого момента начинается новый тип попытки сопоставления со 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3754)~~  строкой снова, и начиная с этого момента, мы будем пропускать все это снова, пока  мы снова добираемся до той же самой точки и видим, что есть необязательный пробел. Это необязательный пробел, за которым следует группа букв, одна или несколько из них, и это соответствует, поэтому, когда мы запускаем это, мы получаем список из двух элементов hello и  тогда космический мир, и как ваши дела, если мы добавим больше букв, мы просто получим их вот так, что это делает и почему это важно, мы берем нашу строку и вместо того, чтобы напрямую кодировать ее для токенизации, мы сначала разделяем ее и  когда вы на самом деле выполняете код, и мы сделаем это немного более подробно, что на самом деле происходит на высоком уровне, так это то, что он сначала разбивает ваш текст на список текстов, подобный этому, и все эти элементы этого списка обрабатываются токенизатором независимо, и все результаты этой обработки просто объединяются, так что привет, мир, о, я пропустил, как привет, мир, как дела, у нас есть пять элементов списка, все они 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3822)~~  будут независимо независимо переходить от текста к последовательности токенов, а затем это  последовательность токенов будет объединена, все будет объединено, и, грубо говоря, это означает, что вы когда-либо находите слияния только между элементами этого списка, поэтому вы можете рассматривать слияния только внутри каждого из этих элементов индивидуально и  хм, после того, как вы выполнили все возможные слияния для всех этих элементов по отдельности, результаты всех этих элементов будут объединены хм путем конкатенации, и поэтому вы, по сути, то, что вы делаете эффективно, вы никогда не собираетесь объединять это e с  это пространство, потому что теперь они являются частями отдельных элементов этого списка, и поэтому вы говорите, что мы никогда не собираемся объединять eace um, потому что мы разбиваем его таким образом, поэтому, по сути, использование этого шаблона регулярного выражения для разбиения текста на части - это просто  один из способов добиться того, чтобы некоторые слияния не происходили, и мы собираемся углубиться 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3884)~~  в этот текст и увидеть, что на высоком уровне мы пытаемся не сливать буквы поперек.  числа через знаки препинания и т. д., давайте посмотрим подробнее, как это работает, поэтому давайте продолжим, теперь у нас есть / P ofn, если вы перейдете к документации, SLP of n — это любой цифровой символ в любом сценарии, поэтому это числа, поэтому у нас есть необязательный пробел, за которым следуют цифры, и они будут разделены, поэтому буквы и цифры будут разделены, поэтому, если я напишу « Hello World 123, как дела», тогда мир перестанет соответствовать здесь, потому что одна из них больше не является буквой, а одна — это число, поэтому эта группа будет совпадать  для этого, и давайте посмотрим, как работают эти апострофы, итак, если у нас есть, хм, слэш V, или я имею в виду апостроф V в качестве примера, то апостроф здесь - это не буква или цифра, так что привет остановится  сопоставление, а затем мы точно сопоставим это с этим, так что это выйдет как отдельная вещь, так почему они здесь делают апострофы, честно говоря, я думаю, что 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=3952)~~  это такие же очень распространенные апострофы, хм, которые используются, хм, обычно я не люблю  что они сделали это, потому что позвольте мне показать вам, что происходит, когда у вас есть несколько апострофов Юникода, например, которые вы можете иметь, если у вас есть дом, тогда они будут отделены из-за этого совпадения, но если вы используете апостроф Юникода, как этот  затем вдруг это не работает, и этот апостроф фактически станет теперь самостоятельным, и так что, хм, он в основном жестко запрограммирован для этого конкретного типа апострофа, а в противном случае они станут совершенно отдельными токенами, в дополнение к этому вы можете перейти к документации gpt2 и  здесь, когда они определяют шаблон, они говорят, что следовало добавить re.  игнорируйте регистр, чтобы слияние BP могло происходить для версий сокращений, написанных с заглавной буквы, поэтому они указывают на то, что вы хорошо видите, как это апостроф, а затем строчные буквы, потому что они не сделали re.  игнорировать регистр, тогда тогда эти правила не будут 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4016)~~  разделять апострофы, если они в верхнем регистре, поэтому дом будет таким, но если бы я сделал дом, если я в верхнем регистре, то обратите внимание, что апостроф внезапно появляется сам по себе, поэтому токенизация будет работать по-разному в верхнем и нижнем регистре  случай непоследовательного разделения этих апострофов, так что это кажется очень корявым и немного грубым, гм, но вот как это работает, хорошо, так что давайте вернемся после попытки сопоставить кучу выражений апострофа, кстати, другая проблема здесь в том, что они, вероятно, довольно специфичны для языка  поэтому я не знаю, например, что все языки используют или не используют апострофы, но в результате это будет непоследовательно токенизировано, тогда мы пытаемся сопоставить буквы, затем мы пытаемся сопоставить числа, а затем, если это не сработает, мы падаем  вернемся сюда, и здесь снова говорится о необязательном пробеле, за которым следует что-то, что не является цифрой буквы или пробелом в одном или нескольких из них, так что эффективно это делает попытку сопоставить 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4078)~~  пунктуацию, грубо говоря, не буквы и не цифры  поэтому эта группа попытается спровоцировать это, поэтому, если я сделаю что-то подобное, то эти части здесь не будут буквами или цифрами, но на самом деле они будут, э-э, их действительно поймают здесь, и они станут отдельной группой, поэтому мы разделились  уберите пунктуацию и, наконец, это тоже немного сбивает с толку, так что это сопоставление пробелов, но при этом используется утверждение отрицательного просмотра вперед в регулярном выражении, так что это означает сопоставление с пробелом до, но не включая последний пробел Уита. символ, почему это важно, хм, это довольно тонко, я думаю, вы видите, как пробел всегда включается в начало слова, так что хм пробел r пробел u и т. д. предположим, что у нас здесь много пробелов, что здесь произойдет, так это то, что эти пробелы, вплоть до не включая последний символ, будут пойманы этим, и это будет отделять пробелы до последнего символа, но не включая его, так что последний символ может прийти сюда и присоединиться к пространству um, которое вы и  причина, по 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4149)~~  которой это хорошо, заключается в том, что пробел you является общим токеном, поэтому, если бы у меня здесь не было этих дополнительных пробелов, у вас было бы просто место you, и если я добавлю токены, если я добавлю пробелы, у нас все еще будет представление пространства, но теперь у нас есть все это  дополнительное пустое пространство, так что в основном токенизатору GB to действительно нравится иметь пробелы в виде букв или цифр, и он очищает эти пробелы, и это просто то, в чем он единообразен, так что это то, для чего это нужно, и, наконец, у нас есть все последнее  запасной вариант - это символы пробелов, хм, хм, это было бы просто хм, если бы это не было перехвачено, тогда эта штука будет ловить любые конечные пробелы и так далее. Я хотел показать здесь еще один пример из реального мира, поэтому, если у нас есть эта строка, которая  кусок кода Python, а затем мы пытаемся его разделить, и вот такой результат мы получаем, поэтому вы заметите, что список содержит много элементов, и это потому, что мы разделяем его довольно часто, э-э, каждый 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4205)~~  раз, что-то вроде категории изменения, гм, поэтому никогда не будет никаких слияний внутри этих элементов, и это то, что вы видите здесь сейчас, вы можете подумать, что для обучения токенизатора, э-э, открытый ИИ использовал это для разделения текста на фрагменты, а затем запускал только алгоритм BP  внутри всех кусков, но это не совсем то, что произошло, и причина в следующем уведомлении о том, что у нас здесь есть пробелы, э-э, эти пробелы в конечном итоге становятся целыми элементами, но на самом деле эти пространства никогда не объединяются с помощью открытого Ai и того, как вы можете  Скажите, что если вы скопируете и вставите точно такой же фрагмент здесь в токен Tik U токенизатор Tik, вы увидите, что все пробелы остаются независимыми, и все они являются токенами 220, поэтому я думаю, что в какой-то момент opena принудит ввести какое-то правило, согласно которому эти пробелы будут  никогда не объединяться, и поэтому, хм, помимо простого разделения на фрагменты и bpe, есть некоторые дополнительные правила, которые открываются, не совсем 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4264)~~  ясно, сейчас обучающий код для токенизатора gpt2 никогда не был выпущен, поэтому все, что у нас есть, это код, который я уже показывал  вы, но этот код, который они выпустили, является всего лишь кодом вывода для токенов, так что это не обучающий код, вы не можете дать ему фрагмент текста и обучающий токенизатор, это просто код вывода, который Так принимает слияния, которые  мы имеем выше и применяем их к новому фрагменту текста, поэтому мы не знаем точно, как открытие ey обучило обучить токенизатор, но это было не так просто, как разбить его на части и BP, ну, что бы это ни было дальше, я хотел  чтобы познакомить вас с библиотекой токенов Tik от openai, которая является официальной библиотекой для токенизации от openai, так что это токен Tik, установите P на токен Tik, а затем вы можете выполнить токенизацию в выводе, это снова не обучающий код, это только вывод  код для токенизации, я хотел показать вам, как вы могли бы использовать его довольно просто, и его запуск просто дает нам токены gpt2 или токены GPT 4, так что это 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4327)~~  использование токенизатора для GPT 4, и поэтому, в частности, мы видим, что пространство Уита в gpt2 остается неслитым, но в GPT 4 эти пространства Уита сливаются, как мы также видели в этом, где они все не объединены, но если мы перейдем к GPT 4, они объединяются, гм, теперь в токенизаторе gp4 они изменили регулярное выражение  что они используют для фрагментации текста, поэтому способ увидеть это заключается в том, что если вы зайдете в свою библиотеку токенов Tik, а затем перейдете в этот файл Tik token X openi public, это будет что-то вроде определения всех этих различных токенизаторов.  что openi поддерживает, и поэтому, э-э, обязательно, чтобы сделать вывод, им пришлось опубликовать некоторые подробности о строках, так что это строка, которую мы уже видели для gpt2, она немного отличается, но на самом деле она эквивалентна тому, что мы обсуждали здесь, так что  этот шаблон, который мы обсуждали, эквивалентен этому шаблону, он просто выполняется немного быстрее, поэтому здесь вы видите немного другое определение, но 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4391)~~  в остальном это то же самое, мы собираемся немного перейти к специальным токенам, а затем, если  вы прокручиваете вниз до CL 100k, это токенизатор GPT 4, вы видите, что шаблон изменился, и это вроде как основное основное изменение в дополнение к куче других специальных токенов, о которых я расскажу немного позже теперь кое-что я не собираюсь вдаваться в подробности изменения шаблона, потому что, честно говоря, это меня ошеломляет, я бы просто посоветовал вам вытащить GPT чата и документацию по регулярным выражениям и просто пройти через них, но на самом деле основные изменения  номер один, вы видите здесь этот глаз, это означает, что чувствительность к регистру — это совпадение без учета регистра, и поэтому комментарий, который мы видели ранее в разделе «О», нам следовало использовать re.  верхний регистр, ну, в общем, теперь мы собираемся сопоставлять эти апострофы с апострофом D, апострофом M и т. д. ну, мы собираемся сопоставлять их как в нижнем, так и в верхнем регистре, так что это исправлено, есть куча разных, таких как обработка 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4453)~~  пробелов, которые я  Я не буду вдаваться в подробности, и еще одна вещь: вы заметите, что когда они совпадают с числами, они соответствуют только одному-трем числам, поэтому они никогда не будут объединять числа, которые состоят из более чем трех цифр.  только до трех цифр чисел когда-либо будут объединены, и это одно изменение, которое они также внесли, чтобы предотвратить токены, которые представляют собой очень-очень длинные числовые последовательности, но опять же, мы действительно не знаем, почему они делают что-то из этого, потому что  ничего из этого не задокументировано, и просто мы просто получили шаблон, так что да, это то, что есть, но это некоторые изменения, которые внес gp4, и, конечно, размер словаря увеличился с примерно 50 тысяч до примерно 100 тысяч, следующее, что я  Я хотел бы очень кратко рассказать вам о кодировщике gpt2, выпущенном Openi, это файл, о котором я уже кратко упоминал вам, теперь этот файл довольно короткий и должен быть относительно понятен вам на этом этапе, гм, начиная  внизу 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4518)~~  здесь они загружают два файла, кодировщик Json и vocab bpe, и они выполняют некоторую легкую обработку, а затем вызывают этот объект кодировщика, который теперь является токенизатором, если вы хотите проверить эти два файла, которые вместе составляют их сохраненный токенизатор.  тогда вы можете сделать это с помощью такого фрагмента кода, вы можете сделать это с помощью такого фрагмента кода, хм, здесь вы можете скачать эти два файла и проверить их, если хотите, и вы обнаружите, что этот кодировщик, как они называют его в своем коде, в точности  эквивалент нашего словарного запаса, так что помните, где у нас есть этот объект словарного запаса, который позволил нам очень эффективно декодировать, и по сути он перевел нас от целого числа к прощанию, э-э, для этого целого числа, так что наш словарь является именно их кодером, а затем их словарный запас bpe сбивает с толку  на самом деле это слияния, поэтому их слияния BP, основанные на данных внутри словаря bpe, в конечном итоге эквивалентны нашим слияниям, так что, по сути, 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4581)~~  они сохраняют и загружают две переменные uh, которые для нас также имеют решающее значение: переменную слияния и переменную vocab, используя просто  эти две переменные вы можете представлять токенизатор, и вы можете выполнять как кодирование, так и декодирование, как только вы обучили этот токенизатор, теперь единственное, что на самом деле немного сбивает с толку внутри того, что здесь делает открытие ey, это то, что в дополнение к этому кодировщику и декодеру они  также есть что-то, называемое кодировщиком битов и декодером битов, и на самом деле, к сожалению, это всего лишь своего рода детали реализации, и на самом деле они не являются глубокими или интересными, поэтому я собираюсь пропустить обсуждение этого, но что делает открытие ey здесь по причинам, которые я не совсем понимаю, заключается в том, что у них есть не только этот токенизатор, который может кодировать и декодировать, но и еще у них есть целый отдельный уровень, который используется последовательно с токенизатором, и поэтому вы сначала выполняете кодирование битов, а затем  кодируете, 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4636)~~  а затем декодируете, а затем декодируете по кусочкам, так что это цикл, и они просто накладываются последовательно друг на друга, и это не так уж интересно, поэтому я не буду это освещать, и вы можете пройти через это, если хотите иначе  этот файл, если вы проигнорируете кодировщик битов, а декодер битов будет вам алгоритмически очень хорошо знаком, и суть его здесь - это то, что они называют функцией bpe, и вы должны узнать этот цикл здесь, который очень похож на наш собственный цикл y, где они  пытаемся идентифицировать Байрама, э-э, пару, которую они должны объединить следующим, а затем здесь, как и у нас, у них есть цикл for, пытающийся объединить эту пару, э-э, поэтому они пройдут всю последовательность и объединят пару всякий раз, когда  они находят его и продолжают повторять это до тех пор, пока у них не закончатся возможные слияния в тексте, так что это суть этого файла, и, ну, есть функции кодирования и декодирования, точно так же, как мы это реализовали, так что, короче говоря, чего я от вас хочу на этом этапе следует отметить, что, к сожалению, у 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4692)~~  них немного беспорядочный код, но алгоритмически он идентичен тому, что мы создали выше, и тому, что мы создали выше, если вы понимаете, что это алгоритмически то, что необходимо  на самом деле построить BP, чтобы организатор обучил его, а затем закодировать и декодировать. Следующая тема, к которой я хотел бы обратиться, — это специальные токены, поэтому в дополнение к токенам, которые поступают от вас, вы знаете, необработанные байты и слияния BP, мы можем вставить все  виды токенов, которые мы собираемся использовать для разграничения различных частей данных или вводить для создания специальной структуры потоков токенов, поэтому, если вы посмотрите на этот объект кодировщика из открытого AIS gpd2, мы уже упоминали, что это очень похоже на  наш словарный запас, вы заметите, что его длина равна 50257, и, как я уже говорил, это отображение, э-э, и оно инвертировано по сравнению с отображением нашего словаря, наш словарь переходит от целого числа к строке, и они идут наоборот по какой-то удивительной причине, хм, но 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4752)~~  Здесь следует отметить, что эта таблица сопоставления - 50257, откуда взялось это число, откуда что такое токены, как я уже упоминал, есть 256 токенов необработанных токенов, а затем Opena фактически выполнила 50 000 слияний, так что они стали другими токенами, но это будет  было 50256, так что это за 57-й токен, и по сути есть один специальный токен, и этот специальный токен, который вы видите, называется концом текста, так что это специальный токен, и это самый последний токен, и этот токен используется для разграничения документов.  в обучающем наборе, поэтому, когда мы создаем обучающие данные, у нас есть все эти документы, мы маркируем их и получаем поток токенов, эти токены имеют диапазон только от Z до 50256, а затем между этими документами мы помещаем специальный токен конца текста и мы вставляем этот токен между документами и используем это как сигнал языковой модели о том, что документ завершился, и то, что следует за ним, не будет связано с предыдущим документом, в котором говорилось, что языковая модель 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4825)~~  должна изучить это из данных, которые она содержит.  необходимо узнать, что этот токен обычно означает, что он должен стереть свою память о том, что было раньше, и что было до того, что этот токен на самом деле не информативен для того, что будет дальше, но мы ожидаем, что языковая модель просто выучит это, но мы  придав ему особый вид ограничителя этих документов, мы можем перейти сюда, к технологическому токенизатору, и, хм, к токенизатору gpt2, а, к нашему коду, с которым мы играли раньше, поэтому мы можем добавить сюда прямо привет, мир, как дела и мы?  я получаю разные токены, но теперь вы можете видеть, что если что произойдет, если я положу конец текста, вы видите, как, пока я не закончил, это все разные токены. Конец текста по-прежнему устанавливает разные токены, и теперь, когда я заканчиваю его, мы внезапно получаем токен 50256 и  причина, по которой это работает, заключается в том, что на самом деле это не проходило через слияния bpe, а вместо этого код, который фактически пересылал токены, имеет специальные 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4885)~~  инструкции для обработки специальных токенов, гм, мы не видели этих специальных инструкций для обработки специальных токенов в кодировщике, они там отсутствуют но если вы зайдете в библиотеку технических токенов, которая реализована в Rust, вы найдете все виды обработки особых случаев для этих специальных токенов, которые вы можете зарегистрировать, э-э, создавать добавления в словарь, а затем она ищет их, и это происходит всякий раз, когда они их видят.  специальные токены, подобные этому, он на самом деле придет и заменяет этот специальный токен, так что эти вещи находятся за пределами типичного алгоритма кодирования B PA, поэтому эти специальные токены используются повсеместно, а не только в базовом моделировании предсказания следующего  токен в последовательности, но особенно когда дело доходит до стадии тонкой настройки и всех чатов, э-э, gbt, его аспектов, а, потому что мы не просто хотим ограничивать документы Del, мы хотим разграничить целые разговоры между помощником и  пользователь, поэтому, если я обновлю эту страницу токенизатора sck, то 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=4944)~~  пример по умолчанию, который они здесь имеют, использует не какие-то кодеры базовой модели, а модель ftuned, э-э, своего рода токенизаторы, например, с использованием турбо-схемы GPT 3.5, вот все специальные токены, я  начало, я конец и т. д., кстати, это сокращение от Imaginary mcore start, но здесь вы можете видеть, что есть своего рода начало и конец каждого отдельного сообщения, и может быть много других токенов, много токенов, гм, используется для разграничения этих разговоров  и как бы отслеживать поток сообщений здесь, теперь мы можем вернуться в библиотеку токенов Tik, и здесь, когда вы прокручиваете вниз, они говорят о том, как вы можете расширить токен тика, и я могу, вы можете создать, в принципе, вы можете сделать форк, э-э базовые токенизаторы UM CL 100K в gp4, и, например, вы можете расширить его, добавив больше специальных токенов, и это полностью зависит от вас, вы можете придумать любые произвольные токены и впоследствии добавить их с новым идентификатором, и библиотека tikken будет работать правильно  поменяйте их местами, когда 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5010)~~  он увидит это в строках, теперь мы также можем вернуться к этому файлу, который мы рассматривали ранее, и я упомянул, что gpt2 в Tik toen открывает IP, у нас есть словарь, у нас есть шаблон для разделения, а затем  здесь мы регистрируем один специальный токен в gpd2, который был концом текстового токена, и мы увидели, что он имеет этот идентификатор в GPT 4, когда они игнорируют это. Здесь вы видите, что шаблон изменился, как мы обсуждали, но также и специальные токены  изменились в этом токенизаторе, поэтому у нас, конечно, есть конец текста, как в gpd2, но мы также видим здесь три, извините, четыре дополнительных токена. Префикс Thim в середине и суффикс, что такое fim, fim - это сокращение для заполнения в середине, и если хотите  чтобы узнать больше об этой идее, она взята из этой статьи, и я не буду вдаваться в подробности в этом видео, это выходит за рамки этого видео, и здесь есть еще один токен обслуживания, так что это тоже кодировка, так что в основном это очень распространено  обучите языковую модель, а затем, если хотите, 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5075)~~  вы можете добавить специальные токены, теперь, когда вы добавляете специальные токены, вам, конечно, придется выполнить некоторые операции с моделью Трансформера и всех параметров, задействованных в этом Трансформере, потому что вы, по сути, добавляете  целое число, и вы хотите убедиться, что, например, ваша матрица внедрения для токенов словаря должна быть расширена путем добавления строки, и обычно эта строка будет инициализирована небольшими случайными числами или чем-то в этом роде, потому что нам нужен вектор, который теперь  обозначает этот токен, в дополнение к этому вам нужно перейти на последний уровень Трансформера и убедиться, что эта проекция в самом конце классификатора, э-э, также расширена на единицу, так что, по сути, требуется некоторая модельная операция, которая  вам придется внести изменения в токенизацию, если вы собираетесь добавлять специальные токены, но это очень распространенная операция, которую люди делают, особенно если они хотят точно настроить модель, например, перенеся ее из базовой модели в модель чата, 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5126)~~  например  чат GPT, окей, на данный момент у вас должно быть все необходимое для создания собственного токенизатора gp4, сейчас идет процесс разработки этой лекции. Я сделал это и опубликовал код в этом репозитории MBP, поэтому MBP прямо сейчас выглядит так  пока я записываю, но репозиторий MBP, вероятно, немного изменится, потому что я намерен продолжать над ним работать, в дополнение к репозиторию MBP, я опубликовал прогресс этого упражнения, за которым вы можете следить, так что, если вы пойдете на упражнение  .  Доктор, это своего рода то, что я разбиваю задачу, стоящую перед вами, на четыре шага, которые как бы создают токенизатор gp4, и поэтому не стесняйтесь точно следовать этим шагам и следовать небольшим указаниям, которые я  выложил здесь, и в любое время, когда вы почувствуете, что застряли, просто обратитесь к репозиторию MBP здесь, чтобы либо тесты могли быть полезны, либо сам репозиторий MBP. Я стараюсь, чтобы код был достаточно чистым и понятным, и поэтому не стесняйтесь ссылаться на него всякий раз, когда 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5189)~~  вы получите застрял, кроме того, в основном, как только вы это напишете, вы сможете воспроизвести это поведение из технического токена, поэтому, получив токенизатор gb4, вы можете взять его, вы можете закодировать строку, и вы должны получить эти токены, а затем вы можете закодировать и декодировать  точно такая же строка для ее восстановления, и в дополнение ко всему этому вы должны иметь возможность реализовать свою собственную функцию поезда, которую библиотека токенов Tik не предоставляет, это опять же только код вывода, но вы можете написать свой собственный поезд, MBP тоже это делает, и это  позволит вам обучать свои собственные словари токенов, так что вот часть кода внутри M bemean bpe, э-э, показывает словари токенов, которые вы можете получить, так что слева, а здесь у нас есть слияния GPT 4, а, так что первые 256 являются необработанными отдельными  байт, а затем здесь я визуализирую слияния, которые gp4 выполнил во время обучения, поэтому самое первое слияние, которое выполнил gp4, заключалось в объединении 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5245)~~  двух пробелов в один токен, поскольку вы знаете два пробела, и это токен 256, и это порядок, в котором  все объединялось во время обучения gb4, и это порядок слияния, который мы получаем в MBP путем обучения токенизатора, и в данном случае я обучал его на странице Тейлор Свифт в Википедии, ну, не потому, что я Swifty, а потому, что это один из  самые длинные страницы Википедии, очевидно, доступны, но она довольно крутая, и что я собирался сказать, да, так что вы можете сравнить эти два словаря, и вот, например, здесь GPT для слияния I, чтобы войти, и мы сделали  то же самое с этим токеном 259, здесь пробел t становится пробелом t, и это произошло и с нами немного позже, так что разница здесь, насколько я понимаю, снова заключается только в разнице обучающего набора, так что в качестве примера, потому что я вижу много  пробел Я подозреваю, что gp4, вероятно, имел много кода Python в своем обучающем наборе. Я не уверен, ну, насчет токенизатора, и здесь мы видим гораздо меньше этого, конечно, на странице Википедии, так что, 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5313)~~  грубо говоря, они выглядят одинаково, и они  выглядят одинаково, потому что они используют один и тот же алгоритм, и когда вы тренируете свой собственный, вы, вероятно, получите что-то похожее в зависимости от того, чему вы его обучаете, хорошо, поэтому теперь мы собираемся перейти от тикающего токена и способа его открытия  ИИ токенизирует свои строки, и мы собираемся обсудить еще одну часто используемую библиотеку для работы с токенизацией inlm — это фрагмент предложения, поэтому фрагмент предложения очень часто используется в языковых моделях, потому что в отличие от токена Tik он может выполнять как обучение, так и вывод, и довольно эффективен в обоих случаях, он поддерживает ряд алгоритмов для обучения словарных запасов, но один из них - это алгоритм кодирования B-пары, который мы рассматривали, поэтому он поддерживает его теперь, часть предложения используется как ламой, так и сериями мистал и многими другими  модели также есть на GitHub под частью предложения Google, и большая разница с частью предложения, и мы собираемся рассмотреть пример, 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5366)~~  потому что это довольно сложно и тонко объяснить, заключается в том, что они по-разному думают о порядке операций здесь, поэтому в В случае с токеном Tik мы сначала берем наши кодовые точки в строке, мы кодируем их с помощью mutf в байты, а затем объединяем байты, это довольно просто для фрагмента предложения, гм, это работает непосредственно на уровне самих кодовых точек, так что это выглядит  в любых кодовых точках, доступных в вашем обучающем наборе, а затем он начинает объединять эти кодовые точки, и, хм, bpe работает на уровне кодовых точек, и если у вас заканчиваются кодовые точки, возможно, есть некоторые редкие кодовые точки, которые  просто не появляйтесь слишком часто, и редкость определяется этим гиперпараметром покрытия символов, тогда эти кодовые точки будут либо сопоставлены со специальным неизвестным токеном, например ank, либо, если у вас включена опция укуса, тогда это примет эти  редкие точки Cod, он закодирует их с использованием utf8, а затем отдельные байты этой кодировки будут преобразованы в токены, и есть 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5430)~~  эти специальные токены, которые в основном добавляются в словарь, поэтому он использует BP для кодовых точек, а затем возвращается обратно  на байты для редких точек кода, гм, и это своего рода разница, лично я считаю, что токен Tik значительно чище, но это своего рода тонкая, но довольно существенная разница между тем, как они подходят к токенизации, давайте поработаем с конкретным примером, потому что в противном случае  это довольно сложно уложить в голове, так что давайте поработаем с конкретным примером, вот как мы можем импортировать часть предложения, а затем мы возьмем, я думаю, я взял описание части предложения и просто создал  как небольшой игрушечный набор данных, ему очень нравится иметь файл, поэтому я создал игрушку.  txt с этим содержимым, что немного безумно в части предложения, так это то, что существует масса опций и конфигураций, и причина этого в том, что часть предложения существует, я думаю, уже некоторое время, и она действительно пытается справиться с  большое разнообразие вещей и, хм, потому что это было вокруг, я 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5488)~~  думаю, что у него довольно много накопленного исторического багажа, ну, и поэтому, в частности, есть тонна аргументов по конфигурации, это даже не все, вы можете пойти сюда, чтобы увидеть все варианты обучения, хм и хм, есть также весьма полезная документация, когда вы смотрите на необработанный бафф Прото, хм, который используется для представления спецификации тренера и так далее, хм, многие из этих опций для нас не имеют значения, так что, возможно, стоит указать на один пример сокращения Das Das  Фактор, э-э, этот коэффициент сокращения не используется в алгоритме кодирования B-пары, так что это просто аргумент, который для нас не имеет значения, хм, он алгоритму обучения, теперь я пытался сделать здесь часть предложения в способ, который, насколько я могу судить, очень похож, возможно, идентичен тому, как лама 2 была напряжена, поэтому способ, которым они обучали свой собственный токенизатор, и то, как я это делал, по сути, можно было использовать модель токенизатора  файл, который был выпущен мета, и вы 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5551)~~  можете открыть его, используя файл типа Proto protuff, который вы можете сгенерировать, а затем вы можете проверить все параметры, и я попытался скопировать все параметры, которые выглядели релевантными, поэтому здесь мы настраиваем ввод, это  необработанный текст в этом файле будет результатом, поэтому для разговора это будет 400. модель и словарный запас, мы говорим, что собираемся использовать алгоритм BP, и мы хотим, чтобы размер Bap был равен 400, тогда есть масса  конфигурации для основных правил предварительной обработки и нормализации, поскольку они называются нормализацией, раньше были очень распространены, я бы сказал, что до llms в обработке естественного языка, так что в машинном переводе, классификации текста и т. д. вы хотите нормализовать и упростить  текст, и вы хотите перевести его в нижний регистр, и вы хотите удалить все двойные пробелы и т. д., и в языковых моделях мы предпочитаем ничего из этого не делать, или, по крайней мере, я предпочитаю это как человек с глубоким обучением, вы не хотите трогать свои данные  вы хотите 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5609)~~  сохранить как можно больше необработанных данных в необработанной форме, поэтому вы, по сути, пытаетесь отключить многое из этого, если можете. Еще одна вещь, которую делает этот фрагмент предложения, это то, что он имеет концепцию предложений, поэтому фрагмент предложения  он вернулся, он вроде как был разработан, я думаю, в начале тех дней, когда возникла идея, что вы тренируете токенизатор на куче независимых предложений, так что во многом это похоже на то, сколько предложений вы собираетесь обучить  о том, какова максимальная длина предложения, то есть перетасовка предложений, и поэтому для него предложения похожи на отдельные обучающие примеры, но опять же в контексте фильмов я нахожу, что это похоже на очень странное и странное различие, например, предложения такие же, как « не надо»  прикоснитесь, предложения с необработанными данными существуют, но в наборах необработанных данных есть много чего-то вроде inet, например, что такое предложение, что не является предложением, и поэтому я думаю, что действительно сложно определить, что такое фактическое 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5666)~~  предложение, если вы  мне действительно нравится вникать в это, и на разных языках могут быть разные концепции или что-то в этом роде, так зачем даже вводить эту концепцию, если честно, для меня это не имеет смысла, я бы просто предпочел рассматривать файл как гигантский, э-э, поток байты, в нем много обработки редких символов слова, и когда я говорю слово, я имею в виду кодовые точки, мы вернемся к этому через секунду, и у него есть много других правил для гм, в основном разделение цифр, разделение пробелов и чисел  и как вы с этим справляетесь, так что это что-то вроде правил слияния, так что я думаю, что это немного эквивалентно маркеру отметки, использующему регулярное выражение для разделения категорий, это своего рода эквивалентность, если вы прищуритесь T это в части предложения  где вы также можете, например, разделить разделить цифры, ну и так далее, здесь есть еще несколько вещей, к которым я вернусь через некоторое время, а затем есть несколько специальных токенов, которые вы можете указать, и это жестко закодирует 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5723)~~  токен ООН  начало предложения, конец предложения и токен блокнота и токен ООН, насколько я понимаю, должны существовать, а затем еще кое-что, чтобы мы могли тренироваться, и когда я нажимаю поезд, он создаст этот файл, разговор 400. модель и разговор 400.  wab, затем я могу загрузить файл модели и проверить его словарный запас, и поэтому мы обучили размер словарного запаса 400 на этом тексте здесь, и это отдельные части, отдельные лексемы, которые создаст этот фрагмент предложения, поэтому вначале мы видим, что у нас есть  токен с нулевым идентификатором, тогда у нас есть начало последовательности, конец первой и второй последовательности, а затем мы сказали, что идентификатор площадки имеет отрицательное значение 1, поэтому мы решили не использовать его, поэтому здесь нет идентификатора площадки, тогда это отдельные укусы токены, поэтому здесь мы увидели, что резервный вариант укуса в ламе был включен, так что это правда, поэтому далее будут 256 далее будут 256 идентификаторы, а затем внизу, после токенов укуса, идут 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5798)~~  слияния, и это родительские узлы  в слияниях, поэтому мы не видим детей, мы просто видим родителей и их идентификаторы, а затем, после слияний, в конечном итоге появляются отдельные токены и их идентификаторы, и это отдельные токены, поэтому это отдельные токены кода, если  вы это сделаете, и они появятся в конце, так что это порядок, в котором часть предложения вроде как представляет свои словари. Он начинается со специальных токенов, затем жетонов велосипеда, затем токенов слияния, а затем отдельных токенов кодо, и все эти необработанные кодовые точки для токенов  те, с которыми он столкнулся в обучающем наборе, поэтому эти отдельные кодовые точки представляют собой весь набор кодовых точек, которые произошли здесь, поэтому все они помещаются туда, а затем те, которые чрезвычайно редки, что определяется покрытием символов, поэтому, если кодовая точка  произошло только один раз из миллиона предложений или чего-то в этом роде, тогда оно будет проигнорировано и не будет добавлено в наш проигнорировано и не будет добавлено в наш 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5861)~~  словарь, как только у нас будет словарь, который мы можем закодировать в идентификаторы, и мы сможем, ммм, получить список  а затем здесь я также декодирую отдельные токены обратно на маленькие кусочки, как они это называют, так что давайте посмотрим, что здесь произошло, привет, пространство, так что это идентификаторы токенов, которые мы получили обратно, и когда мы посмотрим здесь, э-э, кое- что вроде  э-э, сразу на ум, взгляните на эти символы, корейские символы, конечно, не были частью обучающего набора, поэтому фрагмент предложения сталкивается с кодовыми точками, которые он не видел во время обучения, и с этими кодовыми точками не связан токен  с ними так внезапно это un-токены, неизвестные токены, но поскольку укус возвращается как истинный, вместо этого фрагмент предложения возвращается к байтам, и поэтому он берет это, кодирует его с помощью utf8, а затем использует эти токены для представления этих байтов, и это то, что мы получается что-то вроде здесь, это кодировка utf8, э-э, и в ней она сдвинута на три, э-э, 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5933)~~  из-за этих специальных токенов, которые раньше имели идентификаторы, так что вот что произошло здесь, сейчас еще одна вещь, которую, ммм, сначала, прежде чем я продолжу в отношении bitef back позвольте мне удалить фолдбэк укуса, если это ложь, что произойдет, давайте переобучим, чтобы первое, что произошло, это все токены укуса исчезли правильно, и теперь у нас есть только слияния, и теперь у нас гораздо больше слияний, потому что у нас их много  больше места, потому что мы не занимаем место в размере wab со всеми байтами, и теперь, если мы закодируем это, мы получим ноль, так что вся эта строка здесь внезапно не имеет обратного бита, так что это неизвестно, а неизвестное - это и так это  равно нулю, потому что токен an равен нулю, и вы должны иметь в виду, что это будет учитываться в вашей языковой модели, так что же должна делать языковая модель, когда всевозможные вещи, которые не распознаются, потому что они редки, просто заканчиваются  отображение в Unk, это не совсем то свойство, которое вам нужно, поэтому я думаю, что 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=5998)~~  лама правильно используется в качестве резервного варианта, правда, потому что мы определенно хотим ввести эти неизвестные или редкие кодовые точки в модель и каким-то образом, следующее, что я  Я хочу показать вам следующее примечание, когда мы декодируем все отдельные токены. Вы видите, как пробелы, э-э, пробелы здесь в конечном итоге становятся этим жирным подчеркиванием, я не уверен на 100%, кстати, почему часть предложения переключает пробелы на эти жирные символы подчеркивания, возможно, это для визуализации. Я не уверен на 100&nbsp;%, почему это происходит, но обратите внимание, почему у нас есть дополнительное пространство перед приветом, откуда это взялось, ну, это взято из этой опции здесь, ммм, добавьте фиктивный префикс  верно, и когда вы переходите к документации, добавьте пробелы D в начале текста, чтобы обрабатывать World in world и hello world точно так же, поэтому мы пытаемся сделать следующее, если мы вернемся к нашей галочке мир токенизатора, так как токен сам по себе имеет другой идентификатор, чем космический мир, поэтому у 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6070)~~  нас есть 1917 год, но это 14 и т. д., так что это два разных токена для языковой модели, и языковая модель должна узнать из данных, что они на самом деле являются своего рода  концепция очень похожа на языковую модель в мире токенов Tik, в основном слова в начале предложений и слова в середине предложений на самом деле выглядят совершенно по-разному, и он должен усвоить, что они примерно одинаковы, поэтому это добавляет Дами.  префикс пытается немного с этим бороться, и это работает так: он, по сути, добавляет фиктивный префикс, поэтому в качестве части предварительной обработки он возьмет строку и добавит пробел, он сделает это и  это сделано для того, чтобы сделать этот мир и тот мир одинаковыми, они оба будут космическими мирами, так что включен еще один вид опции предварительной обработки, и Лама 2 также использует эту опцию, и это, я думаю, все, что я хочу  чтобы сказать о моем предварительном просмотре фрагмента предложения и о том, чем он отличается, может быть, здесь я просто вставил представление буфера протокола Raw, в основном 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6140)~~  токенизатор, слишком обученный, так что не стесняйтесь как-то пройти через это, и если  вы хотели бы, чтобы ваша токенизация выглядела идентично мета-э-э-э-э-ламе 2, тогда вам нужно скопировать и вставить эти настройки, как я пытался сделать выше, и да, я думаю, что это все для этого раздела, я думаю, что мое резюме для части предложения  из всего этого я думаю, что в предложении много исторического багажа, много концепций, которые, на мой взгляд, немного сбивают с толку, и я думаю, что потенциально они могут содержать пешие пистолеты, такие как эта концепция предложения, его максимальная длина и тому подобное.  что, хм, в противном случае он довольно часто используется в отрасли, хм, потому что он эффективен и может выполнять как обучение, так и вывод, хм, у него есть несколько особенностей, таких как, например, должен существовать токен, и то, как выполняются резервные варианты укуса, и так далее, я не знаю  Это кажется особенно элегантным, и, к сожалению, я должен сказать, что это не очень хорошо документировано, поэтому мне потребовалось 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6191)~~  много времени, чтобы самому работать с этим, просто визуализировать вещи и пытаться по -настоящему понять, что здесь происходит, потому что, к сожалению, документация, по моему мнению,  не супер, но это очень хороший репозиторий, который доступен вам, если вы хотите обучить свой собственный токенизатор прямо сейчас, хорошо, позвольте мне снова переключиться, поскольку мы начинаем медленно завершать работу, я хочу вернуться к этому  немного подробнее о том, как нам следует устанавливать размер вокала и каковы некоторые соображения по этому поводу, поэтому для этого я хотел бы вернуться к архитектуре модели, которую мы разработали в последнем видео, когда создавали GPT из царапина, так что это был файл, который мы создали в предыдущем видео, и мы определили модель Transformer, и давайте конкретно посмотрим на размер Bap и на то, где он появляется в этом файле, поэтому здесь мы определяем размер voap, э-э, в то время он был 65  или что-то в этом роде, чрезвычайно маленькое число, поэтому оно станет намного больше, и вы увидите, что размер Bap не 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6244)~~  слишком сильно увеличивается в большинстве этих слоев, единственное место, где он появляется, - это именно эти два места здесь, поэтому, когда мы Определите языковую модель: есть таблица встраивания токенов, которая представляет собой двумерный массив, где размер вокала - это, по сути, количество строк, и каждый словарный элемент, каждый токен имеет вектор, который мы собираемся обучить, используя обратное распространение, которое имеет вектор. размер и встраивание, которое представляет собой количество каналов в Transformer, и, по сути, по мере увеличения размера voap эта таблица встраивания, как я упоминал ранее, также будет расти, мы собираемся добавлять строки в дополнение к этому в конце Transformer, есть этот LM верхний слой, который является линейным слоем, и вы заметите, что этот слой используется в самом конце для создания логитов, которые становятся вероятностями для следующего токена в последовательности, и поэтому интуитивно мы пытаемся создать вероятность для каждого отдельного слоя.  токен, который 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6296)~~  может появиться следующим в каждый момент времени этого Трансформера, и если у нас будет все больше и больше токенов, нам нужно будет создавать все больше и больше вероятностей, поэтому каждый отдельный токен будет представлять собой дополнительное скалярное произведение, которое мы должны сделать здесь, в этой линейной  слой для этого последнего слоя в Transformer, так почему же размер вокала не может быть бесконечным, почему мы не можем вырасти до бесконечности, ну номер один, ваша таблица встраивания токенов будет расти, а ваш линейный слой будет расти, поэтому мы собираемся здесь нужно делать гораздо больше вычислений, потому что этот головной слой LM станет более затратным в вычислительном отношении, номер два, потому что у нас больше параметров, и мы можем волноваться, что нам придется выполнять некоторые из этих параметров интуитивно, если у вас очень большой размер словаря.  скажем, у нас есть миллион токенов, тогда каждый из этих токенов будет появляться в обучающих данных все реже и реже, потому что повсюду гораздо больше других токенов, 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6347)~~  и поэтому мы будем видеть все меньше и меньше примеров.  э-э, для каждого отдельного токена, и вы можете быть обеспокоены тем, что в результате векторы, связанные с каждым токеном, будут недостаточно обучены, потому что они просто не возникают слишком часто и не участвуют в прямом обратном проходе в дополнение к этому, поскольку размер вашего словарного запаса увеличивается, вы начнете сильно сжимать свои последовательности, и это действительно хорошо, потому что это означает, что мы будем уделять все больше и больше внимания тексту, и это хорошо, но также вы можете беспокоиться о том, что два больших куска сжимаются в отдельные токены, и поэтому у модели просто не так много времени, чтобы подумать о каком-то количестве символов в тексте, или вы можете думать об этом правильно, так что по сути мы сжимаем слишком много информации  в один токен, а затем прямого прохода Transformer недостаточно для правильной обработки этой информации, и поэтому это некоторые из соображений, о которых вы думаете, когда разрабатываете 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6399)~~  размер словаря, как я уже упоминал, это в основном эмпирический  гиперпараметр, и похоже, что сегодня в современных архитектурах оно обычно составляет около 10 000 или где-то около 100 000 сегодня, и следующее соображение, о котором я хочу кратко поговорить, - это что, если мы хотим взять предварительно обученную модель и  мы хотим расширить размер вокала, и на самом деле это делается довольно часто, например, когда вы выполняете тонкую настройку для cha GPT, хм, поверх базовой модели добавляется гораздо больше новых специальных токенов для поддержки метаданных и всего остального. структура объектов диалога между пользователем и помощником, поэтому для этого требуется много специальных токенов, вы также можете попробовать добавить более специальные токены, например, для использования браузера или любого другого инструмента, и поэтому очень заманчиво добавить много токенов для  все виды специальных функций, поэтому, если вы хотите добавить токен, это вполне возможно. Все, что нам нужно сделать, это изменить размер этого встраивания, чтобы нам нужно было 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6450)~~  добавить строки, мы бы инициализировали эти параметры с нуля, чтобы они были небольшими случайными числами и  затем нам нужно расширить вес внутри этой линейной линии, поэтому нам нужно начать производить скалярное произведение со связанными параметрами, а также в основном вычислить вероятности для этих новых токенов, так что оба из них - это просто операция изменения размера, это очень мягкая модельная операция  и это можно сделать довольно легко, и довольно часто вы замораживаете базовую модель, вводите эти новые параметры, а затем тренируете эти новые параметры только для введения новых токенов в архитектуру, гм, и поэтому вы можете заморозить произвольные ее части или вы  может обучать произвольные его части, и это полностью зависит от вас, но в основном требуется небольшая операция, если вы хотите ввести новые токены, и, наконец, я хотел бы упомянуть, что на самом деле существует целое пространство разработки приложений с точки зрения введения новых токенов в словарь, который выходит далеко за рамки простого 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6499)~~  добавления специальных токенов и специальных новых функций, просто чтобы дать вам представление о пространстве дизайна, но это может быть целое видео само по себе, хм, это статья о том, как научиться сжимать подсказки с помощью того, что они называют хм  суть токенов, и грубая идея состоит в том, что вы используете языковые модели в условиях, требующих очень длинных подсказок, в то время как эти длинные подсказки просто замедляют все, потому что вам нужно их закодировать, а затем вы должны их использовать, а затем вы стремитесь  над ними, и просто, вы знаете, тяжело иметь очень большие подсказки, поэтому вместо этого здесь, в этой статье, они вводят новые токены, и представьте, что у вас есть несколько новых токенов, вы помещаете их в последовательность, а затем тренируете модель с помощью  дистилляция, поэтому вы сохраняете всю модель замороженной и тренируете только представления новых токенов, их вложения, и вы оптимизируете новые токены так, чтобы поведение языковой модели было идентично модели, которая имеет 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6555)~~  очень  длинное приглашение, которое работает для вас, и поэтому это метод сжатия, заключающийся в сжатии этого очень длинного приглашения в эти несколько новых токенов сути, и вы можете обучить его, а затем во время тестирования вы можете отказаться от своего старого приглашения и просто заменить эти токены, и они сортируются  типа, э-э, заменяют это очень длинное приглашение и имеют почти идентичную производительность, и это один из методов и класс методов точной настройки с эффективными параметрами, где большая часть модели в основном фиксирована и нет тренировки весов модели, там нет  никакого обучения Лоры или чего-то подобного новым параметрам. Параметры, которые вы тренируете, теперь представляют собой просто вставки токенов, так что это всего лишь один пример, но это снова может быть похоже на целое видео, но просто для того, чтобы дать вам ощущение, что есть  Здесь есть все пространство для дизайна, которое потенциально стоит изучить в будущем. Следующее, о чем я хочу кратко сказать, это то, что, я думаю, в последнее время 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6601)~~  наблюдается большой импульс в том, как вы на самом деле можете создавать Трансформеры, которые могут одновременно обрабатывать не только текст в качестве входной модальности, но и  множество других модальностей, будь то изображения, видео, аудио и т. д., и как вы подаете все эти модальности и потенциально предсказываете эти модальности из Трансформера, вам нужно каким-то фундаментальным образом изменить архитектуру, и я думаю, с чего начинают многие люди  к чему сходиться, это то, что вы не меняете архитектуру, которую вы придерживаетесь с помощью Transformer, вы просто как бы токенизируете свои входные домены, а затем называете день и притворяетесь, что это просто текстовые токены, и просто делаете все остальное идентичным идентичным образом, так что здесь, например была ранняя статья с красивой иллюстрацией того, как вы можете взять изображение и разбить его на целые числа, хм, и иногда они, х, так что в основном они станут токенами изображений в качестве примера, и эти токены могут быть, эх, жесткими токенами  где вы 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6652)~~  заставляете их быть целыми числами, они также могут быть программными токенами, где вы, э-э, вроде как не требуете, чтобы они были дискретными, но вы заставляете эти представления проходить через узкие места, как в автоматических кодировщиках, а также в этой статье, вышедшей из  откройте SORA, который, я думаю, действительно поразил многих людей и вдохновил многих людей с точки зрения того, что возможно, у них здесь есть графика, и они кратко рассказывают о том, как в фильмах есть текстовые токены. У Соры есть визуальные патчи, поэтому они снова всплыли. с возможностью разбить видео на токены, когда они владеют словарями, а затем вы можете либо обрабатывать дискретные токены, скажем, с помощью автореггрессивных моделей, либо даже программные токены с диффузионными моделями, и все это своего рода э-э-э, над чем активно работают, разработано и  выходит за рамки этого видео, но я хотел кратко упомянуть лишь то, что теперь, когда мы довольно глубоко углубились в алгоритм токенизации и понимаем гораздо больше о том, как он работает, давайте вернемся 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6709)~~  к началу этого видео и пройдемся по нему.  некоторые из этих пунктов и действительно понять, почему они происходят, и, во- первых, почему мой llm не может очень хорошо писать слова или выполнять другие очень хорошо писать слова или выполнять другие задачи, связанные с заклинаниями, так что по сути это потому, что, как мы видели, эти символы разбиты на токены, и некоторые из них  токены на самом деле довольно длинные, поэтому в качестве примера я обратился к словарю gp4 и посмотрел на один из самых длинных токенов, так что стиль по умолчанию оказался одним отдельным токеном, так что для одного токена много символов, так что мое подозрение  в том, что в этот единственный токен втиснуто слишком много всего, и я подозревал, что модель не должна очень хорошо справляться с задачами, связанными с написанием этого одного токена, поэтому я спросил, сколько букв L в стиле слова по умолчанию и, конечно же,  мое приглашение намеренно сделано таким образом, и вы видите, что стиль по умолчанию будет одним токеном, так что это то, что 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6767)~~  видит модель, поэтому я подозреваю, что она не будет очень хороша в этом, и на самом деле это не так, она на самом деле не знает  сколько там букв L, он думает, что их три, а на самом деле их четыре, если я сам не ошибаюсь, так что все прошло не очень хорошо, давайте посмотрим на другой вид задачи на уровне персонажа, поэтому, например, я спросил  э-э, gp4, чтобы изменить стиль строки по умолчанию, и они попытались использовать интерпретатор кода, и я остановил его и сказал, просто сделай это, просто попробуй, и это привело меня в замешательство, так что он на самом деле не знает, как перевернуть эту строку, начиная с  справа налево, ну, так что это дало неправильный результат, так что снова похоже на работу с этой рабочей гипотезой, что, возможно, это связано с токенизацией. Я попробовал другой подход. Я сказал: хорошо, давайте перевернем ту же самую строку, но выполним следующий подход, шаг первый, просто распечатайте  каждый отдельный символ разделен пробелами, а затем на втором этапе переверните этот список, и 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6823)~~  он снова попытался использовать инструмент, но когда я остановил его, он сначала создал все символы, и это было на самом деле правильно, а затем он перевернул их, и это было правильно  когда-то у него это было, так что каким- то образом он не может отменить это напрямую, но когда вы сначала начинаете, ну, вы знаете, перечисляете это, чтобы он мог это как-то сделать, а затем он может, как только он разбивается таким образом, это становится всеми этими отдельными персонажами и  так что теперь ему намного проще увидеть эти отдельные токены, перевернуть их и распечатать, так что это довольно интересно, так что давайте продолжим, почему llms хуже говорят на неанглийском языке, и я уже кратко рассмотрел это, но в основном это так.  не только языковая модель видит меньше неанглоязычных данных во время обучения параметров модели, но и токенизатор не достаточно обучен неанглоязычным данным, и поэтому здесь, например, привет, как дела, это пять токенов и их перевод  15 жетонов, так что это тройное увеличение, и, например, 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6886)~~  ананг - это просто привет по- корейски, и в итоге получается три жетона. Я на самом деле немного удивлен этим, потому что это очень распространенная фраза, просто типичное приветствие. типа «привет», и в конечном итоге это три токена, тогда как наш «привет» — это один токен, и поэтому в основном все намного более раздуто и разбросано, и это, я думаю, отчасти причина того, что модель работает хуже на других языках, ну, возвращаясь, почему LM  плохо разбираюсь в простой арифметике, которая связана с токенизацией чисел, и вы заметите, что, например, сложение очень похоже на то, что есть алгоритм, похожий на уровень символов для выполнения сложения, поэтому, например, здесь мы сначала добавили бы единицы, затем десятки, а затем сотни, вы должны ссылаться на определенные части этих цифр, но эти числа представлены совершенно произвольно в зависимости от того, что произошло слияние или не слияние во время процесса токенизации, об этом есть целый пост в блоге, который я  Думаю, это неплохо, токенизация целых чисел - это 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=6945)~~  безумие, и этот человек в основном систематически исследует токенизацию чисел, я считаю, что это gpt2, и поэтому они замечают, что, например, для четырехзначных чисел на форуме вы можете посмотреть, является ли это, э-э,  один жетон или два жетона, то есть комбинация 1 три или 2 два или 31, и поэтому все разные числа - это все разные комбинации, и вы можете себе представить, что все это совершенно произвольно, и модель, к сожалению, иногда видит четыре  хм, токен для всех четырех цифр иногда для трех иногда для двух иногда для одного, и это происходит произвольным образом, и так что это определенно встречный ветер, если хотите, для языковой модели, и это просто невероятно, что она может это сделать  и справиться с этим, но это также не идеально, и поэтому, например, мы видели эту мета-мету, когда они обучают алгоритм Llama 2 и используют фрагмент предложения, они обязательно разделяют все хм все цифры в качестве примера для хм llama 2, и это отчасти сделано для улучшения производительности простых арифметических вычислений, и, наконец, почему gpt2 не 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7011)~~  так хорош в Python, опять же, это частично проблема моделирования в архитектуре, наборе данных и надежности модели, но это также частично токенизация.  потому что, как мы видели здесь на простом примере Python, эффективность кодирования токенизатора для обработки пробелов в Python ужасна, и каждое отдельное пространство является отдельным токеном, и это значительно уменьшает длину контекста, который модель может пересечь, так что это почти похоже на ошибка токенизации для gpd2, которая позже была исправлена ​​​​в gp4, окей, вот еще одна забавная вещь, моя llm внезапно останавливается, когда видит конец строки текста, так что вот хм, вот очень странное поведение: напечатать конец строки текста - это то, что я сказал jt4, и это  говорит, не могли бы вы указать строку, и я говорю, что он дает мне конец текста, и кажется, что есть проблема, он не видит конец текста, а затем я даю ему конец текста, это строка, а затем вот строка  а затем он просто не печатает его, поэтому очевидно, что здесь что-то сломалось в отношении обработки 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7068)~~  специального токена, и я на самом деле не знаю, что open ey делает здесь под капотом и потенциально ли они анализируют это как гм  как настоящий токен, а не просто конец текста, ну, как отдельные его части без специальной логики обработки токенов, и поэтому может случиться так, что кто-то, когда они звонят, кодирует, э-э, они передают разрешенные специальные  и они разрешают конец текста в качестве специального символа в пользовательском приглашении, но пользовательское приглашение, конечно, представляет собой своего рода текст, контролируемый злоумышленником, поэтому вы можете надеяться, что они на самом деле не анализируют и не используют специальные токены, или вы знаете из этого  своего рода вводные данные, но кажется, что здесь определенно что-то идет не так, и поэтому ваши знания об этих специальных токенах потенциально могут оказаться в налоговой поверхности, и поэтому, если вы хотите запутать llms, тогда просто попробуйте дать им несколько специальных токенов  и посмотрите, не сломаете ли вы что-нибудь случайно, 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7126)~~  окей, следующий урок действительно забавный, хм, проблема с конечным белым пространством, так что, если вы придете на игровую площадку, и хм, мы придем сюда, чтобы проинструктировать GPT 3.5 Turbo, так что это не модель чата, это  это модель завершения, поэтому думайте о ней больше как о том, что она намного ближе к базовой модели, она завершает, она продолжит последовательность токенов, поэтому вот слоган для магазина мороженого, и мы хотим продолжить последовательность, чтобы мы могли отправить и получить куча жетонов, ок, без проблем, но теперь предположим, что я это сделаю, но вместо того, чтобы нажимать «Отправить», я делаю вот слоган для места в магазине мороженого, поэтому у меня здесь есть место, прежде чем я нажму « Отправить», мы получим предупреждение, что ваш текст заканчивается в конце Линг  пространство, которое приводит к ухудшению производительности из-за того, что API разбивает текст на токены, поэтому то, что здесь происходит, все равно дает нам своего рода завершение, но давайте посмотрим, что 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7183)~~  происходит, поэтому вот слоган для магазина мороженого, а затем как это выглядит как и в реальных данных обучения, предположим, что вы нашли завершение учебного документа где-то в Интернете, и фильм обучился на этих данных, так что, возможно, это что-то вроде «о да, может быть, это ужасный слоган, но обратите внимание, что когда я создаю o  вы видите, что, поскольку символ пробела всегда является префиксом этих токенов в GPT, так что это не токен O, а токен пробела o, пробел является частью O, и вместе они составляют токен 8840, это пробел o, так что же происходит  Вот что, когда у меня это просто есть, и я позволяю ему завершить следующий токен, он может выбрать токен пробела, но вместо этого, если у меня есть это, и я добавляю свое пространство, то, что я делаю здесь, когда я кодирую эту строку, я по сути, здесь есть линия для магазина мороженого, и это место в самом конце становится токеном 220, и поэтому мы добавили токен 220, и в противном случае этот токен был бы частью слогана, потому что, если здесь действительно есть 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7252)~~  слоган, так что пробел  o - это токен, и поэтому это внезапно становится распределением для модели, потому что это пространство является частью следующего токена, но мы помещаем его сюда вот так, и модель сама по себе видела очень-очень мало данных о реальном пространстве, и мы '  вы просите его завершить последовательность, например добавить больше токенов, но проблема в том, что мы вроде как начали первый токен, а теперь он был разделен, и теперь мы вышли из этого распределения, и теперь происходят произвольные плохие вещи, и это просто  очень редкий пример, когда он видит что-то подобное, и поэтому мы получаем предупреждение, поэтому основная проблема здесь, конечно, в том, что llm находится поверх этих токенов, а эти токены представляют собой текстовые фрагменты, они не являются символами в  как мы с вами думаем о них, это атомы того, что видит LM, и из этого выходит куча странных вещей, давайте вернемся к нашему стилю ячейки по умолчанию. Могу поспорить, что модель никогда не была в своем  обучающий набор 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7310)~~  видел стандартную ячейку без Ле, он всегда видел это как одну группу, потому что это какая-то функция в мм, я думаю, я на самом деле не знаю, что это за часть, это какой-то API  но держу пари, что он никогда не видел эту комбинацию токенов в своих обучающих данных, потому что или я думаю, что это будет крайне редко, поэтому я взял это, скопировал и вставил сюда, и я попытался завершить из него, и это сразу же дало  Я совершил большую ошибку, и он сказал, что модель, предсказанная для завершения, начинается с последовательности остановки, что приводит к отсутствию вывода, рассмотрите возможность настройки последовательности приглашений или остановок, поэтому здесь, когда я нажал кнопку « Отправить», произошло то, что модель сразу же выдала и что-то вроде конца текста токен, я думаю, или что-то в этом роде, он по сути сразу же предсказал последовательность остановки, поэтому он не завершился, и поэтому я снова получаю предупреждение, потому что мы отключены от распределения данных, а модель просто предсказывает совершенно произвольные вещи 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7368)~~  он просто очень сбит с толку, в общем, это э-э, это наносит ему повреждение мозга, он никогда такого не видел, прежде чем он в шоке, и он предсказывает конец текста или что-то в этом роде. Я попробовал это снова здесь, и в данном случае он завершил его, но потом по какой-то причине этот запрос Может быть  нарушают нашу политику использования, это было помечено, хм, по сути, что-то типа " не так", и есть что-то вроде Джанка, вы можете просто почувствовать Джанк, потому что модель крайне недовольна именно этим и не знает, как это завершить, потому что такого никогда не происходило в  обучающий набор в обучающем наборе всегда выглядит так и становится одним токеном, поэтому возникают такие проблемы, когда токены либо вы как бы завершаете первый символ следующего токена, либо у вас есть длинные токены, которые затем у вас есть.  только некоторые символы из всего этого похожи на проблемы с частичными токенами, вот как я бы это описал, и если вы действительно покопаетесь в репозитории токенов T, перейдите к коду ржавчины и 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7422)~~  найдите нестабильный код, и вы увидите код um en нестабильные собственные нестабильные токены и множество подобных особых случаев, ничего из этого о нестабильных токенах нигде не документировано, но есть тонна кода, работающего с нестабильными токенами, и нестабильные токены в точности похожи на то, что я описываю здесь, что вы бы сделали например, из API завершения - это нечто гораздо более причудливое, например, если мы добавляем в ячейку по умолчанию sta, если мы запрашиваем следующую последовательность токенов, мы на самом деле не пытаемся добавить следующий токен точно после этого списка, мы на самом деле пытаемся добавить, мы пытаемся рассмотреть множество токенов, хм, что если бы мы это делали или, я думаю, мы пытаемся выполнить поиск по символам, которые, если бы мы сохранили, имели бы высокую вероятность, если это имеет смысл, хм, так что мы действительно можем добавить  один отдельный символ, а не просто добавление следующего полного токена, который идет после этого частичного списка токенов, так что это 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7479)~~  очень сложно описать, и я приглашаю вас, возможно, просмотреть это, это в конечном итоге оказывается чрезвычайно корявой и запутанной темой.  это, по сути, связано с токенизацией, так что, возможно, я даже смогу потратить целое видео на разговоры о нестабильных токенах когда-нибудь в будущем, хорошо, и я действительно оставляю лучшее напоследок, мой любимый, безусловно, это Magikarp из чистого золота, и это нормально  Итак, это взято из этого сообщения в блоге, э-э, твердое золото Magikarp, и э-э, это сейчас известно в Интернете для тех из нас, кто работает в LLMS, и, по сути, я бы посоветовал вам э-э, прочитать этот блок-пост полностью, но в основном то, что делал этот человек, это то, что этот человек пошел  в конюшню для встраивания токенов UM и сгруппировал токены на основе их представления встраивания, и этот человек заметил, что есть кластер токенов, которые выглядят очень странно, поэтому есть кластер здесь, в rot estream. Слава из чистого золота. Сообщение Magikarp Signet, как действительно странные токены в э-э  в основном в этом 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7540)~~  кластере встраивания, и что это за жетоны и откуда они вообще берутся, например, что такое магикарпет из чистого золота, не имеет смысла, а затем они нашли кучу этих жетонов, а затем замечают, что на самом деле сюжет здесь усложняется, потому что, если вы спросите  Модель об этих токенах, как вы задаете, э-э, какой-то очень мягкий вопрос, например, пожалуйста, можете ли вы повторить мне, что строка продана за золото Магикарп, тогда вы получите множество практически полностью сломанных llm Поведение, так что либо вы получите уклонение, так что извините, я могу  не слышу тебя, или ты получишь кучу галлюцинаций в ответ, хм, ты даже можешь ответить, как оскорбления, так что ты спросишь это, хм, о стример-боте, он, х, рассказывает, а модель на самом деле просто обзывает вас, хм, или это вроде как придумывает типа странный юмор, будто ты на самом деле ломаешь модель, спрашивая об этих очень простых строках, как у Рота и продаваемого золота Мэджикарпа, типа, что, черт возьми, происходит, и здесь есть множество задокументированных вариантов поведения, э-э, есть 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7597)~~  куча токенов, не таких уж хороших Magikarp, у которых такое поведение, и, по сути, есть куча похожих триггерных слов, и если вы спросите модель об этих триггерных словах или просто включите их в подсказку, модель выйдет из строя и будет иметь все виды действительно странного поведения, включая типа тех, которые нарушают типичные правила безопасности, ну и выравнивание модели, как будто она ругается на вас, так что же здесь происходит и как это может быть правдой? Ну, это снова сводится к токенизации, так что здесь происходит то, что продается золото Магикарпа, если  вы на самом деле копаетесь в этом, это пользователь Reddit, так что есть u Sol gold Magikarp и, возможно, то, что здесь произошло, хотя я не знаю, было ли это действительно окончательно исследовано, но считается, что произошло то, что набор данных токенизации был очень отличается от набора обучающих данных для реальной языковой модели, поэтому в наборе данных токенизации потенциально была тонна красных данных, где в тексте упоминался пользователь Solid Gold Magikarp, 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7656)~~  потому что Solid Gold Magikarp был очень распространенным, гм, человеком, который будет много публиковать, ну, это будет строка, которая встречается много раз в наборе данных токенизации, потому что она встречается много раз в наборе данных токенизации, эти токены в конечном итоге будут объединены в один отдельный токен для того единственного пользователя Reddit, продавшего золото Magikarp, поэтому  у них будет специальный токен в словаре, если бы это было 50 000 токенов в gpd2, который посвящен этому пользователю Reddit, а затем происходит то, что набор данных токенизации содержит эти строки, но позже, когда вы обучаете модель, сама языковая модель, хм, эти данные из Reddit не присутствовал, поэтому во всем обучающем наборе для проданной языковой модели золото Magikarp никогда не встречается, этот токен никогда не появляется в обучающем наборе для фактической языковой модели позже, поэтому этот токен никогда не активируется, он инициализируется случайным образом в начале  оптимизация, затем у вас есть проходы вперед и назад и обновления 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7714)~~  модели, и этот токен просто никогда не обновляется в таблице внедрения, вектор строки никогда не подвергается выборке, он никогда не используется, поэтому он никогда не обучается и совершенно необучен, это что- то вроде нераспределенной памяти в  типичная двоичная программа, написанная на C или что-то в этом роде, поэтому она имеет нераспределенную память, а затем во время тестирования, если вы вызываете этот токен, вы в основном извлекаете строку таблицы внедрения, которая совершенно необучена и которая передается в Transformer и создает  неопределенное поведение, и это то, что мы видим здесь, это совершенно неопределенное поведение, которое никогда раньше не наблюдалось в обучающем поведении, и поэтому любой из этих странных токенов будет вызывать это поведение, потому что по сути модель, хм, выходит за рамки выборки, вне распределения, окей  и самое последнее, что я хотел бы кратко упомянуть, хотя я думаю, что многие люди прекрасно знают об этом, это то, что разные виды форматов, разные представления, разные языки 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7765)~~  и так далее могут быть более или менее эффективными с токенизаторами GPD.  или любые токенизаторы для любого другого L в этом отношении, так что, например, Json на самом деле очень плотен в токенах, а yaml намного более эффективен в токенах, так что, например, это одно и то же в Json и в yaml, Json равен 116, а  yaml равен 99, так что это небольшое улучшение, и поэтому в экономике токенов, где мы платим за токен разными способами, а вы платите за длину контекста, и вы платите в долларах за э-э, стоимость обработки всех  такого рода структурированные данные, когда вам нужно, так что предпочитайте использовать theal вместо Json, и в целом что-то вроде плотности токенизации - это то, о чем вам нужно постоянно заботиться и беспокоиться, и пытаться найти эффективные схемы кодирования  и проводить много времени в тиковом токенизаторе и измерять эффективность различных токенов разных форматов и настроек и так далее, окей, на этом завершается мое довольно длинное видео о токенизации. Я знаю, что это попытка, я знаю, что 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7826)~~  это раздражает, я знаю, что это раздражает, мне лично очень не нравится на данный момент я должен сказать, что не отмахивайтесь от этого, здесь много острых краев, проблемы с безопасностью, э-э, проблемы с безопасностью ИИ, поскольку мы видели подключение нераспределенной памяти к э-э, языковым моделям, так что, гм, стоит это понять.  этап, где говорилось, я скажу, что вечная слава достается любому, кто сможет избавиться от нее, я показал вам одну возможную статью, в которой пытались сделать это, и я думаю, что надеюсь, что со временем последует гораздо больше, и мои окончательные рекомендации для прямо сейчас, если вы можете повторно использовать токены GPT 4 и словарь в своем приложении, то вам следует подумать об этом и просто использовать Tech токен, потому что это очень эффективная и хорошая библиотека для вывода для bpe. Мне также очень нравится уровень бита BP  это, х, Tik toen и openi использует, х, если вы по какой-то причине хотите тренировать свой собственный словарный запас с нуля, тогда я бы использовал, х, bpe с 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7885)~~  частью предложения, хм, как я уже говорил, я не большой поклонник части предложения, я не  мне не нравится его запасной вариант, и мне не нравится, что он выполняет BP для кодовых точек уникального кода. Я думаю, что это, э-э, у него также около миллиона настроек, и я думаю, что здесь много фут-гонсов, и я думаю, что это действительно легко ошибиться  откалибруйте их, и в конечном итоге вы обрежете свои предложения или что-то в этом роде из-за какого-то типа параметра, который вы не до конца понимаете, поэтому будьте очень осторожны с настройками, попробуйте скопировать и вставить точно, может быть, там, где какая мета сделала, или в основном потратили много  времени, просматривая все гиперпараметры, просматривая фрагмент кода предложения и убеждаясь, что у вас есть правильный вариант, но даже если у вас все настройки правильные, я все равно думаю, что алгоритм отчасти уступает тому, что происходит здесь, и, возможно,  лучше всего, если вам действительно нужно тренировать свой словарный запас, может быть, лучше всего 
 - ~~[▶](https://www.youtube.com/watch?v=zduSFxRajkE&t=7931)~~  просто подождать, пока M bpe станет максимально эффективным, и это то, над чем, возможно, я надеюсь поработать, и в какой-то момент, возможно, мы сможем тренировать в основном то, что  мы хотим, чтобы мы хотели отметить токен, но обучающий код, и это идеальная вещь, которая в настоящее время не существует, и MBP - это ее реализация, но в настоящее время она находится на Python, так что в настоящее время это то, что я должен сказать по поводу токенизации, может быть  расширенное видео, которое в будущем станет еще суше и еще более подробным, но сейчас я думаю, что мы оставим это здесь, и я надеюсь, и они увеличивают размер контакта с gpt1 с 512 до 1024 и  GPT 4 два, следующий, хорошо, следующий, я хотел бы, чтобы мы кратко пробежались по коду открытого Извините, я сейчас чихну, а потом что происходит. Вот это супружеский слой, который я объясню ниже.