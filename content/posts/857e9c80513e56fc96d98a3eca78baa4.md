---
title: 5 генераторов синтетических данных на Python и как их использовать, когда вам
  не хватает данных - UPROGER | Программирование
date: 2023-01-29
src_link: https://www.notion.so/5-Python-UP-ec00f20c80fa40bb9f1ba9a536a0b157
src_date: '2023-01-29 13:49:00'
gold_link: https://uproger.com/5-luchshih-generatorov-sinteticheskih-dannyh-na-python-i-kak-ih-ispolzovat-kogda-vam-ne-hvataet-dannyh-statya-v-razrabotke/
gold_link_hash: 857e9c80513e56fc96d98a3eca78baa4
tags:
- '#host_uproger_com'
---


5 генераторов синтетических данных на Python и как их использовать, когда вам не хватает данных
===============================================================================================

* 26.01.2023


![](https://uproger.com/wp-content/uploads/2023/01/image-2023-01-29-145652.jpg "5 генераторов синтетических данных на Python и как их использовать, когда вам не хватает данных")

В 2022 году ежедневно производилось 2,5 квинтиллиона байт (2,5 миллиона терабайт) данных. Сегодня это число стало еще больше. Но, по-видимому, этого недостаточно, потому что экосистема Python имеет множество библиотек для создания **синтетических данных**. Такие библиотеки полезны для :


1. Для машинного обучения, генерация **синтетических данных** полезна, когда реальные данные недоступны или их трудно получить для обучения модели;
2. Для конфиденциальности и безопасности данных: Замените конфиденциальную информацию в наборах данных реалистичными, но не настоящими данными;
3. Для тестирования и отладки: тестирование и отладка программного обеспечения с синтетическими данными это очень удобный вариант.
4. Для экономии. Искусственное создание большего количества данных из существующих данных это очень дешевый способ создания датасета .


В этой статье будут показаны пять библиотек Python для вышеуказанных целей и то, как их использовать.


[@bigdatai](http://t.me/bigdatai) – наш канал о Big Data.


##### **Генерация данных помощью [Faker](https://pypi.org/project/Faker/0.7.4/)**


Faker – одна из лучших библиотек Python для генерации всех типов случайной информации. Некоторые часто используемые атрибуты, которые генерирует Faker:


* Личная информация: имя, день рождения, электронная почта, пароль, адрес;
* Все виды информации о дате и часовом поясе;
* Финансовые реквизиты: кредитные карты, SSN, банковские услуги;
* Разное: URL-адреса, предложения, языковые коды
* и так далее…


Библиотека имеет интуитивно понятный API. После инициирования класса Faker вы можете сгенерировать поддельный элемент, вызвав его метод:



```
from faker import Faker

fake = Faker()

>>> fake.name()
'Nicole Perkins'

>>> fake.address()
'11669 Foster Cliffs Suite 161\nPort Elizabethfurt, OK 47591'

>>> fake.url()
'http://www.wade.com/'
```

Все эти методы возвращают новые данные при каждом вызове, поэтому легко создать искусственный набор данных CSV с помощью фрагмента кода, подобного приведённому ниже:



```
import pandas as pd

df = pd.DataFrame(
    [
        {
            "name": fake.name(),
            "address": fake.address(),
            "birthday": fake.date_of_birth(),
            "email": fake.email(),
            "password": fake.password(),
        }
        for _ in range(1000)
    ]
)

df.to_csv("data/fake.csv", index=False)
```


```
df.sample(5)
```

![](https://uproger.com/wp-content/uploads/2023/01/1_03f-r1itnccl71bhljsrlq.webp "5 генераторов синтетических данных на Python и как их использовать, когда вам не хватает данных")
Если вы заметили, имена и адреса электронной почты не совпадают. Это один из недостатков использования Faker — сгенерированные Faker наборы данных легко идентифицируются при публичном использовании.


Узнайте больше об этом [из документации](https://faker.readthedocs.io/en/master/).


##### Синтетические наборы данных с Sklearn для задач ML


Sklearn – это настолько обширная и удобная библиотека, что она имеет специальные функции для генерации синтетических данных.


Её модуль `datasets` включает в себя множество функций для создания искусственных датасетов для различных задач машинного обучения. Наиболее популярными функциями являются `make_classificatio`n и `make_regression`.


Обе имеют параметры `n_samples` и `n_features` для управления количеством строк и объектов, выводящихся результатом синтетического набора данных.


Чтобы контролировать сложность задачи, вы можете указать, сколько функций нужно или не стоит использовать, с помощью параметровn\_informative (коррелированных) или n\_redundant (линейных комбинаций информативных функций).


`make_classification` также обеспечивает большой контроль над целью классификации, а именно количеством классов, кластеров для каждого класса и весовых коэффициентов классов.


Существует также функция `make_blobs` для генерации задач кластеризации, как показано ниже:



```
import seaborn as sns
from sklearn.datasets import make_blobs

X, y = make_blobs(n_samples=500, n_features=2)


sns.scatterplot(X[:, 0], X[:, 1], hue=y);
```

![](https://uproger.com/wp-content/uploads/2023/01/1_vs1dhhsbbmx1zubbtbwakq-1.webp "5 генераторов синтетических данных на Python и как их использовать, когда вам не хватает данных")
Если вы ищете что-то необычное, есть и другие функции, такие как `make_checkboard`, `make_circles`, `make_moons` и `make_s_curve`.


##### Набор данных с аномалиями в [PyOD](https://pyod.readthedocs.io/)


**Обнаружение аномалий** – популярная проблема в Data Science. Качественные наборы данных с выбросами всегда трудно найти. К счастью, библиотека обнаружения выбросов **Python (PyOD)** имеет удобную функцию для генерации синтетических данных с выбросами:



```
from pyod.utils.data import generate_data
import seaborn as sns
import matplotlib.pyplot as plt

X, y = generate_data(
    n_train=500, contamination=0.13, n_features=2, train_only=True, random_state=1
)

# Plot
sns.scatterplot(X[:, 0], X[:, 1], hue=y)

# Modify the damn legend
legend = plt.legend(labels=['Inlier', 'Outlier'])
legend.legendHandles[1].set_color("orange")
```

![](https://uproger.com/wp-content/uploads/2023/01/1_03f-r1itnccl71bhljsrlq-1.webp "5 генераторов синтетических данных на Python и как их использовать, когда вам не хватает данных")
`generate_data` позволяет контролировать количество строк в обучающих и тестовых наборах, а также процент выбросов в результирующих наборах.


**PyOD** также обладает большим набором алгоритмов обнаружения аномалий в экосистеме Python.


##### Синтетические данные поверх другого набора данных с помощью [CTGAN](https://pypi.org/project/ctgan/).


Когда у вас ограниченный набор данных, моделям машинного обучения трудно хорошо обобщать их . В таких случаях вы можете использовать генеративные состязательные сети — **CTGAN.**


После того, как вы подгоните **CTGAN** к любому набору данных, они могут генерировать синтетические выборки из датасета, который вы выбрали для образца для генерации. Это отличный способ повысить как безопасность данных, так и размер датасета.


CTGAN предоставляется проектом Synthetic Data Vault (SDV). Его API предоставляет класс `CTGAN` , который принимает набор данных и спискок его категориальных столбцов.


Вы можете сгенерировать столько данных, сколько захотите, с помощью функции `sample`. Ниже мы сгенерируем 20 тысяч строк из набора данных `cliché Iris`:



```
import seaborn as sns
import pandas as pd
from ctgan import CTGAN

# Extract categorical data types
iris = sns.load_dataset("iris")
categoricals = iris.select_dtypes(exclude="number").columns.tolist()

# Fit CTGAN
ctgan = CTGAN(epochs=10)
ctgan.fit(iris, categoricals)

# Generate the data
synthetic_iris = ctgan.sample(20000)
synthetic_iris.head()
```

##### [Mimesis](http://pypi.org/project/mimesis/) — Продвинутый Faker


[Mimesis](https://pypi.org/project/mimesis/) – это полноценный генератор случайной информации, построенный на базе Faker. Он может генерировать гораздо больше случайных атрибутов, чем Faker:



```
from mimesis import Generic
from mimesis.locales import Locale

# Spanish locale
fake = Generic(Locale.ES)

print(dir(fake))
```


```
address     code           development  food      locale   payment  text     
binaryfile  cryptographic  file         hardware  numeric  person   transport
choice      datetime       finance      internet  path     science
```

Его генераторы случайных чисел сгруппированы по 20 категориям, что делает Mimesis удобным в работе.


Он также в поддерживает информацию о конкретной стране для 32 локализаций (языков). Ниже мы генерируем тысячу строк поддельных данных на испансом:



```
from mimesis import Generic
from mimesis.locales import Locale
import pandas as pd

# Spanish locale
fake = Generic(Locale.ES)

df = pd.DataFrame(
    [
        {
            "name": fake.person.full_name(),
            "country": fake.address.country(),
            "birthday": fake.datetime.date(),
            "email": fake.person.email(),
            "password": fake.person.password(),
        }
        for _ in range(1000)
    ]
)

df.head()
```

Узнайте больше из [обширной документации](https://mimesis.name/en/v6.0.0/index.html) библиотеки.


##### Генерация данных TensorFlow


Одним из наиболее эффективных методов искусственного увеличения размера наборов изображений в задачах компьютерного зрения является аугментация.


Идея проста: когда у вас есть небольшой набор данных изображений, слишком маленький для эффективного обучения нейронной сети, вы можете увеличить их количество, используя различные случайные преобразования изображений. Таким образом, у сети будет больше разнообразных примеров для обучения. Распространенными преобразованиями изображений являются:


* Геометрические: поворот, перемещение, масштабирование, переворачивание — изменение размера, ориентации и положения объектов на изображениях
* Цвет и яркость: случайные изменения яркости и контрастности для увеличения вариабельности освещения и цветовых условий.
* Шум и размытие: добавление эффектов случайного шума и размытия для имитации различных уровней качества изображения.


Такие преобразования могут значительно увеличить размер набора данных за счет введения похожих, но не идентичных вариантов изображений. Это, в свою очередь, приводит к повышению производительности нейронных сетей.


Увеличение изображения может быть выполнено многими способами в TensorFlow. Для задач классификации изображений существует класс `ImageDataGenerator`:



```
import tensorflow as tf

train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    rescale=1.0 / 255,
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.15,
    horizontal_flip=True,
    fill_mode="nearest",
)
```

Вы инициализируете его и устанавливаете нужные преобразования. Затем вы можете использовать его метод `flow_from_directory` для чтения изображений из указанного каталога данных:



```
train_generator = train_datagen.flow_from_directory(
    "data/raw/train",
    target_size=(50, 50),
    batch_size=32,
    class_mode="categorical",
)
```

После этого вы можете передать `train_generator` для подгонки моделей Keras. Генератор работает асинхронно – пока партия моделей обучается, генератор применяет преобразования и изменяет размер изображений следующей партии в фоновом режиме.


Чтобы `flow_from_directory` работал, структура папки `dataset` должна иметь иерархию, подобную приведённой ниже:



```
$ tree -L 3 data/raw/train

data/raw/
├── train
│   ├── 0
│   ├── 1
│   ├── 2
|   ...
├── validation
│   ├── 0
│   ├── 1
│   ├── 2
|   ...
```

Набор данных должен содержать учебные и проверочные (и тестовые) каталоги с изображениями, сгруппированными в отдельные папки под именем их класса.


Есть и другие альтернативы, если вы не можете принудительно преобразовать свой набор данных в такую структуру. Например, когда вы создаёте свои модели с помощью Keras Sequential API, вы можете использовать слои преобразования:



```
from tensorflow.keras import layers

resize_and_rescale = tf.keras.Sequential([
  layers.Resizing(IMG_SIZE, IMG_SIZE),
  layers.Rescaling(1./255)
])

image_augmentation = tf.keras.Sequential([
  layers.RandomFlip("horizontal_and_vertical"),
  layers.RandomRotation(0.2),
])

model = tf.keras.Sequential([
  # Add the preprocessing layers you created earlier.
  resize_and_rescale,
  data_augmentation,
  layers.Conv2D(16, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  # Rest of your model.
])
```

Заключение
----------


Несмотря на то, что во всем мире уже и так имеется много данных, синтетические данные становятся всё более популярными. Это отражается в количестве появляющихся стартапов с синтетическими данными. Согласно анализу рынка, глобальная индустрия генерации синтетических данных в 2022 году стоила более 100 миллионов долларов и, как ожидается, будет расти на 34,8% в год.


В этой статье мы лишь поверхностно коснулись отрасли, узнав о некоторых наиболее популярных альтернативах с открытым исходным кодом. Если вы не ищете корпоративные решения, этих библиотек более чем достаточно для ваших основных потребностей.


Спасибо за чтение!


+1 0 +1 2 +1 3 +1 0 +1 0
Просмотры: 2 031