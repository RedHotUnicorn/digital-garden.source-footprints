---
title: 'Локальные нейросети. Аналог ChatGPT-3.5 на домашнем ПК: OpenChat 7B превосходящая
  70B, DeepSeek для кода уровня ChatGPT / Хабр'
date: 2023-12-18
src_link: https://www.notion.so/ChatGPT-3-5-OpenChat-7B-70B-DeepSeek-ChatGPT-498816edc1a740a3b394f9d629f459c8
src_date: '2023-12-18 16:13:00'
gold_link: https://habr.com/ru/articles/776314/
gold_link_hash: 2e3275b9875a5cdc969966a9d830e462
tags:
- '#host_habr_com'
---

![](https://habrastorage.org/getpro/habr/upload_files/903/978/d56/903978d56b1603c02382baa115db5481.png "OpenChat 7B запущенный локально через text-generation-webui")

OpenChat 7B запущенный локально через text-generation-webui

Есть много локальных аналогов ChatGPT, но им не хватает качества, даже 65B модели не могут конкурировать хотя бы с ChatGPT-3.5. И здесь я хочу рассказать про 2 открытые модели, которые всё-таки могут составить такую конкуренцию.

Речь пойдет о OpenChat 7B и DeepSeek Coder. Обе модели за счет размера быстры, можно запускать на CPU, можно запускать локально, можно частично ускорять на GPU (перенося часть слоев на GPU, на сколько хватит видеопамяти) и для такого типа моделей есть графический удобный интерфейс.

И бонусом затронем новую модель для качественного подробного описания фото.

UPD: Добавлена информация для запуска на Windows с ускорением на AMD.

OpenChat
--------

Ссылка: [https://huggingface.co/openchat/openchat\_3.5](https://huggingface.co/openchat/openchat_3.5)

Онлайн демо: [https://openchat.team/](https://openchat.team/ru)

![](https://habrastorage.org/getpro/habr/upload_files/f0d/185/653/f0d1856538235fcdd846cd199c81af94.png "Сравнение с chatGPT")

Сравнение с chatGPT

Эта модель, по заявлению разработчиков, превосходит модели с 70B параметров имея всего лишь 7B параметров и даже может конкурировать с ChatGPT-3.5, который по слухам имеет 175B параметров.

На практике, она не всегда дотягивает до ChatGPT, но она ощутимо превосходит Alpaca, Vicuna и прочие открытые модели. При этом, для модели размером 7B, хорошо, хоть и не идеально, понимает русский язык и сразу же на нем и отвечает, без дополнительных подсказок об этом.

А то, что она маленькая позволяет запускать её без проблем даже на CPU. Требуется 14гб памяти или после квантования до 5 bit всего 5гб, что для модели уровня ChatGPT-3.5 хороший результат.

DeepSeek Coder
--------------

Ссылка: [https://github.com/deepseek-ai/deepseek-coder](https://github.com/deepseek-ai/deepseek-coder)

Онлайн демо: [https://chat.deepseek.com/coder](https://chat.deepseek.com/coder)

Эта локальная модель предназначена для генерации кода и представлена в виде base и Incruct версия с вариантами весов 1.3B, 5.7B, 6.7B или 33B. Incruct версия генерирует код лучше базовой модели, но с базовой модель можно поболтать. Понимает команды на русском.

Модель по тестам для Python'а оценивается выше чем ChatGPT-3.5 на 3% и отстает от ChatGPT-4.0 всего на 5%. С другими языками могут быть другие результаты, но в среднем результаты близки:

![](https://habrastorage.org/getpro/habr/upload_files/704/f88/23c/704f8823cb5da777ba23cc0c75d4eae3.png)LLaVA: Large Language and Vision Assistant
------------------------------------------

Ссылка: [https://github.com/haotian-liu/LLaVA](https://github.com/haotian-liu/LLaVA)

Онлайн демо: [https://llava.hliu.cc/](https://llava.hliu.cc/)

gguf модель 13B: [https://huggingface.co/mys/ggml\_llava-v1.5-13b](https://huggingface.co/mys/ggml_llava-v1.5-13b)

gguf модель 7B: [https://huggingface.co/jartine/llava-v1.5-7B-GGUF](https://huggingface.co/jartine/llava-v1.5-7B-GGUF)

Аналог ChatGPT Vision. Может распознать, что находится на изображении и ответить на вопросы связанные с этим, попытаться объяснить в чем юмор и прочее. Модель старается описать красиво и красочно, при этом сохраняет выверенную детализацию. 

Есть варианты в 7B и 13B параметров.

![](https://habrastorage.org/getpro/habr/upload_files/f2d/83c/8c0/f2d83c8c01411c74008d30923991b57c.png)Можно спросить, что она видит конкретное на изображении?

![](https://habrastorage.org/getpro/habr/upload_files/e33/e0b/a54/e33e0ba549cbb50c51c44b01e99a3dad.png "На экране надпись \"Linux\"")

На экране надпись "Linux"

Где брать модели и как их квантовать самостоятельно
---------------------------------------------------

Модели обычно можно найти на [https://huggingface.co/](https://huggingface.co/)

В поиске нужно ввести название модели. Например, введя DeepSeek + gguf можно сразу найти конвертированную модель с разными уровнями квантования:

![](https://habrastorage.org/getpro/habr/upload_files/5ca/652/a4d/5ca652a4da2371c31f33fb4619e5905b.png)Либо качать оригинал модели и конвертировать вручную с помощью llama.cpp. 

Рассмотрим на примере OpenChat. На странице [https://huggingface.co/openchat/openchat\_3.5](https://huggingface.co/openchat/openchat_3.5) нажимаете на 3 точечки и выбираете Clone repository и там будет инструкция как клонировать оригинальный репозиторий из которого и можно будет сконвертировать в gguf модель.

Если вы под Windows, вам нужно будет установить cmake - [https://cmake.org/download/](https://cmake.org/download/), либо использовать wsl.


```
# клонируем репозиторий с openchat
git lfs install
git clone https://huggingface.co/openchat/openchat_3.5

# клонируем llama.cpp и переходим в её папку
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp

# собираем все нужные бинарники
make
# если установили cmake, то команда будет cmake, вместо make

# установим зависимости
pip install -r requirements.txt

# теперь можно запустить саму конвертацию
python3 convert.py ../openchat_3.5/  --outfile openchat_3.5-f16.gguf

# после этого модели можно устроить квантование 5bit или другое
./quantize openchat_3.5-f16.gguf openchat_3.5-q5_K_M.gguf q5_K_M
```
Теперь у вас есть файлы openchat\_3.5-f16.gguf и openchat\_3.5-q5\_K\_M.gguf - первый будет использовать 15гб памяти, а второй - 6.3гб.

Какой метод квантования выбрать - [https://github.com/ggerganov/llama.cpp/discussions/406](https://github.com/ggerganov/llama.cpp/discussions/406) оптимальный это q4\_K\_M, где меньше 1% потерь, но если память позволяет, можно взять q5\_K\_M. Цитата оттуда:

![](https://habrastorage.org/getpro/habr/upload_files/87d/0de/415/87d0de415657248a58e416b9d985874f.png "+ppl - увеличение неточности по сравнению с моделью f16.+ppl % - процентное увеличение неточности.+ppl от 13b до 7b % - неточность при переходе от модели 13b к модели 7b.")

+ppl - увеличение неточности по сравнению с моделью f16.



+ppl % - процентное увеличение неточности.



+ppl от 13b до 7b % - неточность при переходе от модели 13b к модели 7b.

Графический интерфейс для text2text моделей
-------------------------------------------

Есть как минимум несколько вариантов запуска text2text моделей с графическим интерфейсом, включая AMD на Windows: 

* [text-generation-webui](https://github.com/oobabooga/text-generation-webui) - для тех кому привычен подход аналогичного интерфейса для stable diffusion, запускается сервер, открывается указанная ссылка в браузере и дальше работа в браузере.
* [koboldcpp](https://github.com/LostRuins/koboldcpp) - запускается exe файл и работа происходит в отдельной программе, для Nvidia карт, ускорение через CUDA.
* [https://github.com/YellowRoseCx/koboldcpp-rocm/releases](https://github.com/YellowRoseCx/koboldcpp-rocm/releases) - koboldcpp для AMD на Windows. Сборка с ROCm, вместо CUDA.
* [llama server](https://github.com/nuance1979/llama-server) - реализация сервера для [Chatbot UI](https://github.com/mckaywrigley/chatbot-ui), реализация интерфейса как у chatGPT.
* [open-chat-ui](https://github.com/imoneoi/openchat-ui#-running-locally) - запуск сервера и UI для OpenChat как на их сайте онлайн.

Как запустить LLaVA
-------------------

Хотя изначально LLaVA запускалась только под Linux, но для Windows уже есть несколько вариантов:

* [https://github.com/haotian-liu/LLaVA/blob/main/docs/Windows.md](https://github.com/haotian-liu/LLaVA/blob/main/docs/Windows.md) - официальный способ с графическим интерфейсом, только для f16 моделей, достаточно тяжелый вариант с требованием к видеокарте, так как не может запускать квантированные модели (квантование для этого способа работает только на Linux).
* Запуск через консоль с помощью llama.cpp, там в наборе уже есть llava. Позволяет запускать квантированные модели на cpu/gpu. Нужно скачать и саму модель и mmproj модель (качается также на huggingface). Запускается командой:

`./llava-cli -m models/llava/llava-v1.5-7b-Q4_K.gguf --mmproj models/llava/llava-v1.5-7b-mmproj-Q4_0.gguf --image 01.jpg`

Если указать параметр -p, можно задать вопрос про картинку, например, спросить какого цвета пингвин:

`./llava-cli -m models/llava/llava-v1.5-7b-Q4_K.gguf --mmproj models/llava/llava-v1.5-7b-mmproj-Q4_0.gguf --image red_linux.jpg -p "what colour is the penguin?"`
* С помощью [text-generation-webui multimodal](https://github.com/oobabooga/text-generation-webui/tree/main/extensions/multimodal), удобный графический интерфейс, но придется повозится с установкой и запуска сервера для поддержки множественности типов моделей.

Запуск на Windows с ускорением на AMD видеокартах
-------------------------------------------------

ROCm - это аналог CUDA от AMD. Портирование ROCm на Windows продолжается. Например, уже можно запускать Blender с ускорением на ROCm. И есть форк для koboldcpp с ROCm, который тоже уже работает. Спасибо [@DrXak](/users/drxak)за подсказку, что это уже работает для Windows.

Для запуска вам нужно скачать файл `koboldcpp_rocm_only.exe` и запустить его, там выбрать модель и указать нужное количество GPU слоев (зависит от количества видеопамяти):

![](https://habrastorage.org/getpro/habr/upload_files/60d/b80/6af/60db806af0d632857107b34c3aaf77a3.png)После этого откроется интерфейс в браузере, где можно будет найти поле для ввода промпта, найти параметры генерации, по умолчанию Instruct Mode и сценарии для отыгрыша персонажей. Для продолжения генераций нужно нажимать Generate More.

![](https://habrastorage.org/getpro/habr/upload_files/d5d/de9/4e1/d5dde94e1bb1d0b1c9524cdf3b07c062.png)По скорости 7900 xtx не уступает 4090, 66 токенов в секунду:

![](https://habrastorage.org/getpro/habr/upload_files/ca8/0db/46c/ca80db46cd58765e09cf0239ded51256.png)Установка и запуск text-generation-webui
----------------------------------------

Раньше установка была сложной, но сейчас установка производиться в 1 действие, нужно просто запустить `start_windows.bat` и дальше всё установится само.

Для этого нужно скачать zip-архив репозитория или сделать:

`git clone https://github.com/oobabooga/text-generation-webui` 

После распаковки архива и перехода в папку, нужно найти и запустить скрипт запуска, если вы под Windows, то это будет, например, `start_windows.bat`, или `start_linux.sh` если вы под Linux.

При запуске вам предложат выбрать на чем вы будете запускать, на GPU или CPU. Если у вас совместимая видеокарта, то выбирайте вариант с GPU, даже на видеокарте с 4гб памяти можно будет часть нагрузки разместить на ней, а часть будет считаться на CPU. 

Для AMD видеокарт пока доступен только вариант ускорения ROCm под Linux. Работы над портированием ROCm под Windows ведутся. 

![](https://habrastorage.org/getpro/habr/upload_files/ba3/c51/59b/ba3c5159b2650f3e030702a95e2b9501.png)Модели нужно положить в папку models и после этого выбрать модель и загрузчик:

![](https://habrastorage.org/getpro/habr/upload_files/cde/a94/af7/cdea94af70bfc29bc70b87ee0cb29376.png)Для моделей с разрешением gguf нужно выбирать llama.cpp в качестве загрузчика. 

Также тут можно указать сколько слоев будет обработано на GPU, это позволит ускорить работу даже если у вас видеокарта с небольшим объемом видеопамяти. 

Можно подобрать экспериментально на сколько хватает памяти, поэтому можете выкрутить этот ползунок на максимум, тогда будет выставлено автоматически максимальное значение для видеокарты и после в консоли нужно смотреть, сколько он хочет всего видео памяти, постепенно уменьшая это значение.

После этого остается нажать Load и начнется загрузка модели, спустя некоторое времени появится надпись, что модель успешно загружена. 

![](https://habrastorage.org/getpro/habr/upload_files/d4d/d74/51d/d4dd7451d74922fabf540765d5ce6bbc.png)В параметрах можно выставить 2048 токенов, чтобы не требовалось каждый раз нажимать "Continue" во время генерации, так как по умолчанию выставлено всего 200 токенов и длинные ответы будут залипать на половине ответа. Главное после этого не забывайте создавать новый чат каждый раз, когда хотите сменить тему или получить новые ответы на тот же вопрос.

![](https://habrastorage.org/getpro/habr/upload_files/7d5/9b8/56f/7d59b856fb19a5494f159e03dbd5fbfb.png)Теперь можно перейти на вкладку Chat и попытаться что-то поспрашивать:

![](https://habrastorage.org/getpro/habr/upload_files/6a8/18b/907/6a818b907351244fa4200935cfc35b6e.png)Можно дать текст на анализ:

![](https://habrastorage.org/getpro/habr/upload_files/5fe/21c/4af/5fe21c4af32d8b38cac2bdb2e7f483ec.png)Chat работает в 3 режимах взаимодействия с моделью: chat, chat-instruct и instruct, по умолчанию выставлен chat. В режиме chat модель будет пытаться общаться невзирая на то, что вы у неё просите. 

Например, если вам нужно, чтобы модель писала в ответ только перевод, без пояснений, для этого нужно выбрать режим chat-instruct или instruct:

![](https://habrastorage.org/getpro/habr/upload_files/b4a/555/4c6/b4a5554c6d514ce7ea7965aa5fd3fda8.png)Дальше уже можно поэкспериментировать. Можно попросить написать простой todo на js:

![](https://habrastorage.org/getpro/habr/upload_files/163/f3e/1f1/163f3e1f14a11a289c4595955c82a741.png)![](https://habrastorage.org/getpro/habr/upload_files/325/ed1/970/325ed1970a19b528f3a44b58e042738b.png "Модель предупреждает, что это только основа")

Модель предупреждает, что это только основа

Результат простой и частично рабочий, кнопка X не работает, но об этом модель сразу предупреждает, само добавление работает:

![](https://habrastorage.org/getpro/habr/upload_files/3fc/103/ff6/3fc103ff6a477986308293167088541f.png)Можно проверить насколько 7B модель может в простую математику:

![](https://habrastorage.org/getpro/habr/upload_files/aff/0fe/82d/aff0fe82da0dbeb303f53aabe3f02c97.png "Ответ верный")

Ответ верный

Попробуем что-то посложнее. Запрос: напиши flutter приложение которое имитирует мессенджер:

![](https://habrastorage.org/getpro/habr/upload_files/5c1/a0b/add/5c1a0badd77ab92436754d9630779b5b.webp "Генерация на OpenChat 7B Q5, на 4090 66 токенов в секунду")

Генерация на OpenChat 7B Q5, на 4090 66 токенов в секунду

Как использовать DeepSeek Coder
-------------------------------

OpenChat может генерировать код, но в зависимости от сложности и дополнительных условий, она будет выдавать не совсем то, что ожидается. В этом смысле DeepSeek куда ближе к ChatGPT, чем OpenChat. 

Для генерации только кода, вам нужно взять модель с именем instruct, она генерирует код лучше чем base модель в которой есть режим чата и поэтому она хуже справляется с кодом. А сам чат нужно перевести тоже в режим instruct для лучшего результата (тоже самое можно делать для OpenChat, если нужен только код):

![](https://habrastorage.org/getpro/habr/upload_files/dd5/1c3/282/dd51c32829f879442f1bd408bb3f7e19.png)После этого модель сможет лучше понимать, что вы от неё хотите и лучше додумывать, что это могло бы означать в формате кода. Попробуем попросить его сгенерировать bash-скрипт, с которым у OpenChat проблема.

![](https://habrastorage.org/getpro/habr/upload_files/290/25d/2ee/29025d2ee78a23a5660435271bf72642.png "Напиши bash скрипт, который будет искать в папке и подкаталоги все картинки png и jpg, выполнять для них скрипт, и запоминать уже обработанные файлы в txt файл  ")

Напиши bash скрипт, который будет искать в папке и подкаталоги все картинки png и jpg, выполнять для них скрипт, и запоминать уже обработанные файлы в txt файл 

Если дать тоже задание OpenChat, то с первого раза модель не всегда будет выполнять все условия которые ожидаются, например, будет записывать уже обработанные файлы, но не проверяет их, чтобы повторно не обрабатывать. И с одной стороны правильно, этого не было в условии, но DeepSeek и ChatGPT сразу понимают, что это подразумевается и ожидается.

![](https://habrastorage.org/getpro/habr/upload_files/8ef/a53/483/8efa534830a114888bf1dc88741e1fbd.png " Напиши bash скрипт, который будет искать в папке и подпапках все картинки png и jpg, выполнять для них скрипт, и запоминать уже обработанные файлы записывая обработанные в txt файл.")

 Напиши bash скрипт, который будет искать в папке и подпапках все картинки png и jpg, выполнять для них скрипт, и запоминать уже обработанные файлы записывая обработанные в txt файл.

Попробуем что-то посложнее, с чем и у ChatGPT есть сложность, чтобы с 1 раза сгенерировать всё правильно.

Запрос: Напиши приложение на flutter которое будет показывать случайный комикс xkcd. Должна быть кнопка показать следующий комикс. Должна быть кнопка добавить в избранное и возможность посмотреть своё избранное

![](https://habrastorage.org/getpro/habr/upload_files/353/662/e09/353662e09c4624fcfad5734bd9a0d40f.png)Код с виду рабочий, но при запуске показываются ошибки, пока ничего страшного, такие же ошибки генерирует и ChatGPT. ChatGPT может их исправить если ему кидать сообщения об ошибках, посмотрим как с этим у DeepSeek:

![](https://habrastorage.org/getpro/habr/upload_files/c23/d8a/451/c23d8a451611f5b9d1b75262750d1487.png)![](https://habrastorage.org/getpro/habr/upload_files/c21/8b9/b90/c218b9b90e53164e0195a0e9c9ab120f.png)Применим исправления и попробуем запустить и посмотреть, что получилось. Да, теперь программа успешно запустилась:

![](https://habrastorage.org/getpro/habr/upload_files/230/659/982/2306599824bd23251dda893cbcb1800b.png "Кнопка добавить в избранное работает")

Кнопка добавить в избранное работает

Пролистывание работает, но оно не рандомное, а на 1 комикс вперед. Кнопка добавления в избранное работают, сам список избранного в виде бесконечной ленты отображает, кнопка назад на главный экран тоже предусмотрена.

![](https://habrastorage.org/getpro/habr/upload_files/905/fbb/59c/905fbb59cf711d13de94d1cb5543e5d0.png "Лента избранного (если листать вниз, будет ещё 3 комикса)")

Лента избранного (если листать вниз, будет ещё 3 комикса)

В этой же задачей ChatGPT-3.5 справился похожим образом, он тоже не получал рандомный комикс и генерировал не рабочий Flutter код, но после нескольких итераций сообщения об ошибках, он ошибки исправил, также как и DeepSeek. Можно сделать поверхностный вывод, что в более редких сценариях, чем являются Dart и Flutter, они показали себя равнозначно.