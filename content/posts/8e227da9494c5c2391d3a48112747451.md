---
title: Optimizing for DAG and task complexity in Airflow | by DV Engineering | DoubleVerify
  Engineering | Medium
date: 2023-01-30
src_link: https://www.notion.so/Optimizing-for-DAG-and-task-complexity-in-Airflow-by-DV-Engineering-DoubleVerify-Engineering-J-1204b056a1a140b49510f54b18f9c066
src_date: '2023-01-30 13:49:00'
gold_link: https://medium.com/doubleverify-engineering/optimizing-for-dag-and-task-complexity-in-airflow-4fb6501e34d1
gold_link_hash: 8e227da9494c5c2391d3a48112747451
tags:
- '#host_medium_com'
---

![]()My first data engineering project at DoubleVerify (DV) involved processing files containing billions of rows. The pipeline contained many intermediate steps with complex dependencies and was responsible for applying a series of transformations and enrichments, as well as storing the results in a database.

In this blog post, I will talk about our journey in building this pipeline on Luigi, what we learned from it, and how we migrated to Apache Airflow. I will also describe the various approaches we took to scale, parallelize, and optimize our data pipelines, and will highlight key points to consider while building such pipelines.

Background
==========

The following sketch shows a high-level overview of our data pipeline, which was responsible for determining if a real user viewed an ad impression for one or two continuous seconds.

![]()The first version of our pipeline was in Luigi. Luigi offered many benefits in terms of defining dependencies, automatic retries of failed tasks, and creating dynamic tasks. Dynamic tasks helped create a task per file chunk, which further helped parallelize processing.

![]()However, Luigi had the following limitations:

1. The User Interface (UI) capabilities were limited. Rerunning tasks, observing task logs, etc., could not be done via UI.
2. Backfilling required several manual steps.
3. Both #1 and #2 severely limited our ops team from maintaining and managing pipelines.
4. Resource scarcity was common, especially due to dynamic tasks. This required additional monitoring and infrastructure orchestration to scale up Luigi clusters.

Airflow, by this time, was gaining huge traction due to its powerful UI, which rendered visualizing, backfilling, and performing manual reruns of failed tasks trivial. Thus, it became a natural choice for us to try Airflow out.

However, one area that challenged us was the replication of the dynamic task pattern. While dynamic tasks are valuable, additional tools for scaling and parallelizing your pipeline, we quickly realized that doing so would be at the expense of making the Directed Acyclic Graph (DAG) very complex.

Are there any alternative solutions to finding the right balance between the task and DAG complexity? Let us dive in!

Approach 1: Single-task, serial implementation
==============================================

The most straightforward implementation is to create a single task that will process all the file chunks serially. The following pseudo-code demonstrates this idea:

![]()The code snippet above renders the following DAG.

![]()The main advantage of this approach is that everything is encapsulated within a single task. Either all your data is processed and your task is green, or your data is not processed and your task is red.

The disadvantage of this approach is that:

1. The serial approach does not scale well. If you have 1,000+ file chunks, this process will take 1,000 times as much
2. It has poor failure recovery and resumability, i.e., in case of failures, the entire task would be retried from the start. This results in:
3. a. Wasteful processing of files already processed in the previous run
4. b. The need for special handling to ensure idempotency in the target database (to avoid duplicates due to accidental reprocessing).

![]()Fortunately, Airflow also has dynamic tasks like Luigi, which we will explore next.

Approach 2: Dynamic tasks
=========================

This approach is an improvement to the serial option and is similar to how we implemented it in Luigi. This approach helps:

1. Scale more quickly by creating a task per file chunk.
2. Provide better failure handling and resumability. In case of failures, only the failed task(s) need to be rerun from the UI.

The following pseudo-code demonstrates this idea:

![]()

dynamic\_task.py

However, this implementation suffers from the following downsides:

1. Using the Airflow UI becomes difficult. It is hard to identify choice tasks to rerun, such as those that failed. See the screenshot below of Airflow’s DAG (graph and tree view).
2. Due to hundreds (possibly thousands) of tasks, the following can happen:
3. a. Task scarcity due to resource limits.
4. b. The scheduler crashing, resulting in the pipeline stalling.

![]()![]()![]()*\* Scalable in terms of processing but, overall, puts a lot of load on the infrastructure.*

Approach 1 and Approach 2 are examples of task complexity versus DAG complexity, and shows how the two complexities are at odds with each other. Task complexity is more straightforward than DAG complexity but requires reimplementing a lot of what Airflow offers. On the other hand, DAG complexity allows you to implement powerful dynamic tasks but severely limits operability. Is there an alternate approach?

Before jumping into the final solution, let us break down the problem into two parts:

1. How can we retain the simplicity of a single task while still being able to pick up processing where we left off?
2. How can we replicate Airflow’s ease of processing in parallel, like in the case of dynamic DAGs, without overcomplicating the DAG?

Approach 3: Single task with state store
========================================

One of the main challenges with the single-task approach was reusability during reruns. Tracking file status is a quick way to ensure that a file processed in the previous attempts is not reprocessed when rerun.

The main idea here is to modify the task to perform a lookup of file/processing status.

![]()This can be implemented as demonstrated in the following pseudo code:

![]()This lookup ensures that on task rerun:

1. Files successfully processed in the previous attempt are skipped.
2. Files not processed in the previous attempt are scheduled, resulting in resumability.
3. If all files are processed, the rerun will simply succeed.

Why is this better? For one, we will not have to cherry-pick all the failed tasks in the UI that need to be rerun. Instead, rerunning the single task automatically resolves this issue. Secondly, accidental reruns will have no consequences, making tasks idempotent by default.

Where should the states be stored? They can be stored anywhere, such as Airflow XCOMs, external caches like memcache/Redis, a database, or a file. The choice of the cache itself is irrelevant as long as you can easily incorporate the necessary code in your task to store and retrieve the states..

Let us now address the second question: How do we parallelize processing?

Approach 4 — Single task with state store & parallelism
=======================================================

The “for” loop in the code snippet above is the source of the problem that prevents us from scaling. Parallelizing this can take many shapes and forms, with the simplest being utilizing Python’s ThreadPoolExecutor, as demonstrated in the code snippet below.

![]()In our experiments, we were able to reduce the processing time of the task from six hours to 30 minutes, with thread pools for processing about 5,000 file chunks.

The max\_workers parameter can further help improve performance. While there is no theoretical limit, setting this parameter to an exceptionally high number could result in diminishing returns.

![]()This approach handles all of our previously discussed concerns.

1. Simpler DAG complexity:
2. a. Makes it easy to develop, manage, and maintain the pipeline.
3. b. Results in better resource management — does not result in task scarcity or UI issues.
4. Slightly increased task complexity:
5. a. The tasks now need to reimplement states, and contain the code to retrieve and update said states.
6. b. Idempotency and resumability — A task that is completed is never rerun again.
7. Scalability
8. a. Makes it easy to handle a high volume of data, thanks to the ThreadPoolExecutor.

Optimizing further with KubernetesPodOperator
=============================================

Using KubernetesPodOperator (KPO) instead of the PythonOperator helps further optimize and results in a separation of concerns. The KPO runs any Docker image in a separate container/pod, giving us the flexibility to specify the number of resources required, while PythonOperator runs the task within the Airflow worker.

![]()So, are dynamic tasks bad?
==========================

Not at all! My colleague, [Keo](https://www.linkedin.com/in/keozchan/), wrote an excellent [post](/doubleverify-engineering/skip-python-create-dynamic-airflow-dags-from-yaml-files-5a05f3556378) about how we use dynamic tasks, and I highly encourage you to check it out. The general rule of thumb is to not overdo it and consider the operation implications of using dynamic tasks. Following this general rule of thumb can help save a lot of time and frustration in the long run. While we do not use Airflow directly to manage the life cycle of a single file transformation, we took inspiration from it, such as storing state in a cache and launching jobs dynamically. We also avoided completely reinventing the wheel and continue to find the Airflow UI invaluable. Think carefully about which features you want to leverage when using a new technology, and remember that it does not have to be all or nothing.