---
title: Шпаргалка по рекомендательным системам / Хабр
date: 2024-02-12
src_link: https://www.notion.so/e510801a745644dc96bb53e69a5b7d21
src_date: '2024-02-12 11:46:00'
gold_link: https://habr.com/ru/articles/792994/
gold_link_hash: 7eaaed907f6c5db2b6b95857b4c3f328
tags:
- '#host_habr_com'
---

Цель рекомендаций - порекомендовать пользователю что-то новое. Например, на маркетплейсах, в музыкальных или видео-стриминговых сервисах. 

На деле вы всегда работаете с так называемой матрицей взаимодействий **![](https://habrastorage.org/getpro/habr/upload_files/ec6/178/3fa/ec61783fa175cc1e51441ed11e586af2.svg)**(**user-item matrix**), на каждой строке которой стоят пользователи (**users**), а каждому столбцу соответствует товар (**items**). На пересечении пользователя и товара записано какое-то их взаимодействие (**feedback**). 

![](https://habrastorage.org/getpro/habr/upload_files/7bb/623/904/7bb623904e721fe27f6db5ee99b2b899.png)**Feedback** между пользователем и товаром может быть**:**

* явным (**explicit**): оценки, лайки;
* неявным (**implicit**): клики, скипы;

Причем, нет какого-то строго различия между явным и неявным фидбеком. Часто, при решении задач рекомендаций, необходима некая **уверенность в фидбеке**. 

При этом, и описание пользователя можно представить по-разному:

1. Как **неупорядоченный** набор товаров с которыми пользователь **провзаимодействовал** (список пролайканных треков)
2. Как **неупорядоченный** набор товаров **с оценками** (оценка пользователем фильмов, сколько раз пользователь прослушал трек)
3. Как **последовательность** айтемов **с оценками**
4. Как **последовательность** взаимодействий **с учетом контекста**

Ниже будут рассмотрены следующие алгоритмы:



---

### 1. Коллаборативная фильтрация

Разделяется на **Item2Item (**основан на похожести между товарами) и**User2User (**основан на похожести между пользователями) подходы.

![](https://habrastorage.org/getpro/habr/upload_files/33d/bdc/74d/33dbdc74d4ef91462dbfbaca42321ee4.png)Ниже приведены рассуждения для Item2Item, но, с точностью до замен переменных, они верны для User2User.

Пусть **![](https://habrastorage.org/getpro/habr/upload_files/f96/0e5/280/f960e528073697d55bce6c52b62ec5e3.svg)**матрица похожестей, на столбцах и строках которой записаны Item-ы. На главной диагонали будет стоять максимальная похожесть, так как там стоит один и тот же товар. 

В зависимости от имеющегося фидбека, похожесть будет считаться по-разному: в случае explicit фидбека это может быть [**корреляция**](https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D1%80%D1%80%D0%B5%D0%BB%D1%8F%D1%86%D0%B8%D1%8F), а в случае implicit - [**мера Жаккара**](https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82_%D0%96%D0%B0%D0%BA%D0%BA%D0%B0%D1%80%D0%B0). 

Посчитав все похожести товаров, предсказание можно взять как усреднение ТОПа самых похожих, что, в каком-то смысле, похоже на [KNN](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_k-%D0%B1%D0%BB%D0%B8%D0%B6%D0%B0%D0%B9%D1%88%D0%B8%D1%85_%D1%81%D0%BE%D1%81%D0%B5%D0%B4%D0%B5%D0%B9).

### 2. Матричные факторизации

Этот подход основан на [методе главных компонент (PCA)](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%B3%D0%BB%D0%B0%D0%B2%D0%BD%D1%8B%D1%85_%D0%BA%D0%BE%D0%BC%D0%BF%D0%BE%D0%BD%D0%B5%D0%BD%D1%82) и [матричном разложении (SVD).](https://ru.wikipedia.org/wiki/%D0%A1%D0%B8%D0%BD%D0%B3%D1%83%D0%BB%D1%8F%D1%80%D0%BD%D0%BE%D0%B5_%D1%80%D0%B0%D0%B7%D0%BB%D0%BE%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5)

Матрица взаимодействий **![](https://habrastorage.org/getpro/habr/upload_files/a8f/4fe/abf/a8f4feabf4034316ad1f197becf8b0cf.svg)** очень большая и очень [разряженная](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D0%B7%D1%80%D0%B5%D0%B6%D0%B5%D0%BD%D0%BD%D0%B0%D1%8F_%D0%BC%D0%B0%D1%82%D1%80%D0%B8%D1%86%D0%B0). 

**SVD** ([сингулярное разложение](http://www.machinelearning.ru/wiki/index.php?title=%D1%E8%ED%E3%F3%EB%FF%F0%ED%EE%E5_%F0%E0%E7%EB%EE%E6%E5%ED%E8%E5)) - представление исходной матрицы в виде произведения матрицы поворота, матрицы растяжения и еще одной матрицы поворота. Геометрически, для облака точек, это будет выглядеть следующим образом:

![](https://habrastorage.org/getpro/habr/upload_files/231/e0b/0f4/231e0b0f4213ad088514b42abd6268b6.png "Геометрический смысл SVD")

Геометрический смысл SVD

![](https://habrastorage.org/getpro/habr/upload_files/3fe/ac0/538/3feac05383451f45e48a648edac32a65.png)Формульно:

![](https://habrastorage.org/getpro/habr/upload_files/7a4/61e/9cc/7a461e9cccc1d6e5c0bcd12e0153a7f5.svg)![](https://habrastorage.org/getpro/habr/upload_files/64f/12d/687/64f12d687b0851dfc550c817c24f2b06.svg) и **![](https://habrastorage.org/getpro/habr/upload_files/d74/93f/f61/d7493ff615a5cd59b8594bf704b01f5f.svg)** - матрицы поворота, **![](https://habrastorage.org/getpro/habr/upload_files/b5c/912/979/b5c912979435e3c1c75be73dad75f87e.svg)** - это диагональная матрица с неотрицательными сингулярными числами, которые расположены в порядке неубывания. Они говорят о мере *дисперсии* (разброса) точек: чем больше это число, тем больше разброс. Если мы будем умножать вектор на SVD-разложение, то сначала мы вектор повернем, потом растянем и еще раз повернем.

Задача PSA (метод главных компонент) - понизить размерность точек в многомерном пространстве линейным способом (выбрать линейное подпространство **![](https://habrastorage.org/getpro/habr/upload_files/f27/a8d/49b/f27a8d49b9c93447e93ad0cd16696d56.svg)** такое, что сумма расстояний от точек до этого подпространства была минимальной (точки наилучшим образом приближались этой линейной плоскостью)). Оставляем компоненты, которые объясняют больше всего *дисперсии*.

Так, для разложенной матрицы взаимодействий **![](https://habrastorage.org/getpro/habr/upload_files/637/d0e/7b8/637d0e7b86b4fa76c97eebd4a4dfb3ed.svg)**, оставляют значительно меньшее (часто меньше 100) количество главных (первых) компонент из матрицы ![](https://habrastorage.org/getpro/habr/upload_files/b05/690/761/b056907616bc9fc49fa0fb086f4f3c51.svg), просто отбросив «хвост». Такая манипуляция, с одной стороны, позволяет значительно сократить объемы памяти, с другой стороны, дает наилучшее приближение для ![](https://habrastorage.org/getpro/habr/upload_files/6c4/254/556/6c4254556c9b2a3739d3288722f5fccc.svg) по [норме Фробениуса](https://ru.wikipedia.org/wiki/%D0%9D%D0%BE%D1%80%D0%BC%D0%B0_%D0%BC%D0%B0%D1%82%D1%80%D0%B8%D1%86%D1%8B). 

Сделав замену![](https://habrastorage.org/getpro/habr/upload_files/c05/ac6/87d/c05ac687daddf35588702f974cc341ed.svg), ![](https://habrastorage.org/getpro/habr/upload_files/188/b2e/2ee/188b2e2ee0fc01c22badae6acb493939.svg)матрицу взаимодействий представляют как ![](https://habrastorage.org/getpro/habr/upload_files/bda/c42/5c8/bdac425c8cc7a00d2a635805b085cefd.svg)и говорят, что строка **![](https://habrastorage.org/getpro/habr/upload_files/26d/e46/791/26de46791fec0fc20b2c4470fc2dd841.svg)** - это представление ![](https://habrastorage.org/getpro/habr/upload_files/7d3/c4a/ace/7d3c4aace47f320f82815bff6b022025.svg)-го пользователя, а столбец **![](https://habrastorage.org/getpro/habr/upload_files/9fc/b71/a5b/9fcb71a5b960a416dc83bc2cadf76c69.svg)** - представление ![](https://habrastorage.org/getpro/habr/upload_files/352/272/7da/3522727da2bed47895abbda6a390d24b.svg)-го товара. Тогда, их взаимодействие считается как скалярное произведение: ![](https://habrastorage.org/getpro/habr/upload_files/24e/0bf/96f/24e0bf96f94886cc591c6af616e7f9ca.svg).

Задача - предугадать какое взаимодействие окажется на пустых местах матрицы**![](https://habrastorage.org/getpro/habr/upload_files/887/ac2/d9f/887ac2d9f198b42d5ca0a3bbf09231b1.svg)**. В качестве лосса берется MSE:

![](https://habrastorage.org/getpro/habr/upload_files/84a/433/b53/84a433b53844147638a625733ee098b0.svg)Причем, мы не можем просто применить SVD разложение: поскольку матрица **![](https://habrastorage.org/getpro/habr/upload_files/491/40a/25d/49140a25daf120422c766e866f63887a.svg)** содержит большое количество нулей, то и решение всегда будет близко к нулю. **Идея Funk SVD в том, что мы восстанавливаем матрицу** ![](https://habrastorage.org/getpro/habr/upload_files/770/31d/465/77031d465e75cafea868ac0c0cc36af7.svg) **только по заданным элементам.** Такая задача оптимизации решается [градиентным спуском](https://education.yandex.ru/handbook/ml/article/optimizaciya-v-ml#:~:text=%D0%93%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9%20%D1%81%D0%BF%D1%83%D1%81%D0%BA%20(GD)) и довольно хорошо сходится.

[Алгоритм **ALS**](https://habr.com/ru/companies/yandex/articles/241455/#:~:text=%D0%BA%20%D0%B3%D0%BB%D0%BE%D0%B1%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D0%BC%D1%83%20%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D1%83%D0%BC%D1%83.-,Alternating%20Least%20Squares,-%D0%9E%D0%B4%D0%BD%D0%B0%D0%BA%D0%BE%20%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%20%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D0%BE%D0%B3%D0%BE) (метод чередующихся наименьших квадратов) предлагает поочередно искать оптимальные матрицы **![](https://habrastorage.org/getpro/habr/upload_files/c76/779/b34/c76779b340879059889d5e5f07825d68.svg)** и **![](https://habrastorage.org/getpro/habr/upload_files/f19/286/99f/f1928699f62c59ba2e64dd7d18a32577.svg)**, путем фиксации одной из них. **IALS** немного изменяет функционал оптимизации, но позволяет учитывать места, которые важно хорошо оптимизировать (в которых было больше взаимодействий). 

Модификации функционала:

* Добавление регуляризации на **![](https://habrastorage.org/getpro/habr/upload_files/fa5/dd0/a82/fa5dd0a82f39c34d971e23d17ade76b1.svg)**и **![](https://habrastorage.org/getpro/habr/upload_files/d11/f67/311/d11f6731103667e12ce0ef4f163714b5.svg)**
* Можно учитывать предубеждения пользователей: ![](https://habrastorage.org/getpro/habr/upload_files/be5/1fe/24a/be51fe24a046212ea75c74a36ffd8cfb.svg), где **![](https://habrastorage.org/getpro/habr/upload_files/9f1/2a8/491/9f12a8491ca31f00b934fb7c8dd62ded.svg)** - среднее значение баллов, выставленных по всем товарам всеми пользователями; ![](https://habrastorage.org/getpro/habr/upload_files/0bd/79a/db0/0bd79adb0fb72db0fd55d4602f6477fa.svg) **-** смещение, вносимое товаром;**![](https://habrastorage.org/getpro/habr/upload_files/8e5/294/d70/8e5294d7081f29a6fe2703a0da2e1312.svg)** - предвзятость пользователя.
* [SVD++](https://en.wikipedia.org/wiki/Matrix_factorization_(recommender_systems)#:~:text=problem.%5B8%5D-,SVD++,-%5Bedit%5D) *добавляет* *другой* *набор векторов ![](https://habrastorage.org/getpro/habr/upload_files/65c/d55/9b5/65cd559b57a7e7b211839e65896bc326.svg)*к вектору **![](https://habrastorage.org/getpro/habr/upload_files/e3d/620/c8b/e3d620c8b87633e03c293996c5a470ca.svg)**, если у него в истории был ![](https://habrastorage.org/getpro/habr/upload_files/cac/878/c12/cac878c121946e5ebc9175f2d3ec13b7.svg) товар.
* **timeSVD++** учитывает зависимость от времени. Например, если данные растянуты на несколько лет и вкусы пользователя за это время поменялись. Тогда рекомендовать стоит что-то ближе к текущему времени, а иначе модель будет выдавать некоторое "среднее значение по вкусу".
* и другие.

### 3. Нейросетевые подходы

Пусть имеется матрица взаимодействий ![](https://habrastorage.org/getpro/habr/upload_files/876/4ef/5ad/8764ef5ad14be664f158e910f12a43f1.svg)(implicit feedback). Задача - предсказать какие взаимодействия в будущем наиболее вероятны. Решается - с помощью взвешенной суммы событий из прошлого:

![](https://habrastorage.org/getpro/habr/upload_files/e23/a94/eec/e23a94eecd71f461f0478dd8b92171bf.svg)Причем на матрицу весов наложены следующие ограничения: ![](https://habrastorage.org/getpro/habr/upload_files/c48/4a0/f66/c484a0f6624838c310fd7120f605c5a6.svg) (так модель будет учитывать учитывает только похожие товары) и ![](https://habrastorage.org/getpro/habr/upload_files/71f/d8e/ca5/71fd8eca58b37128f0ebbe075db63aa4.svg) (чтобы избежать элементарного решения). По-сути, ищется взаимодействие пользователя и товара через сумму взвешенных взаимодействий с другими товарами.

Обучается MSE-loss с [Elastic Net регуляризацией](https://en.wikipedia.org/wiki/Elastic_net_regularization) (![](https://habrastorage.org/getpro/habr/upload_files/114/b67/26f/114b6726f26fda2036b23f589159e57d.svg) и ![](https://habrastorage.org/getpro/habr/upload_files/7a9/389/4bf/7a93894bfd49f611ec5de91a15d79a85.svg)): 

![](https://habrastorage.org/getpro/habr/upload_files/a05/03f/e4b/a0503fe4ba3258cde166035f9726b982.svg)причем**основная роль отводится именно L1 регуляризации**, потому что вектор весов мы стараемся максимально занулить (отсюда название метода).

Огромный плюс модели в том, что она очень сильно параллелится (вплоть до каждого айтема). Авторы статьи обучались покоординатным спуском, однако существуют реализации с использованием [**SGD**](https://education.yandex.ru/handbook/ml/article/optimizaciya-v-ml#:~:text=%D0%A1%D1%82%D0%BE%D1%85%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9%20%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9%20%D1%81%D0%BF%D1%83%D1%81%D0%BA%20(SGD)), которые работают значительно быстрее. 

Если изначально посмотреть на корреляции с другими товарами, то можно отбросить те, у которых она маленькая и учить только на каком-то подмножестве товаров. Такая реализация алгоритма оказалась на порядок быстрее, при этом по score не сильно отличается.

Для предсказания для ![](https://habrastorage.org/getpro/habr/upload_files/df4/fa9/c5b/df4fa9c5b8db8800af2469e533fee423.svg)-го пользователя и ![](https://habrastorage.org/getpro/habr/upload_files/0dc/2d4/caa/0dc2d4caae47452ad13cd0d8eee471c8.svg)-го товара перемножаются вектор взаимодействий пользователя и ![](https://habrastorage.org/getpro/habr/upload_files/fe4/797/078/fe47970784764aecff2d44057a9c64ce.svg)-ый столбец матрицы весов ![](https://habrastorage.org/getpro/habr/upload_files/d2d/87f/acd/d2d87facd88490ee52ff1b63997cc01d.svg). 

Такая модель хорошо работает для top-N предсказаний. Для двухстадийных рекомендательных систем SLIM выгоднее использовать в первом этапе, при генерации кандидатов. При этом, во втором этапе (ранжирование и генерирование признаков) удобнее использовать матричную факторизацию.

Основной смысл **факторизационных машин (FM)** в том, что веса при парных взаимодействиях признаков факторизованы. Рассмотрим как строятся такие модели. 

1) Преобразуем матрицу взаимодействий **![](https://habrastorage.org/getpro/habr/upload_files/5de/7bb/cfc/5de7bbcfcecfb3fa5231085377fcfce0.svg)** в матрицу **![](https://habrastorage.org/getpro/habr/upload_files/00f/b09/9f4/00fb099f4ca7ae84d04ac50903ee6b26.svg)**, состоящую из ohe-hot векторов, где ![](https://habrastorage.org/getpro/habr/upload_files/51d/cc4/97c/51dcc497c56963c5af65f7b1363413a9.svg) стоит на месте соответствующего пользователя и товара (**![](https://habrastorage.org/getpro/habr/upload_files/fb3/41b/5dd/fb341b5ddc51cc28b472d72f95e2efa7.svg), ![](https://habrastorage.org/getpro/habr/upload_files/45e/30d/b43/45e30db43caf8987835b6dbe4289ca5f.svg)**).

![](https://habrastorage.org/getpro/habr/upload_files/557/d15/818/557d15818606b596e556ee50e2eef9ad.png)2) Рассмотрим регрессионную модель ![](https://habrastorage.org/getpro/habr/upload_files/71a/361/e7a/71a361e7a65e76fa87b1c1a6513af51c.svg), где **![](https://habrastorage.org/getpro/habr/upload_files/7e0/bff/72f/7e0bff72fe7b4bbd8e8c03294ed04053.svg)** - какая-то one-hot user-a или one-hot item-а фича.

3) Добавим взаимодействия второго порядка. Это позволит учитывать более сложные соотношения между признаками:  

![](https://habrastorage.org/getpro/habr/upload_files/4c3/d40/175/4c3d401759e7d4b18c524f0dcc927bbb.svg)Учет таких попарных взаимодействий позволяет генерировать персональные рекомендации. Можно добавлять и более высокие порядки, однако, выше двойки почти никто никогда не делает.

Однако, матрица **![](https://habrastorage.org/getpro/habr/upload_files/9b8/2bd/8bc/9b82bd8bc175ac889a333231ca006dd3.svg)** будет слишком большого размера: **![](https://habrastorage.org/getpro/habr/upload_files/a27/379/29e/a2737929e8a95a0cf28b98baf69e9fcc.svg)**

4) Факторизуем матрицу весов ![](https://habrastorage.org/getpro/habr/upload_files/cab/9e3/b1d/cab9e3b1df881b81fa57f0489c4d5552.svg). Вместо веса **![](https://habrastorage.org/getpro/habr/upload_files/871/a9b/f25/871a9bf252d631eed4a10828e59874db.svg)**будет учить скалярное произведение векторов **![](https://habrastorage.org/getpro/habr/upload_files/fb6/981/431/fb6981431bbf12c10362dcdaf0bbbc7f.svg)** и **![](https://habrastorage.org/getpro/habr/upload_files/cee/d8d/81e/ceed8d81e733491fb99a1a95a4d2b3a9.svg)** (embedding-и ![](https://habrastorage.org/getpro/habr/upload_files/b4d/be4/c5a/b4dbe4c5ac156be044831f9ee3a89a7f.svg)-ой фичи и ![](https://habrastorage.org/getpro/habr/upload_files/fd2/1d5/a18/fd21d5a186affad8b920efa2676c11e9.svg)-ой фичи):

![](https://habrastorage.org/getpro/habr/upload_files/d5e/0a3/8a0/d5e0a38a0ce6038d0e4ab69a20c327a2.svg)где **![](https://habrastorage.org/getpro/habr/upload_files/048/a97/871/048a97871cd81afafb08bf00545e4c60.svg)** - bias (смещение) всей системы, **![](https://habrastorage.org/getpro/habr/upload_files/cc3/b69/631/cc3b69631dd47d47d986a8bca73e7258.svg)**- bias пользователей и bias товаров, **![](https://habrastorage.org/getpro/habr/upload_files/ff1/30a/8e9/ff130a8e9832ea6f538cfd65b821a205.svg)-**попарные взаимодействия. Таким образом, можно обучать гораздо меньшее количество параметров. Такая модель и называется**Factorization Machines**. 

Обучается модель градиентным спуском. Предсказания модели считаются линейно, что дает неплохое время работы алгоритма.

Если добавить в матрицу **![](https://habrastorage.org/getpro/habr/upload_files/558/6d0/49c/5586d049cd852f14e6694c519d7ac653.svg)** дополнительные one-hot item-ы из прошлого пользователя, образуя хвостик из товаров с которыми пользователь взаимодействовал в прошлом, тогда модель будет дополнительно изучать последовательности взаимодействий. Фактически это будет факторизация марковских цепей.

Идея [FFM - Field-aware Factorization Machines](https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf): разбить признаки из матрицы **![](https://habrastorage.org/getpro/habr/upload_files/f0a/b9d/b99/f0ab9db99d0225040ac1cacbd5081230.svg)** на группы **![](https://habrastorage.org/getpro/habr/upload_files/126/e4a/e70/126e4ae70db709105ecfc9e9465a1b40.svg):**

![](https://habrastorage.org/getpro/habr/upload_files/2d3/343/e5d/2d3343e5d4fe666fa6bb45b469d82af0.png)Тогда модель FFM выглядит как:

![](https://habrastorage.org/getpro/habr/upload_files/c85/cbd/631/c85cbd6312ed15f90c7f1b71a317dfa2.svg)В остальном, как и в FM. 

1. **Two-tower network**

Изначально это все пошло из поска, где обучались **DSSM (Deep Structured Semantic Models) модели**(классические модели поиска и ранжирования) для запросов и документов. Хотелось бы, чтобы вектор запроса был похож на вектор документа, который релевантен и который подходит пользователю. 

Представляем текстовый документ![](https://habrastorage.org/getpro/habr/upload_files/337/9b9/81a/3379b981a4baf1050928ec9962835805.svg)и запрос ![](https://habrastorage.org/getpro/habr/upload_files/3b1/0bc/35a/3b10bc35a9659830632bbc950da1f2a6.svg) в виде [Bag of words](https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%88%D0%BE%D0%BA_%D1%81%D0%BB%D0%BE%D0%B2) (BOW) как ![](https://habrastorage.org/getpro/habr/upload_files/85b/19f/0a0/85b19f0a0ce597bea7e662fee5d68ffe.svg)(~100k). С помощью нейросетей, переводим их в embedding-и небольшого размера (~300). Между ними считается функция близости: косинус, скалярное произведение или др. По значению близости ранжируются документы. 

![](https://habrastorage.org/getpro/habr/upload_files/605/4d9/1fe/6054d91fec7d21d8d68c7c3a472608ed.png)Поиск релевантных товаров можно представить как задачу ранжирования, где в качестве запроса - пользователь, его история и фичи.

![](https://habrastorage.org/getpro/habr/upload_files/e81/43f/04d/e8143f04daf21ad6a9aa9eda06cf7e16.png)**В качестве нейросетей могут быть:**рекуррентные сети (для учета пользователя в прошлом), сверточная сеть, трансформеры, графовая сетка и другие.

**В качестве признаков можно использовать**: стандартные статистики документа: количество лайков, кликов, подписок; признаки автора: количество подписчиков, жанр; неструктурированные данные: текст документа (можно использовать BOW формат, а можно воспользоваться предобученными эмбеддингами), видео и картинки (предварительно представив их в виде эмбеддинга).

**В качестве признаков пользователя можно использовать:**информацию про пользователя: возраст, пол, язык; информацию про контекст запроса: с какого устройства был сделан, в какое время; информацию про друзей/подписчиков пользователя и их взаимодействия; 

**Two-tower** архитектура (в частности DSSM) имеет ряд преимуществ: большое пространство для творчества при конструировании признаков; **быстрый интерфейс**, так как эмбеддинги товаров можно считать оффлайн; возможность построить **оффлайн (и даже онлайн) отбор кандидатов** на обученных эмбеддингаx.