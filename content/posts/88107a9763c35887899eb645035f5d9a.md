---
aliases:
- https://monkeylearn.com/blog/introduction-to-topic-modeling/
title: 'Topic Modeling: An Introduction'
date: 2023-10-02
src_link: https://www.notion.so/Topic-Modeling-An-Introduction-9af7c34c4fbb4713950c94868209e231
src_date: '2023-10-02 15:32:00'
gold_link: https://monkeylearn.com/blog/introduction-to-topic-modeling/
gold_link_hash: 88107a9763c35887899eb645035f5d9a
tags:
- '#host_monkeylearn_com'
---

Topic modeling is an unsupervised machine learning technique that’s capable of scanning a set of documents, detecting word and phrase patterns within them, and automatically clustering word groups and similar expressions that best characterize a set of documents.

You’ve probably been hearing a lot about artificial intelligence, along with terms like [machine learning](https://monkeylearn.com/blog/gentle-guide-to-machine-learning/) and [Natural Language Processing](https://monkeylearn.com/blog/definitive-guide-natural-language-processing/) (NLP). Especially if you work in a company that processes hundreds, or even thousands of customer interactions every day. [Data analysis](https://monkeylearn.com/data-analysis/) of social media posts, emails, chats, open-ended survey responses, and more, is not an easy task, and less so when delegated to humans alone.

That’s why many are excited about the implications artificial intelligence could have on their day-to-day tasks, as well as on businesses as a whole. AI-powered text analysis uses a wide variety of methods or algorithms to process language naturally, one of which is [topic analysis](https://monkeylearn.com/text-analysis/) – used to automatically detect topics from texts. 

By using topic analysis models, businesses are able to offload simple tasks onto machines instead of overloading employees with too much data. Just imagine the time your team could save and spend on more important tasks, if a machine was able to sort through endless lists of customer surveys or support tickets every morning.

In this guide, we’re going to take a look at two types of topic analysis techniques: topic modeling and topic classification. **Topic modeling** is an ‘unsupervised’ machine learning technique, in other words, one that doesn’t require training. **Topic classification** is a ‘supervised’ machine learning technique, one that needs training before being able to automatically analyze texts. 

First, we’ll delve into what topic modeling is, how it works, and how it compares to topic classification. Then, we’ll present various use cases and tools that you can use to easily get started with topic analysis, as well as a series of tutorials that will help you create your own models. 

What is Topic Modeling?
-----------------------

Topic modeling is a machine learning technique that automatically analyzes text data to determine cluster words for a set of documents. This is known as ‘unsupervised’ machine learning because it doesn’t require a predefined list of tags or training data that’s been previously classified by humans.

Since topic modeling doesn’t require training, it’s a quick and easy way to start analyzing your data. However, you can’t guarantee you’ll receive accurate results, which is why many businesses opt to invest time training a topic classification model.

Since topic classification models require training, they’re known as ‘supervised’ machine learning techniques. What does that mean? Well, as opposed to text modeling, topic classification needs to know the topics of a set of texts before analyzing them. Using these topics, data is tagged manually so that a topic classifier can learn and later make predictions by itself.

For example, let’s say you’re a software company that’s released a new data analysis feature, and you want to analyze what customers are saying about it. You’d first need to create a list of tags (topics) that are relevant to the new feature e.g. *Data Analysis, Features, User Experience,* then you’d need to use data samples to teach your topic classifier how to tag each text using these predefined topic tags.

Although topic classification involves extra legwork, this topic analysis technique delivers more accurate results than unsupervised techniques, which means you’ll get more valuable insights that help you make better, data-based decisions.

You could say that unsupervised techniques are a short-term or quick-fix solution, while supervised techniques are more of a long-term solution that will help your business grow. 

Examples of Topic Modeling and Topic Classification
---------------------------------------------------

Let’s take a look at some examples, to help you better understand the differences between automatic **topic modeling** and **topic classification**.

Topic modeling could be used to identify the topics of a set of customer reviews by detecting patterns and recurring words. Let’s take a look at how an ‘unsupervised’ technique would group the below review for Eventbrite, for example:

*“The nice thing about Eventbrite is that it's **free to use** as long as you're not **charging** for the event. There is a **fee** if you are **charging** for the event –  **2.5% plus a $0.99 transaction fee**.”*

By identifying words and expressions such as *free to use*, *fee*, *charging*, *2.5% plus 99 cents transaction fee*, topic modeling can group this review with other reviews that talk about similar things (these may or may not be about pricing).

A topic classification model could also be used to determine what customers are talking about in customer reviews, open-ended survey responses, and on social media, to name just a few. However, these supervised techniques use a different approach. Rather than inferring what similarity cluster the review belongs to, classification models are able to automatically label a review with predefined topic tags. Take this review about SurveyMonkey, for example:

*“We have the **gold level plan** and use it for everything, **love the features**! It is one of the **best bang for buck** possible.”*

A topic classification model that’s been trained to understand these expressions (gold level plan, love the features, and best bang for buck) would be able to tag this review as topics *Features* and *Price.*

In short, topic modeling algorithms churn out collections of expressions and words that it thinks are related, leaving you to figure out what these relations mean, while topic classification delivers neatly packaged topics, with labels such as *Price,* and *Features,* eliminating any guesswork.

How Does Topic Modeling Work?
-----------------------------

It’s simple, really. Topic modeling involves counting words and grouping similar word patterns to infer topics within unstructured data. Let’s say you’re a software company and you want to know what customers are saying about particular features of your product. Instead of spending hours going through heaps of feedback, in an attempt to deduce which texts are talking about your topics of interest, you could analyze them with a topic modeling algorithm.

By detecting patterns such as word frequency and distance between words, a topic model clusters feedback that is similar, and words and expressions that appear most often. With this information, you can quickly deduce what each set of texts are talking about. Remember, this approach is ‘unsupervised’ meaning that no training is required. 

Now, let’s say you train a model to detect specific topics. That’s a whole different kettle of fish, and a step that’s needed for topic classification algorithms – a supervised technique. Let’s compare the two topic analysis algorithms to further understand the differences between them.

### Topic Modeling vs Topic Classification

Topic modeling and topic classification do have one thing in common. They’re the most commonly used topic analysis techniques. Apart from that, they’re both very different and the one you choose, well, that depends on several factors.

In theory, unsupervised machine learning algorithms such as topic modeling require less manual input than supervised algorithms. That’s because they don't need to be trained by humans with manually tagged data. However, they do need high-quality data, and not only that – they need it in bucket loads, which may not always be easy to come by.

At the end of your topic modeling analysis, you’ll receive collections of documents that the algorithm has grouped together, as well as clusters of words and expressions that it used to infer these relations.

Supervised machine learning algorithms, on the other hand, deliver neatly packaged results with topic labels such as *Price* and *UX*. Yes, they take longer to set up since you’ll need to train them by tagging datasets with a predefined list of topics. But, if you label your texts accurately and refine your criteria, you’ll be rewarded with a model that can accurately classify unseen texts according to their topics, as well as results that you can put to use.

At the end of the day, it comes down to this. If you don’t have a lot of time to analyze texts, or you’re not looking for a fine-grained analysis and just want to figure out what topics a bunch of texts are talking about, you’ll probably be happy with a **topic modeling** algorithm.

However, if you have a list of predefined topics for a set of texts and want to label them automatically without having to read each one, as well as gain accurate insights, you’re better off using a **topic classification** algorithm.

Now that we’ve explained the differences between topic modeling and topic classification, we’re going to go into more detail about how each of these machine learning algorithms works… and, yes, things are about to get a bit more technical.

### Topic Modeling

*Topic Modeling* refers to the process of dividing a corpus of documents in two:

1. A list of the topics covered by the documents in the corpus
2. Several sets of documents from the corpus grouped by the topics they cover.

The underlying assumption is that every document comprises a [statistical mixture of topics](https://www.statisticshowto.datasciencecentral.com/mixture-distribution/), i.e. a statistical distribution of topics that can be obtained by “adding up” all of the distributions for all the topics covered. What topic modeling methods do is try to figure out which topics are present in the documents of the corpus and how strong that presence is.

In this section, we’ll help you see the big picture of two topic modeling methods, namely, *Latent Semantic Analysis (LSA) \_and \_Latent Dirichlet Allocation (LDA)*.

Latent Semantic Analysis (LSA)
------------------------------

*Latent Semantic Analysis (LSA)* is one of the most frequent topic modeling methods analysts make use of. It is based on what is known as the [distributional hypothesis](https://en.wikipedia.org/wiki/Distributional_semantics) which states that the semantics of words can be grasped by looking at the contexts the words appear in. In other words, under this hypothesis, the semantics of two words will be similar if they tend to occur in similar contexts.

That said, LSA computes how frequently words occur in the documents – and the whole corpus – and assumes that similar documents will contain approximately the same distribution of word frequencies for certain words. In this case, syntactic information (e.g. word order) and semantic information (e.g. the multiplicity of meanings of a given word) are ignored and each document is treated as a bag of words.

The standard method for computing word frequencies is what is known as *[tf-idf](https://monkeylearn.com/blog/what-is-tf-idf/)*. This method computes frequencies by taking into consideration not only how frequent words are in a given document, but also how frequent words are in all the corpus of documents. Words with a higher frequency in the full corpus will be better candidates for document representations than less frequent words, regardless of how many times they appear in individual documents. As a result, *tf-idf* representations are much better than those that only take into consideration word frequencies at document level.

Once *tf-idf* frequencies have been computed, we can create a *Document-term* *matrix* which shows the *tf-idf* value for each term in a given document. This matrix will have rows for every document in the corpus and columns for every term considered.

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA60lEQVQY02WO4U7DMAyE9/4PCGUDaRqtEKxNm6RJXCe5gzbSNAn/Ovn8+e7UO7xOeLc8zzwbDB7dxMuM3uPNoJt2cZnRGd4cPha8TLhapAyQp5hhhdev8cfGNVMKv5c4urRVmqBmVSn0gruTlOm3encSlKUCwIkAgeHz5u3CY7yzMXiSIhLDShKoKqkJSem4OmDs+RyGwTnX4BBCjJHktm2qSrLWmlKD0aymd7jW2vd9gwGoqoiQTCm1UxFZlr1XKcUYU+tR+pE8jmMIob3MObfAGKO1lqSqNheAc64hO9y+PJo8z/Pyv/iDfwFATpfyEy+aLQAAAABJRU5ErkJggg==)![](/static/5b3e5567ccfcf3f2b8c96c125589062f/3867d/d58b3fa6effe4a00b612495ce2937a10.png)This *Document-term matrix* can be decomposed into the product of 3 matrices (U*S*V) by using *[singular value decomposition (SVD)](https://en.wikipedia.org/wiki/Singular-value_decomposition).* The *U* matrix is known as the *Document-topic matrix* and the *V* matrix is known as the *Term-topic matrix:* 

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAABSUlEQVQoz22RzY4cMQiE+/0fMYccMppNz99qMm0bDFQR0b3a5DAlZH1gl5HxkpnM0nCetjht8bth7fHR4tJx3uLacXr5beDXK56K/E/LN4HZZK63h6Pg1WU6mugMNrEZ7GoWfGMmq6qq5/M5M6fqGIPAVK26SGbKvr7vnJlm9nw+C9w9SiICsPVBsvd+9HhvJiCiw2mBQ2YGUt27U82zzP/8S4D+FRnIU2OzSg2MvY8XpNY4qX5sVRHM5SH4+akfW9yaX5utm9173HrcR0iA5B/lj2tbm9eBzW49rs0+BTO4gOytkZxTp6qquO9PdgfpATAvl9XmdDMZQ1VERkTtLEHIqHmYled4p+86zAE+Hg8z01k6ICIcWMxDdJqHVmub5gcUe9TgmPf7/ftGkmMMAGUOwAP7xyB22NOvYv25hwWm+fSo2EHNzeMv3RKEuIyJ1iMAAAAASUVORK5CYII=)![](/static/7f343842ee871e0fe7393126ed20e069/c8a3b/c6a4c958656249e780f5c8d88b53c421.png)Linear algebra guarantees that the S matrix will be diagonal and LSA will consider each singular value, i.e. each of the numbers in the main diagonal of matrix S, as a potential topic found in the documents.

Now, if we keep the largest **t** singular values together with the first **t** columns of U and the first **t** rows of V, we can obtain the **t** more frequent topics found in our original *Document-term* *matrix*. We call this *truncated SVD* since it does not keep all of the singular values of the original matrix and, in order to use it for LSA, we will have to set the value of **t** as a hyperparameter.

The quality of the topic assignment for every document and the quality of the terms assigned to each topic can be assessed through different techniques by looking at the vectors that make up the U and V matrices, respectively.

Latent Dirichlet Allocation (LDA)
---------------------------------

*Latent Dirichlet Allocation (LDA)* and LSA are based on the same underlying assumptions: the distributional hypothesis, (i.e. similar topics make use of similar words) and the statistical mixture hypothesis (i.e. documents talk about several topics) for which a statistical distribution can be determined. The purpose of LDA is mapping each document in our corpus to a set of topics which covers a good deal of the words in the document.

What LDA does in order to map the documents to a list of topics is assign topics to arrangements of words, e.g. n-grams such as *best player* for a topic related to sports. This stems from the assumption that documents are written with arrangements of words and that those arrangements determine topics. Yet again, just like LSA, LDA also ignores syntactic information and treats documents as bags of words. It also assumes that all words in the document can be assigned a probability of belonging to a topic. That said, the goal of LDA is to determine the mixture of topics that a document contains.

In other words, LDA assumes that topics and documents look like this:

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAIAAAB2/0i6AAAACXBIWXMAAAsTAAALEwEAmpwYAAABFUlEQVQY03VRSU4EMQzs/78OcULigsQMQ29JOqvteIH0gIQElCIfKnaVU5nsF1TVObedQET7H5Oqiggz/2QRsdUGDUTk1DM9eREWFVP9GmZm5/2+70Skp62ed977u60MQlSHyhFiLcW+2yaNWyp+hpg3xNrVTLBp8TulmhEjiwmztJByWKrfcvYGdbQxTxLXVNwMR9kISlfT1DDkupfmMhwZWYZt3NZ5uYT10vY1Hbcu47GTHGvK+w1CXhAKqalv8uzpcaGHmZ5cv0QNlVt4vS5vt9nh8X59fwkbmcmkNQLkgyqkTq3foxlb6TisBmxAjC3lGmuqHYoPa0qkKhMSIfWzEnX680tElZg7S2emPhLvzJ+BfQCVQtIXzLtK4QAAAABJRU5ErkJggg==)![](/static/4fbb5fc319aef0946ed9579db24e4dce/31f17/85066c68036e4e949fe9f680a071f3fd.png)And, when LDA models a new document, it works this way:

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAIAAAB2/0i6AAAACXBIWXMAAAsTAAALEwEAmpwYAAABBUlEQVQY03WQW0vEMBCF9///PX3wRRAWa9emu81tkrlL2iKCejhkIJmPmZyL/6UQwsc8z/NtmqaO6P/o8vvK3FVFRVRYmNz0+8XczFxH3WFV5VMyLOJmYl5JKxmwFdLGgzdRrIAQMZfWwcwuIrLFlHI5XGsNoO9ZnwI9B3658+uDlyruTixpDut6jZ9vcQt6wLmUUqEAlAoAkFEzee4SgSNw7SxjURfGtF6n9RbmGdblhLd0Ts5lePxNBXsjQiJsrRGRmhlT26blcV/CBimY7nDrvSN2JETqiKNNhIiOlFrvFcDH5LFjKqnE3Cqo6oAR9xHjZCRW1Z/hH2HuWRuzIFEnYhk9X2Uk0qUeX3P+AAAAAElFTkSuQmCC)![](/static/6e9f10d41b8953374e8c9678df03ca6a/e56cd/db5acced596a400ca6650486904e1660.png)The main difference between LSA and LDA is that LDA assumes that the distribution of topics in a document and the distribution of words in topics are Dirichlet distributions. LSA does not assume any distribution and therefore, leads to more opaque vector representations of topics and documents.

There are two hyperparameters that control document and topic similarity, known as **alpha** and **beta**, respectively. A low value of **alpha** will assign fewer topics to each document whereas a high value of alpha will have the opposite effect. A low value of **beta** will use fewer words to model a topic whereas a high value will use more words, thus making topics more similar between them.

A third hyperparameter has to be set when implementing LDA, namely, the number of topics the algorithm will detect since LDA cannot decide on the number of topics by itself.

The output of the algorithm is a vector that contains the coverage of every topic for the document being modeled. It will look something like this [0.2, 0.5, etc.] where the first value shows the coverage of the first topic, and so on. If compared appropriately, these vectors can give you insights into the topical characteristics of your corpus.

For more information on how those probabilities are computed, the statistical distributions assumed by the algorithm, or how to implement LDA, you can refer to [the original LDA paper](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf).

Also, for more information on how to compare vector representations to get insights into document similarity or the distribution of topics over a document corpus, you might want to read about [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) or other [similarity measures](https://en.wikipedia.org/wiki/Similarity_measure). All of these comparisons can be used to compute similarities between the output vectors of both LSA and LDA.

### Topic Classification

If you’re running a topic classification analysis, you’ll need to predefine a list of topics. For example, if you’re a software company and you have a set of customer reviews you want to analyze, you’ll probably include topics such as *Functionality , Usability,* and *Reliability* on your list.

To determine your topics, it’s always best to do some research. You could always use the previously discussed topic modeling methods to determine topics for your supervised machine learning model – it’s certainly a quick way to find out what a batch of texts is talking about. However, in most cases, you’ll probably refine topics as you build your topic classification model.

When you have your list of topics and reliable datasets, you’ll be able to start training your algorithm by tagging text data with your predefined list of topics. Once you’ve tagged enough examples, your algorithm will start tagging new reviews automatically – that’s machine learning in action. Keep in mind that before you put a model to work on large sets of data, you should be 100% confident that, one, your topic list will deliver the results you’re after and, two, your tagged dataset is consistent.

To understand the ins and outs of this supervised machine learning model, let’s delve a little deeper into the three ways you can approach automated topic classification: **rule-based systems, machine learning systems,** and **hybrid systems**.

#### Rule-Based Systems

By now, you’re probably a bit more clued-up about machine learning; it goes something like this – humans lead by example and algorithms follow suit, right? Now, let’s throw you off a little (don’t worry, it will all become clear again)... It's also possible to build a topic classifier without machine learning!

You’re probably thinking, doesn’t that make it an unsupervised model?

Well, no. This type of model runs on a rule-based system. It works by directly programming a set of hand-made rules, based on the content of the documents that a human *expert* has read.

The rules represent the code written by the expert, meaning an algorithm can differentiate topics by deciphering semantically relevant elements of a text, while also taking into account the metadata a document may have.

Each one of these rules is made up of a pattern and a prediction. Since we’re focusing on topic analysis, the prediction will be the topic.

Now, let’s imagine you have a set of support tickets you want to analyze using rules. First, you’d need to define a list of words, one for each topic (e.g for billing issues, words like price, charge, invoice, and transaction, and for app features, words like usability, bugs and performance). Once you have your list, your rule-based system is ready to analyze new tickets. It does this by counting the frequency of your topic words to determine which labels to assign new tickets. You can improve these systems as you go along by refining existing rules and adding new ones.

The downside of these rule-based systems is that they're too complex for someone without expert knowledge, and require constant analysis and testing to ensure they’re functioning in the correct way. Plus, when you add new rules, existing rules are altered. In short, these systems are high-maintenance and unscalable.

#### Machine Learning Systems

Rule-based systems are gradually being replaced by automated machine learning systems that deliver better results with much less effort.

A topic classification machine learning model needs to be fed examples of text and a list of predefined tags, known as training data. However, machine learning models can’t understand text, so information first needs to be transformed into vectors (lists of numbers that encode information) before they can recognize patterns, extract relevant information and categorize data. 

One of the most common ways to transform text information into vectors is *bag of words vectorization*, which you can learn more about [here](https://monkeylearn.com/blog/beginners-guide-text-vectorization/). 

Once the text is transformed into vectors and the training data is tagged with the expected tags, this information is fed to an algorithm to create the classification model:

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAAsTAAALEwEAmpwYAAABQ0lEQVQoz42QWU7DQBBEff8TcQRAiOUHEgKE4C12Zu1ZjMeeLuQQISEQQnrqn65SV3WRZvQBhwARYUfQ/7AjpozCJVy2uOlwvceLRu1QfUK/UNJp+0aIE4qQ8Gr5ouHzmjeK1xIrgZXEWn3jSeNmj4qyj+8URxFymFD4hJL4zfLWcEXoPIsBYkDruPFo3JLlCxmm/iB6IWWYF7Mb0fvsRm4cryQamoUQ2pje5weBjcLO4tUuge8FGpusMdbak9kntJRNnBrKa4XaTtp6Q6Gh+VHhxeC2w3W3lHxUKG3eqmmnF/Gp887kTRcqQrvk5Jq4plwTt99jtx594PIo23uOn5crh5/1flIS9JCVFOJw0EMOCUWccPacr7b6bitW+1AdRb+ys8t3tH9Xbuh89gnFOKMLkJFFmFVkPeAP5MAysjrOlPEB4Yc4LhqIpAIAAAAASUVORK5CYII=)![](/static/ab13089489f1706b363c5d5998baa35a/a4bfb/text_process_training-1.png)This classification model is now able to classify new texts because it has learned how to make predictions automatically:

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAIAAAC9o5sfAAAACXBIWXMAAAsTAAALEwEAmpwYAAABKUlEQVQoz42S627CMAyFef9H2gtsErsIpu0Po7S06QUoSZ1AL0l8pgR1Gr826ZNlyTmxc5yF9RgdJo/JhaS3/+JqYRkLNeBR4LnCssLnCQUhjxQ65CH+MJdywp5gJiy6AVuJnQoxJzSG2x5tj8pENMRMbdAYzFUOYjWgvXoz+YL4/Yiys0oqrU1teH3ARqKcxasDBHlN1EolL47G2LkmJy9jqXkrUZM9nYO81j6RSBTeGrxUSDtsJdfGd9q0ks5XH8Q0Iu9ccuzLOGdBXgS4IC51aJt1QVnEvAw3+kxxGCG8ecTHkVNpd+24V05Eb8QvShO45QVhWfGT4FTdDBvxkLhVdl6l7eZwFebe4XuyDuuGX2v+ktEw68OSR8bEGPxfG3bhO9h4zDG+AdToAUEdVWFMAAAAAElFTkSuQmCC)![](/static/5c41f1e06c37e8f80cdf5285da3b149f/bdfee/text_process_prediction-1.png)And if you want to keep training topic classification models to make them even more accurate, you can just tag more data, without having to go through complex procedures. 

Now that you’re knee-deep in machine learning, let’s meet some widely used algorithms for topic classification:

##### Naive Bayes

[Naive Bayes](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) (NB) is a family of simple algorithms – the most popular being Multinomial Naive Bayes (MNB) – that deliver great results when dealing with small amounts of data, say between 1,000 and 10,000 texts. It works by correlating the **probability of words appearing in a text** with the **probability of that text being about a certain topic**.

##### Support Vector Machines (SVM)

[SVM](https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/) is slightly more complex than Naive Bayes, but follow the same rules. The upside is that they often deliver better results than NB for topic classification; the downside is that they require complex programming and require more computing resources. However, it's possible to speed up the training process of an SVM by optimizing the algorithm by feature selection, in addition to running an optimized linear kernel such as [scikit-learn's Linear SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html).

Once training data is [vectorized](https://monkeylearn.com/blog/beginners-guide-text-vectorization/) (numbered), SVM is able to separate these vectors into topics by creating ‘a boundary’ or imaginary line (otherwise known as a *hyperplane*). Now, when a new text is analyzed by the algorithm, it’s able to categorize it automatically based on the information received from the training data, and decide which side of the line it should appear:

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAVCAYAAABG1c6oAAAACXBIWXMAAAsTAAALEwEAmpwYAAACWUlEQVQ4y31VXYvTQBTdP+xv8BcIss8ii4+iPq3iCiIL+rCC9kFYlqKlSZp020yapptkvufIzCRpk6a9MGQumZy5955zby4AUFgz6Jkxxq12PzQhBCaTCbIsAyEE6/XaPS+MMVXzgTm05opur7Qxb24Ss8yo8+M4MWEYmm1RmDRN3SKEmB4g5xxKaxcB5cqtNjqlDF59XGCVCyjJEIYRxqwHyBiDkMq9sB+//hT3StBaFEWohYRME2xvr/177S8/AlTKA5ItByk4Pv8kiFPWlbksS8SLhfO1FFC07so/Cqi1xiEHL98HmC7Kzg/DEPZcj8cDAkcBdXPy+58NXrydd2BFUSBJkp4KDtdJwPbm/ElgGlVdLPMggJSyV9PD2rqUAVSt39bQRqgH2ttsNk5rrnaNElowQR6hWe0j1FpXVi4toDG6B6S1TyeYBx1Ql3Lj7+6+gifBPuWGWSM4Q0UFfk13qJgCF57xNE1BSOb1SGuI9PGom0ZT1pIjTms8v/qHyw8Brm48AbOZJ6a4vQZfhtjdfRslZpSUNuXVhiF70iDpCnmed7orJz/A1/EesK1lsx+0HgPlEvfzpyZm6Zj14Xv2ba1kTvakDMjrAWrFsSQ1nl0+gAlguUywK4ru8PbLO2hadReMDKnx1rMepRRRGLpDLPyL+v53E7Tsa9CcAXTTpullOwCq0recWCfg8XyU2bPTpq5rF5kdmrPZDEJK1FUFJhWYNqDN+1PLBtMDlFK423sDoJXFQNRnI2zHtX3atG2EY/1qJXXut3CU8lD1w5oxRqGbGp8C/A+p9GZUy1QCLAAAAABJRU5ErkJggg==)![](/static/f914d9b97af5119b70e76331ea8a8df5/ae702/78a17d5336644ee8a136f8efacad1e3e.png)Ever played battleships? Well, all SVMs components work in a similar way. Played on ruled grids with a wall separating the two players (the hyperplane), each player's fleet of ships are marked (vectorized), and each side represents a different fleet of ships (topic). Now, when one player enters a code (prediction), and hits the location of one of the ships, you’re well on your way to eliminating the rest of the fleet, or in an SVM’s case, identifying the topic.

##### Deep Learning

This is a machine learning technique that teaches computers to do what comes naturally to humans: [learn by example](https://www.mathworks.com/discovery/deep-learning.html). While it’s been around since the 1950s, deep learning is making a come back, not just because of lower computing costs, increased computing power and availability of data, but also because it’s achieving more accurate results than ever before – results that often exceed human-level performance.

Topic classification, in particular, has benefited from deep learning’s revival and uses two main deep learning architectures: [Convolutional Neural Networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNN) and [Recurrent Neural Networks](https://en.wikipedia.org/wiki/Recurrent_neural_network) (RNN). If you’d like to delve even deeper, and find out what the differences are between these two frameworks, [check out this comparison](https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f).

Before you get too excited about deep learning algorithms, however, be aware that they require much more training data than traditional machine learning algorithms. Instead of 1,000 training samples, you’ll need to millions. On the other hand, traditional machine learning algorithms such as SVM and NB are limited. After you go beyond the threshold of training data for these algorithms, accuracy stops improving, while deep learning classifiers continue to improve the more data they have:

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAIAAAAmMtkJAAAACXBIWXMAAAsTAAALEwEAmpwYAAABN0lEQVQoz42SW0/DMAyF+/9/HC8gJIQ0xG1bYW2TJk7ixDlGpR3rYNLwQ6JcPp/EPg0AvRbrK6UuS6g2wIxfSIEJwzxXYGS8OOxokQPQLOgqORTrpVQ9BH1308hyesUEqyrnXI97+H5CBaqWQ6YHSzf9+Ohsy7Yv9lBMn92P3gSHGOuRFlRbXMvmztpbE7okGbkgp5pjnUau5VyZF2USbnO/jWEzVpsUV6p4VFYoSdxxv6W69VNJ9fznl2EAIrWgPIfueUQfl8pf7eCp2htv7ocUi/4HO8Hzb18pspwJAuCcmXNKPHsBcw9X0aztsT4QkcGMzo3eu/rHC4vy06BvHVk7+hB+2UtVh6R7JynGFLjPriUTKXoi56kUaT68ftpgrPX+AmxZ9zYT+UDJZOriGCg6Ik9BRL4A1zP9xPpTkCgAAAAASUVORK5CYII=)![](/static/3d09638f3a92faf67513f1026318a455/6b7b4/deep-learning-graph.png)There are many machine learning algorithms out there that serve a different purpose depending on the problem you’re trying to solve. For instance, [spam detection](https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering) was declared ‘solved’ a couple of decades ago using just Naive Bayes and n-grams. [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) or [GloVe](https://nlp.stanford.edu/projects/glove/) are great deep learning algorithms for getting better vector representations for words when using traditional machine learning algorithms.

#### Hybrid Systems

These are simply combinations of machine learning classifiers and rule-based systems, which improve results as you fine-tune rules. You can use these to rules to tweak topics that have been incorrectly modeled by the machine learning classifier.

### Metrics and Evaluation

Ok, algorithms, done. Now, let’s focus on the results, and a question that you’ve probably been wondering from the start. How do you know how good your topic classification model is? 

Well, you already know that supervised machine learning models need to be fed training data that is relevant to your predefined topic list. By using a subset of that tagged data to test your model, you can find out how accurate it is at making predictions. You can test the actual tag for a specific text and compare it to the predicted tag then, with the results, calculate the following evaluation metrics:

* **Accuracy**: the percentage of texts that were assigned the correct topic
* **Precision**: the percentage of texts the classifier tagged correctly out of the total number of texts it predicted for each topic
* **Recall**: the percentage of texts the model predicted for each topic out of the total number of texts it should have predicted for that topic
* **F1 Score**: the average of both precision and recall

The best way to test your model and receive these evaluation metrics is through [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)). This involves randomly splitting your training dataset randomly into equally sizes (e.g 4 sets, each with 25% of the data), then training a classifier with one of these sets, and using the remaining unseen 75% to test your classifier. Then you can build the final model by using all the sets as training data.

Use Cases & Applications
------------------------

From sales and marketing to customer support and product teams, topic modeling and topic classification can help eliminate manual and repetitive tasks, as well as speed up processes in a simple and cost-effective way. 

Teams spend hours routing support tickets, customer feedback and the like by tagging and sorting data, which machine learning algorithms can do automatically, and in real-time. They also function 24/7 and deliver more accurate results that businesses can use to make insightful decisions. On top of that, getting started is easy. With software like [MonkeyLearn](https://monkeylearn.com/), you don’t need coding skills or much knowledge about machine learning (that’s not to say this article isn’t full of useful information; it always helps to understand the tools you’re working with!).

To understand how machine learning could help you, let’s take a closer look at the areas in which topic classification and topic modeling are making waves:

### Customer Service

In this day and age, customer service can break or make a company. It’s no longer about having an awesome product when there are competitors out there with similar products. Your unique selling point (USP) now needs to be about constantly delivering a kick-ass customer service that will make you stand out from the crowd. 

[Thirty-three percent of Americans say they’ll consider switching companies after just a single instance of poor service.](https://www.helpscout.com/75-customer-service-facts-quotes-statistics/) But how can you deliver great customer service when your teams are bombarded with more data than ever before?

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEPUlEQVQ4yy1Ty1NTdxjNotNF/4MuOt21m8501Y6Oi74XOlqdzrSdjhbr+Bhr7fjotKUiltEB1GKt9UlrEZGXTkaQCAQTIAncBBICkZuAhCTk5r7zggh53vv7nc6N7r7Fd+ac75zzmUSi87qWBfpGx2r2H+wdHN5/5Gjt7w2nGs6eaWp++MRW33z+u0OHT9bWtXZ1KYBAiEiJBCpRahJBBKqrwFJmtddqD8mKPxYbcE3cH7Y+mfJOhha8kUjnI8vNe51Olo0Xi7ymCZrOVyqJctkAS0A0n/eyM2zI8zS6lKJYBXJAFsgAXKEkFsvPq7M3vGy2joz7/P02u285YhKongQmgyw/by4LIwwzFMuXZEp5TVOAkKz4/MO20Z6Z6HIKCHDcoGvCNc9aPVPcRsEAK0BQlqeY/oHHHSMedxaQdCLqJAlMRZbba79v+WTzjTtt19vv3rrbceXf1pAsGzukejNP9BzQzfiPDbt/Pdf80GZPAjLAFUtWj+d03bmak01N167e7mhsuXahretmV999vlSWKV6C14AOl3ffn62Xrl0f8/klQmRKJUK59Y3zdzq2Npw5+MOX/9xtvn3v0oiru+Xv+unwUgYvwJRkAJ+Sbo+q0wmxAIiEiIQqFPGN0qn6w7u2vLp10ysfb3lj564Pvtnz+Y5t77qD/jRgkig1ckpnnjDuYDQ6F40xC8/EckUBVEJTgFwuhTOp0Sm7PxxwPp0anWHm4ksqIFNqCqczi2pqNha3jLucAZZhF0a9/okAyz3PL8hqOJnhNgqxXD62lo9k12O5/EouH8k+X1STXL5g4suVeKnEVyoKpUJFSwJzCeH4L7UXr97Yve9Aw/mLTZevWCaYNCCUyolCkcsXEoViolASKxWTAiiARKmkGyapQKJcZhYW71tH2swPzbbRSzdu+ZYjS5m1QJyXNGIsV+2UCTVJBDJFuloglUKiRoBJvCzZGqXrwGw0Mu3pG3eZx/wzSUDRdYOMUoOZr2hMkLVOupYyWRVQqvwyrSrSjdrb3GOOjgu3tn96rr5+xOuzOFypFyVJAxMBP+vuFoJmu2OI1ygriJ7FZ6wgeiPRkKxkAR/Hndixs+b1Nw8e//nBsHXA4cwCok4MsNPnFlgz0k6noz9aKHEbG5ZxZ1df/2PnRHh1VQZUICCr+1t7D9nmrvaYVePmKrMRcipttjxo7Gy7MzisaMQ6Pd0zYJld4cSKplblSYCYX6975NgdKd5kZtMgAjH6Z+J1UgC6nd6vndzev/7buf3D1s7env7BoUnHalWbRKjhy9rqj4PMVwrO2qdXcmvSy8cgRAUut997/7PN7216a9uOj/Ye2PPO2681Np9YBwRNe6FwJZcbWowO5DSXoMRLRZFS4zFUYF6Qegf7mxoP//bTF7W13x49sv3CH8fmE7FkFawCQUF8ZLNbHU5/KDQZCFgZz4KiqsD/I2zM/+5epfcAAAAASUVORK5CYII=)![](/static/60d82edaf2b6e04d57a068f6dbb379d9/ebf75/af5fc8efbb6d4ce599a630e16fb475cd.png)The answer lies within machine learning. Topic modeling and topic classification models can be used in customer support to help teams handle large amounts of data by:

* Automatically [tagging customer support tickets](https://monkeylearn.com/blog/automate-your-zendesk-ticket-tagging-with-keyword-extraction/) according to topic, or recognizing patterns and delivering results in the form of frequently occurring words and expressions.
* Automatically [triaging and routing conversations](https://monkeylearn.com/blog/automatically-route-customer-support-tickets-to-the-most-appropriate-person/) to the most appropriate team. For example, tickets tagged *Billing Issues* or *Refunds*, or containing expressions such as ‘credit card transaction’, ‘subscription error’, and so on, would be sent to the accounts department. Likewise, queries tagged with *Bug Issues and Software,* or containing expressions such as ‘strange glitch’ and ‘app isn’t working’ would be sent to the dev team.
* Automatically [detecting the urgency](https://monkeylearn.com/blog/detecting-squeaky-wheel-urgency-in-customer-support-tickets/) of a support ticket and prioritizing accordingly. For example, if a ticket is tagged as *Bug Issue, Urgent,* or a machine recognizes expressions such as ‘right away’, ‘immediate attention’ etc. This approach to text analysis has helped companies avoid a potential PR crisis, and even [make the most out of a bad situation](https://www.jcsocialmedia.com/kfc-experts-in-using-social-media-to-avoid-ruffled-feathers/).
* Getting insights from [customer support conversations](https://monkeylearn.com/blog/analyzing-customer-support-interactions-on-twitter-with-machine-learning/)

Instead of tasking customer support teams with tagging and triaging customer queries, which is both repetitive and time-consuming, you can [delegate these tasks to machines](https://monkeylearn.com/for-support/). This means customer support agents are able to focus on more important tasks, like personalized customer attention or dealing with more complex issues that machines are unable to solve alone. 

### Customer Feedback

We’ve all come across the term customer-centric – [a strategy that’s based on putting your customer first, and at the core of your business.](https://www.superoffice.com/blog/how-to-create-a-customer-centric-strategy/) That means listening to the [Voice of Customer](https://en.wikipedia.org/wiki/Voice_of_the_customer) (VoC), in other words, what customers have to say, via reviews, social media posts, emails, chats and surveys, and responding in a way that will make them want to use your service or product again, and even recommend it to others. [After having a positive experience with a company, 77% of customers would recommend it to a friend.](https://www.qualtrics.com/blog/customer-service-stats/)

The best way to approach this customer-centric strategy? With machine learning in tow. 

Not only can you use topic classification and topic modeling for processing customer feedback and responding in a more timely and effective manner, you can also use it to make more informed decisions, either on the spot when dealing with individual customers, or when making improvements to your product or service. 

Topic analysis models can help automatically analyze open-ended responses (unstructured data) such as NPS responses, customer surveys and product reviews, all valuable sources of information that can help shape your product or service, and encourage business growth. 

Let’s say you receive a batch of NPS responses, where the first question in your survey asked customers to rate your product on a scale of 1-10. The quantifying answers to this question are easily analyzed, and customer are categorized as promoter (9 or 10), passive (7, 8) or detractor (6 or below).

However, the second question is a follow-up asking the customer to explain the score they gave – this explanation is the most important data. However, as luck would have it, it’s also the most difficult to analyze. Imagine that a customer leaves a score of four and provides the following explanation:

*“The app is simple to set up, but the cost per user license can be prohibitive for small companies.*

That’s one short piece of feedback with two topics (*Ease of Use* and *Pricing*). Now, imagine receiving thousands of responses just like this, both positive and negative, and all mentioning different topics. Processing all these open-ended answers manually is bound to give you a headache. Topic classification and topic modeling can eliminate this manual labor by sorting these survey responses automatically. While topic modeling will group similar responses together, topic classification will deliver topics for each piece of data. 

Here’s an example of how one company was able to use topic classification to [analyze open-ended NPS responses using MonkeyLearn](https://monkeylearn.com/blog/how-retently-automated-customer-feedback-analysis-using-monkeylearn/):

[Retently](https://www.retently.com/), an NPS tool, trained their classification model to tag each response with different topics, including *Product, UX*, *Customer Support* and *Ease of Use.* Then, within these topics they divided the responses into Promoters, Passives, and Detractors to determine the most popular topics for each group. The results? Take a look for yourselves:

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAAsTAAALEwEAmpwYAAABm0lEQVQoz2WRbU8TQRDH7/t/BV8Yy1teaEyhbVKwplFCTUwLaksiveN65TgKtXu3D/OwO7hXURKT304mk/ln/rOTqNqoxvxSza7W3nuRECTIntASnxeRHdSvloP+1aHMD1TvUJ8OEyJGIkQCpBBbo86L8HPcE0RAQuXNjpXQjk3DxiTsA5D3rYyDv1Lrh+21PC5CZN7GhTwuZDuXzXVIV5QVLi/c8qdbF4nSoAzsZ4LHd8XnYTa4XHany6NpejzNetO0N0uPv6bds5vTctaHyVv9YaDnP+wqT7Sl2iDHbQU99crJm9W4U5x1bj918lEnHx3ko9f56Gg9nl+ebNbfUBe2Ki17EEkag8rg3rbxdH47uU+7Rfo+W3y82ZZZXWXqbqnuqrryVel22rJY9tahA0q0I+I4trVNw/LLedbnzYzrhtEzCVNgCoQhyogdoAU0gICcaIcO+VmMo+pi1WxjBwcL9BfX8rICRElj/4kd4/j+ew0O0FuIxhz+IQr2eVu08bQcf/tBWQMUL+ytyiZaNRbIOLQvMP8lv3d+AkPuMaq1PZQHAAAAAElFTkSuQmCC)![](/static/c11b2a90ef69095d334bef3c891250fe/5f3ac/7b3779e3edd04318a1ccf5a86b6cf911.png)There are other surveys that you could use instead of NPS. For example, customers surveys created with [SurveyMonkey](https://www.surveymonkey.com/) which lets you create sophisticated surveys and send them out via email, chat, mobile, social media, web, and more. As opposed to NPS surveys, [more than 40% of SurveyMonkey’s surveys include open-ended responses](https://www.surveymonkey.com/curiosity/sentiment-analysis-and-word-cloud/). That’s great in terms of data, but you’re still faced with the same problem as you are with open-ended responses from NPS scores – how do you analyze all this unstructured text data? To learn how, check out this article about [auto-tagging responses in SurveyMonkey with AI](https://monkeylearn.com/blog/auto-tag-responses-in-surveymonkey-surveys-with-ai/).

By importing information from surveys into your topic classification models, you’ll be able to find out the topics your customers are talking about in no time at all!

Product reviews are also a great source of unstructured customer feedback. Not only are they available in abundance on sites like Capterra, G2Crowd, Siftery, Yelp, Amazon, and Google Play, they’re also easy to gather using [web scraping tools](https://en.wikipedia.org/wiki/Web_scraping). These tools automatically collect specific information from different websites, so that you don’t have to spend hours copying and pasting reviews that mention your brand.

Plus, there are plenty of easy-to-use scraper tools available that let you build your own, without the need to code. Some of the best include [Dexi.io](https://dexi.io/), [ParseHub](https://www.parsehub.com/), and [Import.io](https://import.io/). Once you have all your information in one place (Excel sheet, CSV file, Google sheets), you can use a topic classifier to detect the topics your customers are talking about most often.

### Resources

You’ll want to get started with topic modeling and topic classification as soon as possible. And the great news is that you can. With so many tools and resources available, which are simple to use, it’s easy to get started. 

We’ll introduce you to a selection of tools, including topic classification and topic modeling APIs, [open-source libraries](https://monkeylearn.com/topic-analysis/#open-source-libraries) and [SaaS APIs](https://monkeylearn.com/topic-analysis/#saas-apis), to steer you in the right direction. If you want to dive deeper into how topic detection works, and practice what you’ve learned, our recommendations for [papers](https://monkeylearn.com/topic-analysis/#papers) and [online courses](https://monkeylearn.com/topic-analysis/#courses-and-lectures) are sure to pique your interest. Finally, we’ll provide you with some [tutorials](https://monkeylearn.com/topic-analysis/#tutorials) that will help you create your own topic classification and topic modeling tools.

### Topic modeling APIs

Application programming interfaces (APIs) are a great way to seamlessly connect applications and extend the functionality of your apps. Luckily, there are plenty of topic modeling tools with their own API, and various languages in the data science community that are ideal for these machine learning models. Let’s take a closer look:

#### Open source

If you know how to code, there are many open source libraries for implementing a topic modeling solution from scratch. These are great because they offer flexibility and customization, and give you complete control of the whole process – from the pre-processing of data (tokenization, stopwords removal, stemming, lemmatization, etc), to feature extraction and training of the model (choosing the algorithm and its parameters).

In the following section, we’ll cover some of the best libraries for topic modeling using Python and R. 

**Python**

Python is one of the most popular programming languages for machine learning and data analysis. Its focus on code readability makes it super easy-to-use, and it has a large community of contributors who have developed a wide range of options to implement NLP models. 

One of the top choices for topic modeling in Python is [Gensim](https://radimrehurek.com/gensim/), a robust library that provides a suite of tools for implementing LSA, LDA, and other topic modeling algorithms.

[NLTK](https://www.nltk.org/) is a framework that is widely used for topic modeling and text classification. It provides plenty of corpora and lexical resources to use for training models, plus different tools for processing text, including tokenization, stemming, tagging, parsing, and semantic reasoning. Although NLTK can be quite slow and difficult to use, it’s the most well-known and complete NLP library out there.

[SpaCy](https://spacy.io/) is the fastest framework for training NLP models. Although it is less flexible and supports fewer languages than NLTK, it’s much easier to use. SpaCy also provides built-in word vector and uses deep learning for training some models.

[Scikit-learn](https://scikit-learn.org/) provides a wide variety of algorithms for building machine learning models. It has excellent documentation, and intuitive methods that make it easy to train a model for topic modeling. If you are a beginner, you’ll find lots of useful tutorials that will help you get started with topic modeling using scikit-learn.

**R**

R is a statistical programming language that has become quite popular among data scientists and machine learning enthusiasts. It has a very strong community and over 10,000 packages that greatly extend R core functionality. 

[Topicmodels](https://cran.r-project.org/web/packages/topicmodels/index.html) is the go-to package for topic modeling in R. It provides an easy-to-use interface for creating LDA models and is built on top of [tm](https://cran.r-project.org/web/packages/tm/index.html), a powerful package for text mining applications.

#### **SaaS APIs**

Perhaps you don’t have time to code your own solution, or your problem doesn’t justify the costs of implementing your own machine learning software. Maybe you don’t have the resources to train and maintain your own topic classification model. Whatever your reasons, rest assured that there’s always a way with machine learning!

Machine learning is now offered as a service, one that’s simpler to use and doesn't require programming expertise. That’s right, instead of creating your own APIs and algorithms, all you have to do is configure your machine learning service with your existing data, via a user-friendly interface. 

To use one of the APIs that machine learning services offer, you’ll need to integrate them with SDKs (Software Development Kits) for popular programming languages or with integrations from third-parties. Other than that, the rest is smooth sailing.

Here are some machine learning services that you can try out for free:

* MonkeyLearn
* Amazon Comprehend
* IBM Watson
* Google Cloud NLP
* Aylien
* MeaningCloud
* BigML

### Tutorials

We’ve explained the ins and outs of topic modeling and topic classification, and now it’s time to put what you’ve learned into practice. But, before diving headfirst into topic analysis, we’ve put together a series of tutorials to help you create a model that will deliver the best possible results. 

Whether you’re using open source libraries to create your own topic modeling and topic classification models, or using one of the neatly packaged machine learning services we mentioned earlier, we’ve provided step-by-step explanations below. 

First, we’ll go into more detail about building topic classification and topic modeling models using Python and R, then we’ll show you how easy it is to build similar models using MonkeyLearn.

#### Tutorials using open source libraries

**Topic Classification in Python**

In this tutorial, you’ll learn how to train a text classifier using [NLTK](https://www.nltk.org/) and [Scikit-learn](https://scikit-learn.org/).

To get started, download this CSV with sample data. This dataset contains 223 hotel reviews, and it has three columns: review texts, review sentiments (e.g. *Positive*, *Negative*, and *Neutral*), and the review topics (e.g. *Location*, *Comfort*, *Facilities*, *Cleanliness*, etc). You’ll be using this data to train your classifier:

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAIAAAABPYjBAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAyklEQVQI10XIW46DIBQAUJfSiYnWB1QQBe2gra+kUuWioN3/SibzM5Ocr+NFsf943p1bwSgN8zi135LLRnSD5ILQHDGGuSAlJ6zAwfUShP+8OPGHsbVuBVAAs3X6/GybXazTXS/LMquqXNR5fWe8okV5K3nGittvCupFsd/10jn9fk8rvHa7gpnBqFkNOUMZiQlNCE1onrICIxymKEhRgHCIcOilKOiH5jxBw2vfF3vAti/aqOMAY5RsRD8049S2j5rQJIr98Pr15we6jjfGl8PShQAAAABJRU5ErkJggg==)![](/static/b6515787c24142ef54b23f5c0a3ffe79/835d7/2ac13f274249453ebd78c6556c8e3846.png)Next, install the following libraries using pip:


```
pip install nltk
pip install sklearn

```
Then, create a list of list that represents the rows and columns of the CSV file:


```
import csv
reviews = [row for row in csv.reader(open('reviews.csv'))]
print reviews

```
This generates the following output:


```
[
 ['Text', 'Sentiment', 'Topic'],
 ['Room safe did not work.', 'negative', 'Facilities'],
 ['Mattress very comfortable.', 'positive', 'Comfort'],
 ['No bathroom in room', 'negative', 'Facilities'],
 ...
]

```
Now, you’re ready to do some data cleaning. Keep in mind that this is a key step; you cannot build an accurate model with [dirty data](https://www.techopedia.com/definition/1194/dirty-data). With this in mind, we’ve defined a rule to use NLTK to filter out stopwords, remove non-alphabetic characters, and stem each word to its root:


```
import re
import nltk

# We need this dataset in order to use the tokenizer
nltk.download('punkt')
from nltk.tokenize import word_tokenize

# Also download the list of stopwords to filter out
nltk.download('stopwords')
from nltk.corpus import stopwords

from nltk.stem.porter import PorterStemmer
stemmer = PorterStemmer()

def process_text(text):
    # Make all the strings lowercase and remove non alphabetic characters
    text = re.sub('[^A-Za-z]', ' ', text.lower())

    # Tokenize the text; this is, separate every sentence into a list of words
    # Since the text is already split into sentences you don't have to call sent_tokenize
    tokenized_text = word_tokenize(text)

    # Remove the stopwords and stem each word to its root
    clean_text = [
        stemmer.stem(word) for word in tokenized_text
        if word not in stopwords.words('english')
    ]

    # Remember, this final output is a list of words
    return clean_text

```
Next, we use the process text function to process the data:


```
# Remove the first row, since it only has the labels
reviews = reviews[1:]

texts = [row[0] for row in reviews]
topics = [row[2] for row in reviews]

# Process the texts to so they are ready for training
# But transform the list of words back to string format to feed it to sklearn
texts = [" ".join(process_text(text)) for text in texts]

```
This process transformed the reviews into a nice list of stemmed words without any stopwords, that is, words that don’t provide value to a classification model (e.g. *‘the’*, *‘is’*, or *‘and’)*. Now, the hotel reviews looks like this: 


```
['room extrem small practic bed',
 'room safe work',
 'mattress comfort',
 'uncomfort thin mattress plastic cover rustl everi time move',
 'bathroom room',
 ...
]

```
Now that we have cleaned the data, we can go ahead and train our classifier using scikit-learn. First, we need to transform the texts into something a machine learning algorithm can understand. So, we’ll be transforming the [texts into numbers](https://monkeylearn.com/blog/beginners-guide-text-vectorization/):


```
from sklearn.feature_extraction.text import CountVectorizer
matrix = CountVectorizer(max_features=1000)
vectors = matrix.fit_transform(texts).toarray()

```
Next, we’ll partition the data into two different groups: data that will be used for training the model (AKA training data) and data that will be used for understanding how accurate it is (AKA testing data):


```
from sklearn.model_selection import train_test_split
vectors_train, vectors_test, topics_train, topics_test = train_test_split(vectors, topics)

```
Finally, the exciting part: training the machine learning model! We’ll train a [Naive Bayes classifier](https://monkeylearn.com/topic-analysis/#naive-bayes) that will be able to predict the topics of hotel reviews:


```
from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(vectors_train, topics_train)

```
And voilà! You have trained a text classifier with machine learning! But how accurate is this model? Let’s check by using the testing data to obtain performance metrics:


```
# Predict with the testing set
topics_pred = classifier.predict(vectors_test)

# ...and measure the accuracy of the results
from sklearn.metrics import classification_report
print(classification_report(topics_test, topics_pred))

```
This outputs the [precision, recall, and F1-score](https://monkeylearn.com/topic-analysis/#metrics-and-evaluation) for the different categories of the classifier:



| *precision* | *recall* | *f1-score* | *support* |
| --- | --- | --- | --- |
| *Cleanliness* | *0.43* | *0.43* | *0.43* | *7* |
| *Comfort* | *0.52* | *0.57* | *0.54* | *23* |
| *Facilities* | *0.55* | *0.50* | *0.52* | *22* |
| *avg / total* | *0.52* | *0.52* | *0.52* | *52* |

Besides the performance metrics, this output shows the number of training samples used for training each category (also known as *support*). Taking into account the small size of the training dataset, the accuracy of the classifier is pretty good!

Keep in mind that training a machine learning model is an iterative process. From here, you should experiment and tweak the model to get better results. For example, you can add more training data so the algorithm has more information to learn from. Then, you can experiment with the number of maximum features to find the optimal setting for this particular model. You can eventually try another machine learning algorithm (such as [SVM](https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/)) to see if this improves the performance metrics.

This may be a simple classifier, but you have completed all the necessary steps for training a machine learning model from scratch, that is, cleaning and processing data, training a model, and testing its performance.

If you want to keep practicing your skills, you can follow the same step-by-step process with the same dataset to train a classifier for [sentiment analysis](https://monkeylearn.com/sentiment-analysis/). Instead of using topics to tag each review, use sentiment categories to train your model.

**Topic Modeling in Python**

Now, it’s time to build a model for topic modeling! We’ll be using the preprocessed data from the previous tutorial. Our weapon of choice this time around is [Gensim](https://radimrehurek.com/gensim/), a simple library that’s perfect for getting started with topic modeling. 

So, as a first step, let’s install Gensim in our local environment:


```
pip install gensim

```
Now, pick up from halfway through the classifier tutorial where we turned reviews into a list of stemmed words without any connectors:


```
['room extrem small practic bed',
 'room safe work',
 'mattress comfort',
 'uncomfort thin mattress plastic cover rustl everi time move',
 'bathroom room',
 ...
]

```
With this list of words, we can use Gensim to create a dictionary using the *bag of words* model:


```
from gensim import corpora
texts = [process_text(text) for text in texts]
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

```
Next, we can use this dictionary to train an LDA model. We’ll instruct Gensim to find three topics (clusters) in the data:


```
from gensim import models
model = models.ldamodel.LdaModel(corpus, num_topics=3, id2word=dictionary, passes=15)

topics = model.print_topics(num_words=3)
for topic in topics:
    print(topic)

```
And that’s it! We have trained our first model for topic modeling! The code will return some words that represent the three topics:


```
0, '0.034*"room" + 0.021*"bathroom" + 0.013*"night"')
(1, '0.056*"bed" + 0.043*"room" + 0.024*"comfort"')
(2, '0.059*"room" + 0.033*"clean" + 0.023*"shower"')

```
For each topic cluster, we can see how the LDA algorithm surfaces words that look a lot like keywords for our original topics (*Facilities*, *Comfort,* and Cleanliness).

Since this is an example with just a few training samples we can’t really understand the data, but we’ve illustrated the basics of how to do topic modeling using Gensim.

**Topic Modeling in R**

If you want to do topic modeling in R, we suggest checking out the [Tidy Topic Modeling](https://cran.r-project.org/) tutorial for the [topicmodels package](https://cran.r-project.org/web/packages/topicmodels/index.html). It's straightforward to follow, and it explains the basics for doing topic modeling using R.

#### How to Create a Topic Classification Model with MonkeyLearn

If you’re not familiar with the programming languages we mentioned above, or you don’t have the resources to implement these models, fear not. With [MonkeyLearn](https://monkeylearn.com/), building topic classification models is easy, fast, and affordable (in fact, so affordable you can get started for free!). 

To get started, [sign up for free](https://monkeylearn.com/signup/) and follow the steps below to discover how machine learning models can simplify your topic sorting tasks. 

**1. Create a new classifier**

Go to the [dashboard](https://app.monkeylearn.com/main/dashboard/), click on ‘create a model’ and choose your model type, in this case a classifier:

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAABYlAAAWJQFJUiTwAAABzklEQVQ4y21TwW7UMBDdT6Vw48AH8F/dA5z4A0BCQoLS7G6rJmk3GydxbI/HfjBOnE1ULD3ZM/P8Zjy2d4Mh6NFByzxjmG1jfYJ1flmv43k9Wo+XpoPqDXY5OGTM9qXTeD4r1JcBh6pDfVZ4bhR6bRehNV8NJsV2S3BGLxkNoWkH3B+fUDzWuDtVODxWuD+VaDudBHrtNvtyxbthnI5MnhFCAGIAhwBHDA4R68EcQDTzENPsyG+Ek6AsRMgxcFTTZiEGDjDB4VtXLD5mBjPwUJBowjO/rlAcMQZoF/CzppR9EozQ3uL7pUCMSNWJILmA00EjhPg/walUCbatQlWWsNbBe05tUKrDS9VgNCbZcuyu7/FUl8nnObwWlIX0w1i5kKuYzNYJ0cJ7v/ikSuHJHrE3gv1ICOTwtQy4uQXe3AIfv8gFePyoGTd7JP+Hz4BxjAfFeLcH3u6B958AZRjGyClXPZRsnQ343UT8OkccVQSRR28Yd01M/qKd+qodJ1vw5xJhidMrGXKF+R1KFqYJnlwiSSJnXeqpIPsyT6DN9R3OzyYbhH7B5Ou0Q6UcSkWo1MTZ8mg5av45u+vfXOP6pUYztWQ062q2WC7lX4V/AZmy6xcpUoMSAAAAAElFTkSuQmCC)![](/static/8169d032170501bd0382eb263a3b6d9c/cc514/choose-model.png)**2. Select how you want to classify your data**

To create a model that classifies texts based on topic, aspect or relevance, click on topic classification:

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAABYlAAAWJQFJUiTwAAABU0lEQVQoz5WTi26DMAxF+f+fnNqVinchQB52kjvFJZRu6tZFunIS5MN1LBej0li0E62GYBxD24eMZVjHP+6zUp42hMTpxxnFvNodlvZ1N6IdFNphQtNPaPsJp3rCtVMYxhnDtKDbvne3WfJyrloMinTIQLVanMsaZdXiUnUo6x7na4OPS4PTtUVZdfi8tnKXYtXedpcZXORDjj4CKwNEDDUvWFYNxIi0trAv7yOO+UlFJi86RQdLAe0KOEcCW7WBsQ7WOYneB4QYEWMEkX9yJw4fBydQYo/I95jlQ0TYlIC8KTXrJTB/CMzA0IizfhhxGyc450BEcETyFHGrPUHfcuiD30tLrsQpsZSY9v9w6MAUYBVgrMVtUpjUDGaPEIL8JEvekP3fJXsOYAMpLzVFayMNSU1KslI+S8m/Ao9Q7eg+JS8nhGQ6jjlPwOPo5f27+g79Ar4DpbD+RfTqAAAAAElFTkSuQmCC)![](/static/f36fed41a39dc01c3f5b692ca95e7d1d/fee0e/choose-classification.png)**3. Import your training data**

You’ll need to train your topic classification model using relevant data – data that mentions aspects about your product or service. You can upload your data as an Excel or CSV file, or from third-party apps such as Zendesk, Freshdesk, Twitter, Gmail, or RSS feeds: 

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAABK0lEQVQoz42S4W6DMAyEef8XXKVWGz/QtFYbkJA4iRO4yoaslG3SkCLD+fLFOdFYH+EDw1F6WqJRzKDACJG1yvdf3n50EFYz+fRtklpBk48YrcdoCbeRtA7Gq6+C9/sG63VPsxflhN44mInwfv3C6dLi0nY4vXY4v3V4Obe4fo56UIUfh2n2Y0u1LurSd8qwxHA+Qm5CkXUK6Uvdgyqn2cOec0n4MAk3kzAYpxk9exjHYRRYPyT4XApyLlpjykjMKKKVGaXM6kmcwbloFS8dr1yBYpDHJocFK0A2zFhg2WtPPMuyIMSEfrSqSQy/AmWieZ5hJoec1ymYV41CVJB45CBPAXZy2gs/gWseXkT5JbSxZbNlqWY6aumh7a7dKEwAKvIj8MD/1hS8TXgH6+MIyR7vNZkAAAAASUVORK5CYII=)![](/static/acda0e37460c38fe24e23fee942766ca/f50de/import-data.png)**4. Define the tags for your classifier**

You will need to define at least two tags to start training your classifier, and you can add more later on. We recommend using no more than 10 tags to begin training your model, as well as using broad tags for which you have enough examples. Check out this article on [best practices for choosing and defining you tags](https://monkeylearn.com/blog/ticket-tagging-best-practices/):

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAABYlAAAWJQFJUiTwAAABs0lEQVQ4y31TW44TMRCcL04AR+CsSPzBF5cAJKS9BkgIAlmxye5GsySZp8dvt5lC7Xkkw2b5KLmnp7tcXbYzZTy0DRehTIB1BGkjChlhfITzEf/ryYZGfxFSe3xfb/FldYOvv3J8/naN7e43jKMnexJh22kcK4FD2SYcyxb7oknxXX7AZnfAbl9hvX3AXX5EUS9reS0qgU7Zk8KqkamwrDsUIzhXtxJ543HfEHIRUQqTahgsYloboYeRJz8Mw9FFX4z1M7Q9+WfdgKlvHvncA+dDwlTsA8GHOCOMKxFBmIhKUdpk9nBQ6NOJchyIYB2rcdDWIcaIvu8X+NP3AHrsZY9NFWGtm/uziXlSysYypB7AsZBmzJsxNhCpxkBru+jPTndu9DL5QfOOimNLkCYk8L/ze6rMsn8kHFTywdR1AyE6ePaRD0IKWNWhJw9yFko0MNYubHqkcEgOxHIcl8lq5bEpHG4YZcCm9LgtpxFPhI8UqjOV/N3pAESP9ytC9gp4/gZ4+a7Hs9fAi7fAQxvg3Ej45NPjxPn1cR63VcCHFeHTT8LVNeHjD8LVmtDqMCvU/7zrv74R5FlbdvG6AAAAAElFTkSuQmCC)![](/static/56a3ffe68e7c4fc6ac7b1faab425edbf/1fbad/tags.png)**5. Start training your topic classification model**

You’ll need to train your model manually, so that it knows how to label each text. Check out the example below to see how each text is allocated a tag (topic). Once you’ve tagged enough examples for each tag, you’ll notice that your model starts to make predictions on its own:

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAABYlAAAWJQFJUiTwAAABT0lEQVQoz42TyU4DMQyG+5y8CXdegZfgxJ07EnfEgdIWUEWHdtrOktVZfuRkOkPpRkaf7NiZP7aSjKQmnEIZh0YovM1LfBYbSG1xbj3nR0JZ1K1ONMJgXbXYVKKnWG3xOptj8rFAud3PVa2CUATWyFAW5CoYIXVCaQttCdo6KEMw1kEbSiTfumQTlOFYqpB/IMdJ6rAZYxFCwOGI8N4BccjFGOG8HwQ58F941MondmI8fAip7V4w732knt+CXazRDq25IPi3iuP+YLN/qcIToudixwWxa2loOvSV7Ld/eET5c72gJgQXQMYhuojoGcCTB5GHcx7UwbeAyPVzBnzYjEcW3AiJcfmFt3WB6XaJ97pMzOoVylagFQaNNGgTtrMZIS2ux3e4er7FzeQeki92owwK0eBbNMnu/IWoUUmdFvHOp57bYznBw/IFT+tpmv8A6nan9COBSg8AAAAASUVORK5CYII=)![](/static/9e58e2f21aa4dda36053b2e9694fd41f/36876/tag-data.png)Remember that the more training examples you feed your model, the smarter it will be at making accurate predictions.

**6. Test your classifier**

Once you’ve tagged enough examples, click on ‘run’ then ‘demo’ to see how well your model is performing. You can use new data – data that the model hasn’t seen – or you can have a go at writing your own text, to test your topic classifier. In this example below, notice what happens when you write your text and click on ‘classify’:

### Test with your own text

### Results

TagConfidenceStaff92.6%If you think your model needs refining, go back to the ‘build’ tab to continue training your model. MonkeyLearn also provides tools that can help you understand how your model is performing. Go to the ‘stats’ section and you’ll notice statistics such as precision and recall, as well as a [keyword cloud](https://help.monkeylearn.com/en/articles/2173840-using-keywords-to-increase-classifier-performance) with the most common terms and expressions for each tag. You’ll also find ‘false positives’ or ‘false negatives’, in other words, texts that were tagged incorrectly, which you can retag to help your model correct its own errors in the future. 

**7. Integrate the topic classifier**

You’ve made it to the final step! Once you’re happy with the performance of your topic classification model, you’re ready to use it. But how can do you sync the data you want to analyze with your new model. Well, you have three options:

* You could opt for batch processing – [classify text in a batch](https://help.monkeylearn.com/en/articles/2174086-batch-processing-of-text-analysis) – by going to ‘run’ then ‘batch’, and uploading a CSV or Excel file. Your classifier will analyze this data and send you a new file with its predictions.
* [Integrations](https://monkeylearn.com/integrations/) are available with MonkeyLearn, and are used to automatically import new text data into your classifier. You could integrate Google Sheets, Zapier, Rapidminer, and Zendesk, for example, all without having to type a single line of code (see example below)
* If you’re a dab hand at coding, you’ll probably opt for [MonkeyLearn's API](https://monkeylearn.com/api/v3/) to import new data.

![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAABYlAAAWJQFJUiTwAAABzUlEQVQ4y22SaW7bQAyFdf8zFD1K+6tXaBGnQIs6TepNsWYlOcsrSC2xgxAgBpI+cR75OJzHK/48H3BxCSwVmQtiFqSb1OeQGFW6pTLKkvL0xus5UGlofkQ/PSGxIMaE3jta63ZqshSgAj+nF/xyRwgJJu/hfACLbJyUioEB4PAb+P4NjirOlxGlVNwGidj5afcFn3dfAWr4ezhCu6v1jdX/BpGCLAVTIoQQEWLC5IOlqtUfimZpGIPDy3hBStm4GLOp9CHOnBUs1QqdzqPdeBlfMY6vVjATWysKzlzCNHl4KxbtQi1GbH0uLet8LPr8sgKp3HW8FZypmaM653tuyCQotaO2brKzdHhqkFIsFSIWKNeMacZF7vBZvxfo2GThBpaGwgHTdUTKjOvkkFJCbc2ytWYu6wyvPuDqPVImTM7bt7Zx8zYMtQNh/Ifdww9kFhyOZ5vfbahSjcf9Ho/7JxAxDqeLrYyNofd3LpPAhWSOOR8tddiTC8iZDNTbmcWMWZ1VpZnIzrKMZ9CNZ+alWDDHKefZCCnWzmqKzk5VqeK27N+61JvLIvdWcfvA5do2l9fIRbl+33JdXV7UqOxIBS6XD12+5QJp1k39yv0H4Lr3KLtkEiIAAAAASUVORK5CYII=)![](/static/ca090e9a55a1ab58477873a536b0d790/6e847/integrations.png)Wrap-up
-------

Topic modeling and topic classification are both very different approaches to machine learning, and the one you choose will depend on the problem you're trying to solve, as well as the resources available to you. 

In this article, we provided an overview of the algorithms available for different purposes, as well as outlined several tools for both beginners and programming experts. By understanding how topic modeling and topic classification works, you’ll be better equipped to choose an approach that’s suited to you and your business. Whichever technique you choose to implement, you’ll be one step ahead on your journey towards better decision making, whether you work in sales and marketing or customer support and product teams.

Topic analysis is already being used to streamline and improve processes, as well as save time and, ultimately, help businesses grow. To get started on your machine learning journey, why not try out topic classification? 

MonkeyLearn offers ready-to-use topic classification models that are easy to use and implement, without the need to code. And if you are looking to use a custom set of tags or a model trained with your own data, you could also train a custom top classifier in a few simple steps. Sign up to MonkeyLearn for free, [request a demo](https://monkeylearn.com/contact/), and our team will help you get started with topic analysis.