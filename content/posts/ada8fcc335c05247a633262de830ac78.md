---
title: 99 (S4E04). Data Mesh & dbt
date: 2023-10-19
src_link: https://www.notion.so/1-99-S4E04-Data-Mesh-dbt-YouTube-Data-Coffee-a28183571366450a98fc2d763c5e4389
src_date: '2023-10-19 19:14:00'
gold_link: https://www.youtube.com/watch?v=YZhaZCLresQ
gold_link_hash: ada8fcc335c05247a633262de830ac78
tags:
- '#host_www_youtube_com'
---

![](https://www.youtube.com/watch?v=YZhaZCLresQ) 
# Description 
Тема выпуска Data Mesh & dbt

Shownotes:
0:00 Капсульные кофеварки
1:57 Выступление на конференции
3:52 dbt
6:32 Data Mesh
10:23 Data Ownership
17:01 Проблема Data Mesh
20:40 Кто отвечать будет
23:16 Нужны новые данные
30:08 Доклад
39:29 SQL и тибетский монастырь
43:23 Зачем дружить dbt с Airflow
54:32 Как делить модели на DAGs
55:28 Есть ли проблемы с backfill
1:02:09 Альтернативы dbt
1:04:52 Как разобраться с dbt

Сайт: https://datacoffee.link, канал в Telegram: https://t.me/datacoffee, профиль в Twitter: https://twitter.com/_DataCoffee_
Чат подкаста, где можно предложить темы для будущих выпусков, а также обсудить эпизоды: https://t.me/datacoffee_chat
# ru
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=2)~~  Всем привет С вами подкаст дата кофе и Сегодня у нас необычный гость гости зовут Евгений Ермаков и вы возможно про него что-нибудь слышали возможно вы были на его выступлении на смарт-дате но до того как мы зададим основные вопросы хотелось бы узнать Женя Как ты относишься к кофе [музыка] Изменилась ли твое отношение к кофе с прошлого гостевого эпизода где ты был гостем вот я бы сказал что не изменился радикально с одной стороны с другой стороны связи с переездами по миру меня перестало рядом под боком быть кофе-машина которая кофе-машинные зерен и у меня появилась nespress Я приобщился к миру нас пресса и вполне неплохо Мне казалось что это сильно хуже чем оно есть на самом деле самом деле достаточно неплохо И я также достаточно много кофе и стараюсь это она не хуже по вкусу но мне кажется по цене не очень выгодно получается или очень гораздо быстрее чем любая другая кофемашина И вообще любой другой способ приготовления кофе кроме растворения хотел бы сказать что не смотрю на цены таких вещей но нет да Но действительно 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=94)~~  дороже Но с другой стороны на вкус больше то есть если ты покупаешь какой-нибудь объем кофе засыпая что у тебя какой-то стандартный вкус кофе на протяжении вашего периода времени Здесь каждый капсула своя и Расскажи нам Жень О чем О чем ты самосмордату с морда-то это одна из немногих На мой взгляд конференции в рузоне который посвящена именно данным есть highland всем известный и на Хэллоуин сильное смещение в сторону бэкенда Ну понятно причинам изначально высокая нагрузка и доклады про данный хранилище данных платформа данных есть но они обычно не так хорошо заходят в аудиторию есть достаточно но конференции которая посвящены данному с точки зрения анализа мать маркетинг любые другие доцентистки конференции но это не про дата инженерию и вот здесь Smart Data это чуть ли не единственная единственная которую я знаю возможно наши слушатели скажу что есть другие это конференция которую я знаю который посвящена именно дата инженерии и каким-то аспектом вопросом построения я так вышло все четыре года что она существует 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=193)~~  каждый год на ней выступают с разными темами и вот сейчас в этом году сегодня буквально момент когда мы записываем про кофе рассказываешь про кофе не какой Я выступал с Темой про девяти добилдинг я думаю что наши слушатели в курсе что это такое про то как мы втолкаем внедряли девяти в связке была тема моего доклада а что такое дебютит Жень вот я представлю наших слушателей вопрос от меня Что такое dbt ты Алекс можешь не говорить для меня потому что я уже говорил что у меня как будто была раса я даты инженерии изучает нашим подкастом Мне кажется нужно сертификацию не самый быстрый путь на самом деле он как мне кажется он такой думаю Quake надо пробираться возможно он не самый быстрый но самый короткий потому что таких ёмких определений и чётких пониманий как бы через чтение Ну не добиться особенно особенно в моменты когда гости наши рассказывают Чего делать не надо это пожалуй самое ценное как не работает но тем не менее Что такое dbt Особенно для меня это интересно потому что я реально не шарю и никогда dbt не знаю в жизни 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=284)~~  DVD во всех случаях он себя фреймворк который отвечает за букву т-процессов делают все инженеры данных Вот dbt это комментирование за все трансформации с данными которые происходят в нашей при этом что еще лично для меня интересно dbt как компания они пропагандируют такую роль как аналитик с инженер это что-то среднее между датой инженером и дата аналитиком но можно представить 3 столбца слева даты инженер справа дата аналитика посередине аналитик инженер и в концепции которая продвигает dbt дата инженер не должен заниматься этой процессами дата инженер должен заниматься именно развитием платформы данных инженерной составляющей всех этих инструментов не прогрузка данных потому что инженером все-таки далеко от бизнеса далеко от всех этих данных дата аналитик должен заниматься аналитически сложными задачами он должен прогнозировать он должен сказать Инсайт он должен строить модели что-то прям Действительно сложное не аналитик инженер это человек который может и в построении этой процессов и в аналитику тот человек который может 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=374)~~  прийти в бизнес построить какую-то операционную отчетность рассчитать нужные витрины как их визуализировать то что это такая смесь между этими разработчиком и аналитиком и для меня это лично отвлекается в совмещении с подходом меж Да это меж который я пытался в такси в яндекс.гонит который сейчас у нас более-менее успешно внедряется Толкай под это меж мы отрицаем централизованное хранилище данных у нас не должно быть централизованной команды которая занимается только данными которые есть и трейлер разработчики прогружают данные все это не нужно нам нужна команда дата инженеров которые делают платформу данных Тулы по работе с данными и нам нужны доменные команды инженеров которые погружают данные формируют какие-то отчетные срезы кажется что прям совместить ты сейчас произнес речь которая сгенерировала слишком большое количество вопросов в моей голове Мне будет сложно вспомнить то что был сгенерировано в начале поэтому я с конца начну А ты сказал что Date mesh отрицает не совсем централизовано хранить централизованную двх команду то есть не 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=483)~~  всё равно централизованные скорее Да здесь прям явно не прописано это меж насчёт централизации платформу Да должна быть общая При этом если платформа данных состоит из большого количества инструментов по хранению данных то это с точки зрения хранения децентрализованная Ну вот я как раз к этому и хотел подвести то есть очень сложно мне допустим было бы как пользователю сидеть разбираться из какой системы вот этот домен мне надо выковыривать И как потом эти данные друг с другом подружить поэтому было бы гораздо удобнее если у меня был один допустим SQL интерфейс к общему хранилищу где Пусть Окей каждый Пусть за свои данные отвечает самым владеет проверяет и так далее но мне хотелось бы их видеть в одном месте чтобы иметь одну точку доступа ко всем этим корпоративным данным здесь Да это умеешь это больше про организационные административно-технический подход То есть он не отрицает то что у тебя есть единый интерфейс у тебя может быть например одна в своих Green Plan в который данные просто организованы это могут быть или 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=559)~~  отдельные схемы отдельные каталоги отдельные базы данных или как угодно можно это организовать но с точки зрения пользователя доступен единый при этом безусловно правильный вопрос А как сделать таким образом чтобы данные в разных доменных были интерабельны в том плане что ты можешь с ними единым подходом предлагается организовать федеративную Data гаурмус команду Ну то есть это представители с каждого Data домена который каким-то образом в этом федеративном органе голосуют за то как правильно работать с данными Как правильно их называть Как правильно складывать таблички и так далее соответственно если у нас есть такой Федеративный орган имеющий власть над командами доменов данных он может обеспечить интердеберабельность или одинаковость данных во всех доменах Следующий вопрос Евгений скажите пожалуйста каким образом вы заставляете команды которые никогда в жизни не видели никакие даты инструменты не может быть даже не работали с SQL языком Как вы заставляете загружать куда-то там данные куда они вам сказали вам надо загрузить 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=648)~~  а здесь подход следующий по самой концепции меж домены данных делятся на два типа они два типа в исходной статье и три типа в книге если читать Джама где Хани она не суть Пусть это будет два как в исходных статьях есть домены которые приближены к источнику данных к сервису микросервису каком-то консервису и домены которые приближены потребителям к аналитикам к бизнес-пользователям и так далее соответственно в организации скорее всего за домены потребителей будут отвечать аналитики А за доменный источник будет изучать Бэкон команда и в этой развилке если смотреть на аналитиков Я считаю что любой аналитик даже Джонни Руни даже стажер должен уметь писательский запрос Ну то есть если аналитик не умеет запрос то скорее всего это аналитик с которым стоит поработать и научить его писательский запрос если аналитик умеет писать запросы то у него супер простой порог входа в dbt dbt это такой фреймворк котором ты можешь писать просто искрив запрос каким-то образом помечать какой тип материализации нужен инкрементальный пересчитывать и так 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=732)~~  далее но любом случае просто запрос если мы говорим про Back and команду то здесь позиция лично моя следующая что контент-инженер который создает высоко нагруженный сервисы он может легко и быстро разобраться в том как устроены первичный забор данных и первичная обработка данных Там нет ничего сложного там обычно приведение к кому-то плоскому табличному виду конвертация типов что-то приемлемое для особо с которым работаем любой может разобраться прям за неделю взять любого Я думаю я думаю неделю с учетом того что он будет встраивать над этим РМ здесь дату платформы должна предоставить инструменты чтобы было не сложно и вот здесь слайс на тот опыт который сталкеа и мы все покрыли макросами dbt то есть загрузка нового сообщения Это буквально описать про то схему того формата который ты отправляешь Хорошо мой вопрос был как вообще люди будут это делать которые это никогда не делали твой ответ на это то что людям легко разобраться Окей насколько много времени на это уходит у этих людей то есть Может быть разобраться быстро но 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=816)~~  нужно на это сколько-то тратить времени там обслуживать как-то если у вас что-то меняется в ваших микросервисах то меняется выходной файл в какой-то момент меняется структура его или там Новые поля появляются или старые уходят им же это как раз та проблема которую пыталась решить дыхание придумывает если у нас есть централизованный двх команда то Back and меняет типы сообщений меняет набор полей которые внутри и двх команда Она всегда отстает от этого изменения которые надо как-то административно договариваться что вот прежде чем изменить договоритесь с командой двх и возникает очень много таких организационных барьеров предложение которое идет как ядро Да это меж Зачем так делать Давайте отдадим инструменты по работе с данными в БК команду она изменила сообщение персонажей поменяет то как сообщение выглядит аналитических системах звучит супер здраво осталось только сделать так чтобы эти изменения были простыми с точки зрения бэкенда Вот как реализовано это в потолок и достаточно делать обратно совместимые изменения в 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=901)~~  просто схеме то есть только добавлять поля в это просто схему или если мы меняем тип поля чтобы этот тип менялся на более широкий Правильно я понимаю что в такой схеме мы не забираем данные из источников а отсылаем данные из источников хранилище то есть на самом деле перекладываем ответственность за то как именно будет храниться тот или иной объект у нас хранилище [музыка] Я согласен с первым с первой частью утверждения что в любом в таком подходе Back and сам отправляет данные в хранилище У нас есть какой-то стандартный путь он проходит через энхаб ну или через кавку и формат сообщения достаточно строго регламентирован он должен быть описан через этот схему а но вторая часть Что отличает за то как это сообщение будет разложено есть скорее я не согласен потому что сложность раскладки сообщения плоский вид в себя выбирает даст платформу если буквально то Да просто схема которая поставляет вместе с данными это правда схема является основой для того как мы разложим в табличный вид и про эту схему она тоже шлет в кафку 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=995)~~  либо про то схема заранее схема присылает схему она в гитхабе Ну то есть это как часть нашего контракта с бэкэндом так тут возникает дальше вопрос я просто Ну много много вижу проблем которые надо было пройти решить договориться и устаканить внутри компании чтобы все это наладить вопрос Следующий Вот у тебя есть какой-то бэкент там микросервисы которые допустим Давайте что-нибудь простое мороженое в магазине продаем У нас есть там ассортимент мороженого есть клиенты которые покупают соответственно Back and который отвечает за кассовое оборудование отправляет данные о том что купили и кто купил такая схема все отлично они описали свою вот эту протомета схему своих данных прислали в кафку разобралась все загрузилось вы Кстати куда это потом и без якорной модели просто плоские таблички Ага ну то есть там у каждого своя схема или что как это организовано я сейчас поясню Почему я эти вопросы задаю в этой же схеме с ларьком с мороженым Да есть у нас еще поставщики которые допустим привозят мороженое я не знаю какие есть 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=1091)~~  виды мороженое мороженое Сникерс допустим и только его и у них есть какое-то ассортимент этого мороженого То есть в принципе сущность тип мороженого или типа товара она совпадает Но для поставщика который поставляется Сникерс Это только определённые там артикулы а для кассового оборудования его бэкэнда там могут быть может быть всё разнообразие Как понять Можно ли там допустим Взять и простым Join нам с джойнить данные поставщиков с данными по кассе как вот это разруливается при аналитических запросах уже потому что это вроде бы то же самое мороженое но здесь только ограниченный ассортимент в данных вот этого поставщика Это хороший вопрос мне кажется это общая проблема дейта межподхода И на самом деле проблема любого микросервисного подхода который есть например в мире би Канда мы когда делим наш Монолит в котором все понятно на набор микросервисов Ну любом случае получаем определенного вида кашу как микросервис между собой взаимодействуют как например друг другу события Рома то же самое происходит и в Date mesh потому что у нас много 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=1170)~~  маленьких доменов Вот они могут быть не все данные и мне кажется здесь единственное решение это более-менее адекватное описание что лежат в данных То есть у нас становится критически важным наличиком дает каталога или тела которые позволяют тебе найти нужные интересно Хорошо как надо описать тогда данные чтобы аналитика было это легко сделать и легко найти то что он ищет есть хорошие Максима Как правильно это именно максимум Потому что я не видел ни одного описания который бы соответствовало этому определению Но это а вот мне кажется Дина очень верно подметила что вся идея В итоге выкладывается в то что надо снять ответственность с дата команды и переложить на микросервисы продуктовые колонки и как бы потому что проблемы которые в итоге остаются они в принципе похожи на те которые были до все Единственное что мы можем условно Единственное что мы можем условно сказать что мы что-то добились хорошего это то что потенциально количество источников данных там эти или процессов и прочего становится легко горизонтально масштабируемо но это очень 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=1284)~~  громкое заявление на самом деле кто-то должен со стороны этих микросервисов понимать что он делает Какие как он эти данные предоставляет и сопровождать просто в другом месте теперь все верно Это буквально перекладывание ответственности с Центральной двх команды одна команды аналитиков или на команды кэнда Но это явным образом объясняется тем что команда dvhi если он централизованная ей поставляют данные из бэкэнда команды backenda которые не заинтересованы по ставке данных они заинтересованы прежде всего в том чтобы кэнд работал что там аналитики Как там данные будут поставляться неважно это за меня сделать команду двх Это уже неприятно уже не Супер классно для команды двх потому что приходят какие-то административные барьеры преодолевать Но если с другой стороны посмотреть команда аналитиков она понимает Зачем им нужно проанализировать данные Зачем нужно построить отчетлинку сделать если они это делают Не сами отдают команду двх то здесь команда двх Опять оторвана от бизнеса они делают витринки для каких-то аналитиков которые в общем-то супер 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=1357)~~  далеко от них они могут в этом цикле даже не принимать какое-то явное участие и возникает вопрос зачем держать такую двх команду из достаточно высоко стоящих высокую альтерных людей которые в общем-то выгорают они делают работу которую не нужна ни за слева ни справа ни бэкэндером ни аналитиком не вовлечены не туда Давайте раздадим эту работу кэндером и аналитиком тем кто ближе к этим задачам ближе к этим вопросам от инженеры будут заниматься именно тем за что им платят построение платформы данных аналитикам нужны какие-то данные Да вот у них есть какая-то задача что-нибудь посчитать какую-нибудь модельку сделать и они даже знают У каких бэкендеров можно Ну то есть у каких команд бэкенда можно эти данные получить они разобрались вопросе все хорошо они приходят А те говорят Ну у нас сейчас не сезон вернее наоборот сезон мы держим про то обеими руками и принесем вам данные через два с половиной месяца а аналитикам нужно посчитать сегодня и вот как решается этот вопрос мне просто очень интересно понятно что это не вопрос дата инженеров в данном случае 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=1441)~~  это вопрос именно взаимодействия но тем не менее команд бэк-эндом много и каждый из них Поддерживает у них у всех свои Pay планы свои загрузки у кого-то там половина народу разбежалась или ушло в отпуск на самом деле это классный вопрос его надо рассматривать немножко на уровне выше Зачем аналитики делают эту отчетность скорее всего они делают ее для того чтобы продукт менеджер или владелец продукты неважно в компании называется мог понять как правильно развивать этот продукт и здесь если возникает такая ситуация что аналитик с аналитиков требует нового отчётности аналитику а бкнд команда не может предоставить здесь на самом деле правая рука не понимает что с левой рукой с левой ногой продукт который управляет команды и управляет продуктом заказал аналитиков какую-то даже борт какой-то Запрос который не может найти ответ аналитики пришли его же команде продукта его же команда продукта не может этим аналитикам предоставить Данные есть просто на уровне кампании полный разбор если аналитики эту отчетность делают для 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=1525)~~  там силы вола на Супер высоком уровне то кажется что команда бэкенда должна отложить свои текущие запросы и помочь аналитикам сделать какую-то выгрузку для селевела в общем то что ты описываешь то что описываешь это полный раздрай в операционных и бизнес-процессах внутри компании или аналитики делают что-то не то они пытаются предоставить отчетность в том месте в котором она не нужна или продукт не управляет команды и не может направить команду бэкенда на помощь аналитик или все хорошо но просто нет достаточного количества людей там вот в этом бэкенте который сейчас вот прям броситься что-то будет исправлять Я очень прекрасно понимаю вопрос Дины у нас там платежи Извините стоят Да и мы не можем день мы сейчас деньги перестанем получать А вы говорите что это мне отчетность для силы вола Мне кажется Какая бы отчетность для сил не нужен не была бы нужна всё равно в первую очередь некоторые бэк-н-сервисы будут обрабатывать свои бэклоги и исправлять свои ошибки А завтра у нас контракт с платежным провайдером заканчивается мы на другого переходим А 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=1604)~~  послезавтра у нас ещё что-нибудь это бесконечно может быть история данную сигнал о том что не хватает бендеров Ну в смысле если у тебя бы кэндер всегда пожаре abcander не должен грузить и данные вам выгружать микросервисы должен делать это как раз Ключевой вопрос который в мицелуйде это меж поднимается это неправильно должен отвечать за те данные которые его сервис делает это то же самое что не знаю я не отвечаю за бардак на моём рабочем столе Пусть уборщица приходит каждый день убирает Бардак на столе так можно жить в принципе так можно работать но за мой рабочий стол отвечаю я и здесь то же самое но почему-то в мире данных сложилась такая ситуация что есть рабочий стол есть бэкендер который на нем сидит но за порядок с данными и с вещами на рабочем столе часть команды Это потому что такое разграничение ответственности бэкент Может быть там даже достаточно людей но в какие-то Высокий сезон ну там распродажи еще что-нибудь там маркетинга компания пришла и привалило кучу народу и там надо отмасштабировать срочно сервис 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=1690)~~  в хорошей компании работа всегда больше чем людей Ну не бывает так что люди сидят скучают думают Какую бы нам еще работу поделать если это происходит так значит компания не развивается значит Эти люди не нужны Но то же самое можно говорить анализ данных очевидно на втором месте после того чтобы мы наконец справились но не поставка Да можно тоже самое говорить про тесты то есть я бы кэнтер я пишу микро сервиса Пусть тест пишут какие-нибудь люди тесты буквально Ну некоторые пишут разные компании по-разному Но есть тесты которые Да там отдельно тестируют но есть стандартная практика когда ты пишешь тест-драйв девелоп например когда ты пишешь тесты прежде чем что-то разработать есть хорошие практики backender сам пишет тесты но есть другой вариант допустим теста за меня пишут другой человек Пусть документацию за меня пишет другой человек например мы нанимаем техписов и кот Еще желательно gpt пишет за меня кота Зачем нужен То есть кажется что backender и команда продуктовая она должна отвечать за весь цикл вокруг и аналитика она должна быть 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=1772)~~  неотъемлемой частью развития продукта продуктового менеджер должен понимать что вот есть команда бэкенда и этой команды надо отправлять какие-то сообщения систему анализа данных для аналитиков и аналитики продукты должны этому уже продать менеджеру приносить как развивать твой продукт какие-то цифры какие-то метрики это все должно быть неразрывным циклом должно быть отдельными командами которые конкурируют между собой это вот именно то что пропагандирует в моем понимании Слушай звучит очень классно Но действительно Пора наверное вернуться к теме доклада твоего ради бити про airflow как это все устроено и что вы сложно ответить на этот вопрос фактически этот вопрос про мой доклад 40-минутный пытаясь ответить кратко мы взяли dbt не стали покупать dbt Cloud потому что это достаточно дорого взяли dbt и с интегрировали его с airflow у нас уже был airflow были на нем нам хотелось удобство Комфорт и ту функциональность Которая несет dbt Соединить с airflow душа фактически стандарт де фактов код ривендел платформах И если мы берем любой другой инструмент 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=1864)~~  который есть в modernet таки Скорее а можно я назад откачиваюсь я извиняюсь что я снимаю нас немного с темы твоего доклада но как бы идею дата меша условно Будем считать что ты продал менеджером который нас слушают вот идею использования dbt Пока еще нет Хорошо dbt Может быть как буковка Т Что такое буковка Т для меня в итиль или LT это возможность трансформировать каким-то образом данные их куда-то загрузить в другие объекты в той же базе данных у меня для этого есть SQL скажи о тебе я все равно пока не понимаю зачем мне нужен dbt я могу с помощью Skillet трансформировать данные преобразовать типы разложить там Джейсон на плоский вид и всё это проще сделать зачем мне это прекрасно У тебя есть SQL которым ты можешь пользоваться и делать всё что угодно с данными Но у тебя в любом случае возникают такие банальные но достаточно острые вопросы а как мне просто стоит Lineage что чего мне зависит что зачем запускать Или например как протестировать мои данные Могу ли я каким-то образом контролировать дейт-колете вот dbt 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=1948)~~  он с одной стороны позволяет им писать на SQL и с версии 1.4 по моему на питоне кстати немножко отклоняясь от твоего вопроса питающий модели сделаны крайне сыра и до сих пор они не могут их улучшить кажется не просто ими никто не пользуется кроме нас но возвращаясь к сквелю мы просто пишем запросы то как будет происходить миграция как данные будут вставляться как они зависят друг от друга как мы Тестируем эти данные все в себя скрывает dbt как такая платформа для описания и а вот то как они будут тестироваться и как они друг с другом дружат и прочее это не SQL это скилл но в девяти вместо не то что место Скелет как бы как расширение Skill используется Джинджер шаблонизация и это на самом деле и плюсы минус Минус том плане что достаточно сложно тестировать описывать плюс в том что ты можешь использовать определенные возможности джинсы как-то автоматизировать запросы вместо конструкции From табличка в дебютита указываешь From и макрос реф референс на какую-то другую модель Благодаря использованию таких макросов dbt с 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=2035)~~  простраивает Граф зависимости твоих моделей или объектов между собой вообще 9 модели такой кубик девяти минимальный который состоит pip-файл и яму быстрее в пай файле мы описываем логику Каким образом этот объект простраивается Это просто Селект запрос или dataframe в pays-парке которому возвращаем в ямле мы описываем какую-то мета-информацию нашего объекта описание таблички описание полей и какие-то ожидания от состава полей ожидание это тесты например мы ожидаем что поле уникальное или мы ожидаем что поле не содержит налы или мы ожидаем что поле число в таком диапазоне есть очень много расширений в dbt всевозможных тестов Но в любом случае это выглядит как Ожидание и описав таких кирпичики с помощью команды dbt Build или dbt campile мы расстраиваем dbt проект и формируем уже запросы после Джинджер шаблонизации которые полетят в нашу целевую СУБД и здесь большое преимущество dbt На мой взгляд в том что все сложности вставки все сложности datal Lineage все сложности тестирования данных он достаточно легко скрывает внутри себя 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=2127)~~  настолько легко что и команда аналитиков которая может быть не погружена в дата инженерию и команда бэкэнда которая вообще далека от этого всего обе эти команды могут легко понять как в девяти опять громкое заявление прям можно на стенку вешать Женя сказать проблему девяти тебе вот ты пытаешься продать А я пытаюсь уговорить наших менеджеров все тех же которые слушают не покупайте не использовать DVD покупать может вообще не стоит с другой стороны но вот хотя бы не использовать а во-первых аналитики аналитики мы предполагаем что они даже упомянул Это явно что они строят наши и телепроцессы там и прочее Я так понимаю у вас они используют dbt и строят модели в 9 а так вот если вчера аналитик мог просто взять свои знания по SQL и пойти писать то что ему нужно создавать новые объекты в базе и прочее то сегодня он этого сделать не может ему какую-то еще джинджу магическую на Изучить и еще и какой-то dbt и понять как там это всё связано то есть ну во-первых я считаю что это большой минус я буду таким противником идеи dbt второй момент на то 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=2213)~~  что ты сказал что хочется возразить тесты на данные - это очень круто но проблема девяти в том что все тесты в dbt работают на данные которые ты уже загрузил ты загрузил данные в своё хоронилище и потом запускаешь тесты и говоришь Ой а мы ждали что в этой колонке не будет налов они у нас есть а дальше чего Но мы уже их загрузили а dbt не может нам показать как делать эти тесты до того как мы их загрузили куда-то потому что отвечу на эти вопросы Давай по первой части если аналитик не готов взять Изучить dbt и джинджу которая на самом деле простая Ну несложная И даже если ты не хочешь углубляться в джинджу тебе достаточно понять ее принцип и использовать самые основные её Ну тимплейты это первый вопрос Если аналитик если аналитики не хотят развиваться то тут вопрос в том Зачем нам такие аналитики которые не хотят погружаться это первое завтра скажете гол изучили раз аналитику потому что это нужно для нашего проекта Мы же хороших хранилище строим модные новые с горящими глазами вот и всё аналитики с горящими глазами у них 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=2294)~~  другая проблема они будут сидеть и изучать эти голы и раз вместо того чтобы нормально данные загружать и хранились мы говорим о том чтобы узнать что такое Джу и показать в Новую и до ешку на самом деле это не изучить новый подход к работе Ты всего лишь два небольших инструмента с каждым из которых можно познакомиться Ну за час Давайте только второй что вот по поводу джинджи час и аналитиков вот у меня не укладывается Прямая линия vrflow пишут естественно тоже с джинджей Естественно они будут рады если имена будет писать таки а надо будет просто sqlge мне кажется мне кажется тебе надо уволить своих аналитиков это напишем на стеночки я вам больше скажу есть просто проекты на которых аналитики ещё и SQL не знают только Excel они точно аналитики они прекрасны дают бизнесу и сил левелам по данным которые они могут получить с помощью Excel мы говорим аналитики Мне кажется Любой любой здравомыслящий человек за две недели просто можно взять любого человека если за две недели я лично ему не смогу в долбить Что такое SQL Я не знаю уйду из 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=2389)~~  толоки пойду куда-нибудь в тибетский монастырь и буду там 10 лет думать что касается второй части твоего вопроса Это не проблема буквы т это проблема буквы Л то есть вот то что вам приехали данные которые не удовлетворяют контракту который вы ну который вы согласовали значит в первой части проблема не в части dbt погоди возникает вопрос мы говорим что тесты в dbt это круто они позволяют много проблем с данными найти или удобно эти проблемы изыскивать выискивать если мы говорим о том что данные уже хорошие и удовлетворяют контракту и такими пришли зачем нам тогда нужны тесты но нам нужно это все-таки плюс или нет на то что две таблицы между собой нормально ладят что у них нет разъезда данных каких-нибудь что возможно из одного источника равна второй Циферки из другого источника но Мы это можем узнать с помощью D5 только тогда когда мы эти данные уже загрузили Конечно а эти данные вот в момент когда ты грузишь подразумевает загрузку и проверку что с данными Все ок она не подразумевает что сверяем данные из разных источников 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=2491)~~  например Мы смотрим где у нас колоночки поехали Все ли у нас загружено возможно как всего количество сегодняшних строк относится к количеству по поводу тестов могу сказать как мы делали в такси у нас были тесты трёх видов Первые тесты которые сравнивают что мы все получили с источника по количеству по составу то есть мы идентичны тому что есть в источнике второе тест что у нас вход на хранилище дано совпадает с выходом например было там миллион поездок на выходе тоже миллион поездок и третий вид тестов что у нас выхода между собой совпадают То есть например два отчета которые делают срезы похожим количеством поездок или там количество денег тоже должны совпадать соответственно внутри хранилище почти нет тестов мы проверяем что вход это то что мы получили вход совпадает с выходом выхода совпадает между собой три набор тестов OK Ладно двигаемся дальше Убедили пока Мы думали ты будешь рассказывать что у dbt есть какая-то глубинная проблема там он плохо работает держит не держит нагрузку и путает модели Не я пока пытаюсь понять зачем 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=2574)~~  использовать инструмент если он не решает существующие проблемы и вот Какие проблемы Он решает Женя говорит он добавляет тесты на тесты не показывает что основные проблемы у тебя решит HR хороший компании уволив меня я согласен Вот это решение может быть решит много проблем по крайней мере для меня Окей D5 добавляет дополнительный уровень можно сказать абстракции обогащает Plane sequal чем-то дополнительным джиндже и там еще чем-то все хорошо Зачем нам его интегрировать теперь ты сейчас сказал что в dbt есть модели Мы можем с помощью ref макросов объединять модели друг с другом dbt сам для нас построит Граф построит по сути Даг который нам надо выполнять зачем здесь airflow в плане интеграции ну максимум я могу понять что можно запустить dbt насколько глубже нужно играть тот вопрос который мы столкнулись где-то год назад когда внедряли dbt у нас не то что нет денег У нас нет желания платить за девяти Клауд который запускает dbt вот у нас есть и описанные девяти модели Женя Если бы вы все не покупали к сожалению там то есть мы платим да от 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=2668)~~  инженерам чтобы они скидывались на инструменты к сожалению к сожалению Там слишком большой разброс цен между этими причем они постоянно повышают цены вернемся к слову вот у нас есть у нас есть dbt в dbt можно запускать модель через тип тиран У нас есть airflow который тоже умеет SQL с джинджей умеет запускать пока что то же самое Давайте совместим airflow И dbt причем эта идея с одной стороны поверхности и мы ее подсмотрели в астрономии астрономер это менеджер Flow Если не ошибаюсь они предлагают следующего следующий интеграцию в момент компиляции dbt выплевывают манифест json гигантский Джейсон файлов в котором есть зависимость между разными моделями фактически это есть тот Граф который мы хотим получить VR Flow Давайте в airflow возьмем этот файл и на базе него генерируем наши тогда погоди а можно а можно я на этом моменте уточню А ты рассказываешь как делать интеграцию dbt Air Flow А мне хочется понять А зачем делать Зачем деградацию dbt сам строит весь Дак зависимости знает что надо запустить после чего и все что со стороны airflow 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=2765)~~  по идее нам надо сделать запустить девяти ран всего проекта ответ на этот вопрос достаточно несложный с моей точки зрения airflow позволяет интегрироваться почти С чем угодно если взять любой инструмент из модов стека Монт Карл Дэвид деток что угодно скорее всего будет коннектор Flow потому каждый квартал уходит выходит новый убийцы раз в квартала не выходит и раз следующий квартал не умирают Air Flow жив до сих пор Теперь давайте возможно воспользуемся возможностями по интеграции С чем угодно только засунуть на dbt Чтобы понимать Какая модель когда сформировалась Какой тест упал как перезапустить Как забыть то есть мы используем все возможности но Окей То есть если я правильно понимаю идею вместо того чтобы отдать все на управление dbt самому То что создано внутри нашего dbt проекта а дебите знает то что он генерирует в этих манифестах и прочих файлов вместо этого мы все это Разбираем по полочкам и управляем каждый отдельной модели со стороны airflow а это нам нужно для того чтобы удобно это все видеть VI и при необходимости 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=2864)~~  перезапускать с какого-то момента не там обращаясь к себе зная немножечко дебютил возникает Следующий вопрос Если в 9 что-то падает то dbt хранит стейт последнего запуска можно его куда-нибудь сохранять и попросить 9 запускать собственно поломанные модели после того как мы что-то сделали что-то исправили и это может опять сам dbt делать то есть вся идея пока в том чтобы видеть Все не только T не только точнее и часть от LT и видеть все модели отдельно и строить Lineage правильно я понял мы все возможности девяти по перезапуску моделей можем также интегрировать в airflow или переиспользоваться для в этом плане мы мозг или компиляцию дагов графов оставляем dbt но ua и непосредственно сам запуск по крону переводим в airflow между ними такой симбиоз возникает и безусловно здесь есть минусы в том плане что мы создаем дополнительную интеграцию или дополнительную библиотеку которую я очень надеюсь мы запомсорсинг концу года С одной стороны с другой стороны мы используем преимущества Air Flow с точки зрения интеграции удобную UI и 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=2956)~~  возможности дописывать свои которые не сиквел ни в dbt и мы используем и возможности dbt Окей перед тем как я начну опять какой-нибудь гадский вопрос задавать Расскажи пожалуйста что вы сделали для интеграции Вот ты начал со страномера у астрономером Мне кажется есть свои какие-то операторы для airflow которые они написали и я видел Open Source на уже пару-тройку штук которые по сути тоже разбор манифестов делают и строят так внутри Air Flow Зачем свой нужен на тот момент когда мы изучали этот вопрос Это примерно год назад было кроме астрономера и достаточно такого Ну не на коленочного Ну и мне пишутного кода ничего не было ничего не не могли найти подходящего инструмента для нас Я бы так сказал мы не могли ничего найти кроме вот этой интеграции которая описана астрономии и сам dbt ссылается в общем-то на эту статью номер Как правильно интегрировать дебютировал мы пошли чуть дальше чем сам астрономер мы не генерируем один большой Дак который прогружает вес 9 у нас с одной стороны есть Data межподход который делят все наши данные 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=3040)~~  домены а с другой стороны каждый так запускается своей частотой в общем-то каждая модель может считаться со своей какой-то частотой и у нас это де карту произведения между всеми доменами и всеми возможными типаминга У нас они регламентированы 15 минутки часовые дневные недельные и месячные по месячные никто не использует карта произведения между доменами и вот этим шарингом есть наша они стартуют с разной частотой и они звучит хорошо А где вы храните dbt вообще проекты 9 модели это все вместе с Air Flow лежит или где-то сбоку это лежит в гитхабе такой один единый dbt проект это отдельный вопрос для исследования был когда мы пытались внедрять dbt год назад есть разные подходы приносятся с 9 проекта можно делать один централизованный это очень то что рекомендуется там dbt есть другие варианты Можно например делать послойно срыв данный отдельный витрины отдельно можно делать по микросервисно и Казалось бы это идеальный подход для нас можно делать смешано но были проблемы и они на самом деле до сих пор не решены если мы 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=3125)~~  делаем микросервисный подход например каждый домен это отдельный проект то до версии 1.6 мы обязаны были называть по-разному модели в разных доменах То есть у нас если один домен использует другой домен или один девятипроект импортирует другой dbt-проект название модели не должны пересекаться внутри этих моделей что крайне сложно на самом деле сделать если у нас разрозненная организация раз Родины github проекты сквозное тестирование празднование проектам это сложно на самом деле проблема больше не в этом проблема с именованием починили в версии 1.6 Она вышла делить три назад и у нас должно было быть это в новостном выпуске Проблема была в том что 9 проекта не могут импортировать друг друга циклично То есть я вот домен Я хочу импортировать из домена два информацию домен 2 хочет изменить импортировать Какая циклическая зависимость она рушит dbt и по-моему до сих пор это нерешенная проблема мы можем не использовать рефы как рекомендуют dbt использовать сарсы то есть не простраивать полный Lineage но мы теряем прям большую часть того что 9 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=3204)~~  предоставляет Lineage все вокруг В итоге у нас один монолитный проект он лежит гитхабе разные доменные команды живут в этой монорепе просто складывают в отдельные папочки и соответственно наш Сея процесс когда мы релизим собираем dbt собираем манифест Джейсон и все файлы которые лежат в таргете и скармливал в это наши библиотеки которые генерируют в Лоу она просто читает Сейчас самое интересное самое интересное началось Окей То есть у вас есть отдельный монолитный проект dbt потому что под обменом делить не очень удобно там есть свои проблемы хорошо этот девяти проект лежит в льной Репе от Air Flow правильно Да все верно сам по себе живет dbt сам по себе живет вот вы сгенерировали манифест сгенерировали манифест отлично с помощью утилитки там библиотеки разобрали его построили VR Flow ducky ничего не построили еще непонятно как делить Ну вот мне лично Пока непонятно как делить конкретные модели на с помощью наименования конкретная модель делится на через не теги но через 9-шную организацию проекта в девяти проекте модель лежат в папке Models мы 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=3296)~~  внутри добавили иерархию сперва домен данных потом слой данных потом уже непосредственно модель и это на этапе сборки или компиляции 9 проекта прорастает манифест Джейсон где конкретно находится модель понимая В каком домене эта модель находится дальше в зависимости от того В каком домене эта мы его запустили Даже у нас модельки отработали представим все идеальные ситуации они отработали хорошо и все зелененькое отличные данные на месте пользователи счастливы завтра кто-то пошел и добавил или изменил какую-то модель которая у нас была в 9 проекте и манифест сгенерировался другой Это означает что Дак по этому же манифест у будет сгенерирован другой с другим набором тасок VR Flow А что с этим делать Вчера были вчера были пять тасок в запуске А сегодня стало три а как мне запустить вчерашнюю таску а я ее не вижу сегодня Flow он достаточно неплохо вырабатывает такие изменения То есть если у нас появились новые таски с момента когда они появились они будут запускаться если тебе нужно запустить их за вчера позавчера есть backfill который 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=3385)~~  присутствует можно запустить без проблем за бэкфирить данные по этому тоску которого раньше не было сейчас появился Да но проблема в том что модель ушла сейчас ее нет в манифесте и airflow не видит не генерирует эту таску в этом даге Каким образом я ее перезапущу за вчера если ты удалил модель то безусловно ее нет Но её и тогда не надо перезапускать если ты модель перенес ее надо перезапустить за вчера я удалил сегодня потому что данные вчера криво легли Я хочу вчерашнюю модель перезапустить у нас есть такая возможность мы не удаляем модели мы её соответственно Тогда ты можешь перезапускать за тот диапазон Дат когда она у тебя работала А когда она перестала работать Ты ее хочешь выключить но оставить мне это описание оставить код и так далее Она за эти даты уже не будут работать причём делают не сложно через brunchperators группе правило правило хорошего тона не удалять модели Если ты хочешь их перезапустить за вчера если ты удалил модель и в принципе не существует То есть если удалил модель Я бы попутно надо удалить 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=3468)~~  Окей модель осталась но модель теперь строит Дан строит трансформирует данные да выполняет свою те функции по-другому Вчера мы преобразовывали вот это поле к числовому типу А с сегодняшнего дня мы Начали его загружать в текстовое поле Я хочу за вчера перезапустить а состояние этой модели сегодня уже не такое Да здесь D5 прям сейчас предлагает версия нервы не моделей но мы это не используем с версии 1.5 появились если говорить про то решение которое у нас dbt попробует преобразовать поле из числа в текст в момент когда мы преобразовали число в тексте поле так это более общее конструкция то россияна отработает и у нас все полями плохой пример был Жень давай я другой лучше чтобы на нем долго не заострять внимание модель изменилась таким образом что поле у тебя осталось То же самое выходное поле в таблице как был числовой так и осталось но то как мы собираем это поменялось Select который у нас в модели в девяти пробит он изменился Почему почему я всё это веду А почему но поменялось бизнес-логика Но если она поменялась за сегодня хорошо бы её 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=3553)~~  перезаписать за все оставить За все предыдущие А вчера она правильно работала А как это вчера правильно Ну что за ситуация вот если немножечко поконкретнее Ну допустим но она может быть например Покажи Допустим мы там до января считали одним образом после января другим образом так получается что нам надо использовать тот же самый бренч оператор и версионирование мы прямо сейчас мы делаем следующим образом если мы до января считали одним образом с января другим образом и ладно мы поменяли расчет пошел Единственное что если мы забыли фильм за период До января то мы пересчитаем это по-новому что не должно быть другой вариант Мы можем с помощью джинджа макросов прям в селекте в девяти написать что если дата меньше длиной числа То считай так если больше то считай иначе Честно говоря мы это не требуется аналитиков Но на самом деле Вот лично для меня вот услышав то что ты сейчас сказал про то что мы в джиндже это можем за в зависимости от того какой logicle Date VR Flow до грани сейчас у нас выполняется и мы с помощью джинджа можем 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=3629)~~  как раз это вот проставить вот это для меня хорошая Как бы такая плюшка которую можно от dbt получить если её внедрять на проекте то есть я могу с помощью этого регулировать Каким образом я э данные эти буду загружать и соответственно бэкфилить и всё остальное Единственное что мне не нравится в А да я забыл сказать продал продал ещё одной части наших слушателей дебюти теперь тоже Единственное что мне не нравится то что всё в файлах но мне это не нравится И в airflow тоже вот чтобы сделать модель Мне надо написать мне надо создать файл не знаю быть с этим что-то отлично Вот и поговорили А какие есть альтернативы Просто любопытно Вот это хороший вопрос год назад когда мы изучали dbt Я не видел альтернатив на самом деле есть альтернатива внутри Яндекса платформа dmp про которую не когда-то рассказывал на базе котором мы там делали хана хама и так далее вот та димпишная платформа которая есть в Яндексе она супер похожа на dbt но Разница в том что dmp платформу развивают человек 30 А дебите развивают человек 100-150 как минимум не считая всех остальных 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=3711)~~  поэтому скорости разные из-за этого dbt круто Я думаю надо брать 9 смысле не тебе клад деньги оставим на nespress А если я не знакома с деньга платформы можешь рассказать в двух словах что это такое можно посмотреть доклады Владимира верстого в разных местах На той же Смарт дате которая онлайн была сегодня если бы мы пошли да нам нужна машина времени чтобы выяснить если кратко tmp это та же тот же dbt но вместо ямников используется py-файлы то есть вместо Ямал описание документации по какому-то конкретному таблички или объекту используется py-файлы с наследниками вот Green pumptable с описанием полей при этом концепция то же самое ты описываешь табличку описываешь лоудер пай как-то табличка формируется вся остальная сложность как данные вставляются как не зависит друг от друга как они шевелятся они все уходят внутрь фреймворка димпе для меня это супер похожие вещи поэтому Когда мы переходили таксишные команды из такси в толоку достаточно легко прошел переход потому что девяти очень похоже на то что мы делали до этого в димпе 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=3811)~~  а почему не сделал то же самое в толоки же [музыка] dbt все-таки лучше оказался проще или хотелось новый инструмент пощупать от толок это толок Это тот же тот же стартап то есть толок не обладает всеми мощностями большого Яндекса для создания своих собственных продуктов так-то Если бы у нас были тысячи 2000 разработчиков Почему не свой эти написать Или там не свой Да там Брикс написать когда у тебя это достаточно сложный вопрос в мире происходит разные поэтому нельзя использовать инфраструктуру Яндекса надо использовать инфраструктуру который вообще доступна в мире поэтому у нас нет тех инструментов которые не залпом сорсили кстати пить например недавно сортили но позже чем мы переезжали на общем инфраструкту ру Поэтому вот как-то так слушай очень интересно на самом деле все не знаю как Алекса мне продал dbt Я продал продал последний вопрос который один из ведущих подкаста дата кофе любит задавать в конце эпизодов вопрос заключается что если я один слушатель и нашего подкаста И надумал вот разобраться в dbt и как его 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=3908)~~  интегрировать с airflow И вообще вот в это все вникнуть что надо сделать что на самом деле можно послушать наш подкаст и если этого не хватило то документация dbt на самом деле супер классная документация dbt одна из наверное немногих документации которую я мог рекомендовать как italon документации там описано много Оно актуально оно объясняет то что нужно есть туториалы прямо документация dbt почти мой идеал помимо документации 9 можно посмотреть доклад на смарта типа по поводу интеграции BTR Flow который рассказывал рукоделитель от платформы долг Евгений Ермаков зовут очень хороший доклад сам его делал рассказывал Всем советую можно в юзер группы dbt на самом деле русскоязычной достаточно большая Там можно обратиться за советом поспрашивать почитать статьи на хамбре и на самом деле медиум иностранные источники очень много компаний используют dbt их внедряют Поэтому информация по 9 очень много Отлично Вы знаете что делать ребята ну а мы на сегодня закругляемся наверное Спасибо большое Я надеюсь они очень каверзные вопросы 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=4005)~~  Всем пока и услышимся через неделю пока 
# en
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=2)~~  Hello everyone, the podcast is here, coffee date and Today we have an unusual guest called Evgeniy Ermakov and you may have heard something about him, perhaps you were at his speech on a smart date, but before we ask the main questions, I would like to know Zhenya How are you  your attitude towards coffee [music] Has your attitude towards coffee changed since the last guest episode where you were a guest, I would say that it has not changed radically on the one hand, on the other hand, due to moving around the world, I no longer have a coffee machine nearby that makes coffee  -machine grains and I got nespress I joined the world of us press and it’s quite good. It seemed to me that it was much worse than it is, in fact it’s actually quite good. And I also drink a lot of coffee and try to no worse  according to taste, but it seems to me that for the price it’s not very profitable or very much faster than any other coffee machine And in general any other method of making coffee other than dissolving I would like to say that I don’t look at the prices of 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=91)~~  such things but no yes But it’s really more expensive But on the other hand it tastes more, that is, if you buy any amount of coffee, falling asleep that you have some standard taste of coffee for your period of time. Here, each capsule is different and the Tell us Zhenya. What did you snotty [ __ ]?  one of the few, in my opinion, conferences in Ruzon that are dedicated specifically to data, there is a highland that is well known to everyone, and on Halloween there is a strong bias towards the backend. Well, the reasons are clear, initially there is a high load and there are reports about this data warehouse, the data platform is there, but they usually do not reach the audience so well. but they usually do not reach the audience so well. enough, but conferences that are dedicated to this from the point of view of analysis, mother marketing, any other assistant professors of the conference, but this is not about data engineering, and here Smart Data is almost the only one that I know, perhaps our listeners will say that there are others, this is a conference 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=178)~~  that I know that is dedicated to  precisely the date of engineering and some aspect of the issue of building a it turned out that for all four years that it exists every year they present different topics on it, and now this year today is literally the moment when we write about coffee, you talk about coffee not  what I spoke with the topic about nine pre-building, I think that our listeners know what it is about how we push implemented nine in conjunction was the topic of my report and what is Zhen’s debut, so I will introduce our listeners a question from me What  so dbt you Alex don’t have to say for me because I already said that it was as if I had a race I’m studying the dates of engineering with our podcast I think I need certification not the fastest way in fact it seems to me he’s like that I think Quake needs to get through maybe he  not the fastest, but the shortest because such succinct definitions and clear understandings seem to be through reading. Well, it’s impossible to achieve, especially especially in moments when our guests 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=269)~~  tell us what not to do, this is perhaps the most valuable thing, how it doesn’t work, but nevertheless, What is dbt Especially for me  this is interesting because I really don’t know and I never know dbt in my life DVDs in all cases it positions itself as a framework that is responsible for the letter of t-processes all data engineers do. Here dbt is a commenting on all the transformations with data that occur  in ours, what else is interesting to me personally what else is interesting to me personally dbt as a company they promote such a role as an analyst with an engineer, this is something between a date engineer and a data analyst, but you can imagine 3 columns on the left, dates engineer on the right, date analyst in the middle, analyst engineer and in the concept which  promotes dbt data engineer should not be involved in this process date engineer should be engaged in the development of the data platform the engaged in the development of the data platform the engineering component of all these tools not loading data 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=348)~~  because the engineer is still far from business far from all this data data analyst should deal with analytically complex tasks he should predict he  I must say Insight, he must build models of something really really complex, not an analyst, an engineer, this is a person who can both build these processes and analyze the person who can come into business, build some kind of operational reporting, calculate the necessary showcases, how to visualize them, what is it  such a mixture between these developers and analysts, and for me personally this is distracting in combination with the approach between Yes, this is the interaction that I tried in a taxi in Yandex.gonit, which is now being more or less successfully implemented in our country. Push under this inter we deny centralized data storage in our country  there should not be a centralized team that deals only with the data that exists and the trailer developers load the data, all this is not needed, we need a team of data engineers who make the Tula data platform for working with 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=421)~~  data, and we need domain teams, engineers who load the data and generate some kind of reporting  the slices seem like combining them directly is a you just made a speech that generated too many questions in my head. It will be difficult for me to remember what was generated at the beginning, so I’ll start from the end. And you said that Date mesh denies not entirely centralized, store centralized  dvh team, that is, there anyway, centralized rather Yes, it’s not explicitly stated here, this is about centralization of the platform Yes, there should be a common one. Moreover, if the data platform consists of a large number of data storage tools, then from the point of view of storage, this is a decentralized Well, that’s exactly what I wanted to bring to this point, that is, it would be very difficult for me, let’s say, as a user to sit and figure out from which system this domain I need to pick out And then how to make these data friends with each other, so it would be much more convenient if I have  there was one acceptable SQL interface to a 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=526)~~  shared storage where Let Okay everyone Let him be responsible for his own data, he owns it, checks it, and so on, but I would like to see them in one place to have one access point to all this corporate data here Yes, you can do it, it’s more about organizational administrative-technical approach That is, it does not deny that you have a single interface, you can have, for example, one in your Green Plan in which the data is simply organized, it can be either separate schemes, separate catalogs, separate databases, or whatever you want, you can organize it but with  from the user's point of view, a single and certainly correct question is available. How can you make data in different domains interoperable in the sense that you can communicate with them using a unified approach? proposed to organize a federated Data Gaurmus team. Well, that is, these are representatives from each Data domain that  somehow in this federative body they vote on how to correctly work with data, how to name them correctly, how to correctly 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=606)~~  fold signs, and so on, respectively, if we have such a Federative body that has power over the commands of data domains, it can ensure interdeborability or the sameness of data in all  domains The next question Evgeniy, please tell me how you force commands that have never seen any dates in their lives; the tools cannot be even worked with the SQL language. How do you force them to load data somewhere where they told you you need to load it, but here the approach is the following  the concept itself between data domains is divided into two types, they are two types in the original article and three types in the book if you read Jama where Hani is not the essence. Let it be two, as in the original articles there are domains that are close to the data source to a service, a microservice, some kind of conservation  and domains that are close to consumers, to analysts, to business users, and so on, respectively, in an organization, analysts will most likely be responsible for consumer domains. And the Bacon team will study the domain source, and in 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=692)~~  this fork, if you look at analysts, I believe that any analyst, even Johnny Rooney, even  the intern must be able to write a query. Well, that is, if an analyst does not know how to query, then most likely this is an analyst who is worth working with and teaching him how to write a query; if an analyst knows how to write queries, then he has a super simple threshold for entering dbt dbt is a framework that you can write simply  by bending the request in some way write simply  by bending the request in some way to mark what type of materialization needs incremental recalculation and so on, but in any case, just a request if we are talking about Back and command, then my personal position here is that a content engineer who creates highly loaded services can easily and quickly understand how the primary data collection and primary data processing work There is nothing complicated there, usually bringing someone to a flat tabular form, converting types, something acceptable for those especially with whom we work, anyone can 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=765)~~  figure it out in just a week, take any I think  I think a week, taking into account the fact that he will build over this RM, here the date of the platform should provide tools so that it is not difficult and here is a slice of the experience that Stalka and we have covered everything with dbt macros, that is, loading a new message. This is literally to describe about that the diagram of that  format that you are sending. Well, my question was how will people do this in general who have never done it? Your answer to this is that it’s easy for people to figure it out. then spend time servicing there somehow, if something changes in your microservices, then the output file changes, at some point its structure changes, or new fields appear or old ones go away, they this is how they do it  since the problem that breathing was trying to solve comes up with, if we have a centralized dvh team, then Back and changes the types of messages, changes the set of fields that are inside and dvh team She always lags behind these 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=855)~~  changes that need to be somehow administratively agreed upon, that before you change, agree with the team  dvh and there are a lot of such organizational barriers a proposal that goes like the core Yes, this is inter Why do this? Let's give the tools for working with data to the BC team, she changed the message of the characters, she will change the way the message looks in analytical systems, it sounds super sensible, all that remains is to make sure that these changes  were simple from the point of view of the backend. This is how it is implemented in the ceiling and it is enough to make backwards compatible changes in a simple schema, that is, just add fields to this simple schema or if we change the type of a field so that this type changes to a wider one. Correctly, I understand that in such a schema  we do not take data from sources, but send data from sources to the storage, that is, we actually shift responsibility for how exactly this or that object will be stored in our storage [music] I agree with the first with the first part of 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=938)~~  the statement that in any such approach Back  and itself sends the data to the storage We have some kind of standard path, it goes through the enhub or through kavka and the format of the message is quite strictly regulated, it should be described through this scheme, but the second part What distinguishes how this message will be decomposed is rather  I don’t agree because the complexity of the message layout, the flat view chooses in itself, will give the platform if literally then Yes, it’s just a scheme that delivers along with the data, it’s true that the scheme is the basis for how we will decompose the message into a tabular form how we will decompose the message into a tabular form and about this scheme it also sends to Kafka or  about the scheme in advance the scheme sends the scheme it is in github Well, that is, this is as part of our contract with the backend, so here the next question arises I just Well, I see a lot of problems that had to be solved, agreed upon and settled within the company in order to fix all this question 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=1021)~~  Next Here is  you have some kind of backend there are microservices that let's say Let's sell something simple ice cream in the store We have an assortment of ice cream there there are clients who buy accordingly Back and which is responsible for the cash register equipment sends data about what was bought and who bought this scheme everything is fine  they described their proto-meta scheme of their data, they sent it to Kafka, everything was sorted out, you loaded it. By the way, where do you then and without the anchor model, just flat plates? Yeah, well, that is, everyone there has their own scheme, or how it’s organized, I’ll now explain Why I  I ask these questions in the same scheme with an ice cream stall. Yes, we also have suppliers who, for example, bring ice cream. I don’t know what types of ice cream there are. Snickers ice cream is acceptable and only that and they have some assortment of this ice cream. That is, in principle, the essence type  ice cream or the type of product, it matches But for the supplier who 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=1109)~~  supplies Snickers These are only the articles defined there, but for the cash register equipment of its backend there can be all the variety How to understand Is it possible there Let’s take it and simply Join us to join the supplier data with the cash register data like  this is resolved during analytical queries already because it seems to be the same ice cream, but there is only a limited assortment in the data of this supplier. This is a good question, I think this is a general problem of the inter-approach data. And in fact, the problem of any microservice approach that exists, for example, in the world of bi  Kanda, when we divide our Monolith in which everything is clear into a set of microservices Well, in any case, we get a certain kind of mess as microservices interact with each other, like for example Roma events to each other, the same thing happens in Date mesh because we have many small domains These can be  not all the data and it seems to me that the only solution here is a more or less adequate description of what 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=1180)~~  lies in the data. That is, it becomes critically important for us to have a catalog or body that allows you to find the necessary interesting. Well, how should I describe then data so that the analyst is easy to do and easy to find what he is looking for there are good Maxims How to correctly this is exactly the maximum Because I have not seen a single description that would correspond to this definition But this is but it seems to me that Dina very correctly noted that the whole idea in the end is that it is necessary to remove responsibility from the data team and shift the product columns to microservices and, as it were, because the problems that ultimately remain are, in principle, similar to those that existed before everything The only thing  what we can conditionally The only thing we can conditionally say that we have achieved something good is that potentially the number of data sources there or processes and other things becomes easily horizontally scalable, but this is a very loud statement, in fact, someone 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=1286)~~  should from the side of these microservices understand what he does, what kind of data he provides and accompany him simply in another place, now everything is correct. This is literally shifting responsibility from the Central dvhi team to one team of analysts or to the kand team. But this is clearly explained by the fact that the dvhi team, if it is centralized, is supplied with data from  backend teams backenda who are not interested in the data rate they are interested primarily in the fact that the backend works what are the analysts How the data will be delivered there it doesn’t matter it’s for me to do the dvh team This is already unpleasant, no longer Super cool for the dvh team because some administrative ones come barriers to overcome But if on the other hand you look at the team of analysts, they understand why they need to analyze the data Why do they need to build a report link if they do it They don’t give the command to dvh themselves, then here the dvh team is again cut off from the business, they make showcases for some 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=1354)~~  analysts who, in general  - super far from them, they may not even take any obvious part in this cycle, and the question arises: why keep such a two-way team of fairly high-standing alter people who, in general, burn out; they do work that is not needed either on the left or  on the right, neither the backend nor the analyst are involved in the wrong way Let's distribute this work to the backend and the analyst to those who are closer to these tasks, closer to these issues from the engineers will do exactly what they are paid for building a data platform what they are paid for building a data platform analysts need some data Yes they have some task to calculate something, make some kind of model, and they even know which backenders can be used. Well, that is, which backend teams can get this data, they figured out the issue, everything is fine, they come, and they say Well, we have now  It’s not a season, or rather, on the contrary, it’s a season, we hold it with both hands and will bring you the data in two and a half months, 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=1431)~~  and analysts need to calculate today and this is how this issue is resolved. I’m just very interested, it’s clear that this is not a question of engineers, in this case it’s a question of interaction, but nevertheless, there are many back-end teams and each of them maintains their own Pay plans, their own downloads, someone there, half of the people fled or went on vacation, in fact, this is a cool question, it should be considered a little at a higher level Why do analysts do Most likely, they do this reporting so that the product manager or owner of the product, no matter the name in the company, can understand how to properly develop this product, and here if a situation arises that an analyst from the analysts requires new reporting to the analyst and the bknd team cannot actually provide here the right hand does not understand that with the left hand with the left foot the product that manages the teams and manages the product has ordered analysts of some kind even on board of some kind A request that cannot find an answer 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=1507)~~  analysts came to his own product team his own product team cannot provide these analysts with Data  there is simply a complete analysis of the hesitation at the campaign level, simply a complete analysis of the hesitation at the campaign level, if the analysts do this reporting for the ox power there at a Super high level, then it seems that the backend team should postpone their current requests and help the analysts do some kind of unloading for the sevel in general, what you are describing is what you are describing  this is a complete breakdown in the operational and business processes within the company or the analysts are doing something wrong, they are trying to provide reporting in a place where it is not needed or the product is not managed by the team and cannot direct the backend team to help the analyst or everything is fine but simple  there are not enough people there in this backend who are now going to rush straight in to fix something. I understand Dina’s question very well, we have payments there. Sorry, they cost Yes, and we 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=1584)~~  can’t, we will now stop receiving money. And you say that this is reporting for me  it seems to me that no matter what kind of reporting is needed for the forces, first of all, some back-and-services will process their backlogs and correct their mistakes And tomorrow our contract with the payment provider ends, we will switch to another And the day after tomorrow we  us something else, this is endless, maybe the story is this signal that there are not enough benders. Well, in the sense that if you had a kender always on fire, abcander should not load and the microservices given to you should unload the microservices should do this. The key question that arises in micelloid this is wrong, I should be responsible for the data that his service does, this is the same thing that I don’t know, I’m not responsible for the mess on my desktop Let the cleaning lady come every day to clean up the mess on the table, so you can live, in principle, you can work like that, but I’m responsible for my desktop  I’m doing the same here, but for some reason in the world of 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=1651)~~  data there is such a situation that there is a desktop there is a backend that sits on it, but for the order with the data and with things on the desktop, part of the exit command. things on the desktop, part of the exit command. This is because there is such a division of responsibility for the backend. Maybe there even there are enough people, but in some High season, well, there are sales, something else there, a marketing company came and brought in a bunch of people and there is an urgent need to scale up the service in a good company, there are always more people than people. Well, it doesn’t happen that people sit around bored and think Whatever  we still have work to do if this happens like this, it means the company is not developing, it means these people are not needed But the same can be said, data analysis is obviously in second place after we finally get it done, but not delivery Yes, you can say the same thing about tests, that is, I would be a canter  I’m writing a micro service Let the test be written by some people 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=1724)~~  tests literally Well, some are written by different companies in different ways But there are tests that Yes, they test separately there but there is standard practice when you write test drive development for example when you write tests before what  - to develop there are good practices backender writes tests himself, but there is another option, let’s say the tests are written for me by another person Let another person write the documentation for me, for example, we hire technical specialists and a cat It is also preferable that gpt writes a cat for me Why is it needed? That is, it seems that the backender and the team product it should be responsible for the entire cycle around and analytics it should be an integral part of product development the product manager should understand that there is a backend team and this team needs to send some messages a data analysis system for analysts and analytics products should already sell it to the manager bring how develop your product, some numbers, some metrics, this should all be an 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=1795)~~  unbroken cycle, there should be separate teams that compete with each other, this is exactly what it promotes in my understanding. Listen, it sounds very cool, but really, it’s probably time to return to the topic of your report for the sake of beating about airflow how it all works and what you difficult to answer this question in fact this question is about my 40-minute report trying to answer briefly we took dbt we didn’t buy dbt Cloud because it was quite expensive we took dbt and integrated it with airflow  we already had airflow, there were dugs on it, we wanted convenience, comfort and the functionality that dbt carries. Connect with airflow soul is actually a standard de facto code for Rivendell platforms. And if we take any other tool that is in modernet, it but can I go back  I'm pumping out, I'm sorry that I'm taking us a little off the topic of your report, but how about the idea of ​​the date mesh conditionally? Let's assume that you sold it to the manager who is listening to us, here's the idea of using dbt Not yet 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=1889)~~  Well dbt Maybe like the letter T What is the letter T for me in itil  or LT is the ability to somehow transform data, load it somewhere into other objects in the same database, I have SQL for this, tell me about you, I still don’t understand why I need dbt, I can transform data using Skillet types can be laid out there Jason and it’s easier to do all this, why do I need this great? You have SQL which you can use and do whatever you want with the data. But in any case, you have such trivial but quite pressing questions, but how can I just use Lineage  what what depends on me what why run Or for example how to test my data Can I somehow control the data code here dbt on the one hand allows them to write in SQL and from version 1.4 in my opinion in Python by the way Diverging slightly from your question, the feeding models are made extremely crudely and so far they cannot improve them, it seems not only that no one uses them except us, but returning to the squel, we simply write queries about how the migration will take place, how the data will be inserted, 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=1977)~~  how they depend on each other  friend like us Testing this data hides everything in itself dbt as such a platform for describing and but how they will be tested and how they are friends with each other and so on this is not SQL this is a skill but in nine instead of not that place Skeleton like  as an extension of Skill, Ginger templating is used and this is actually a plus minus the minus in the sense that it is quite difficult to test to describe the plus is that you can use certain capabilities of jeans to somehow automate queries instead of the From construction in the sign in the debutita you indicate From and the ref reference macro  to some other model Thanks to the use of such macros, dbt, builds a graph of the dependencies of your models or objects among themselves in general 9 models such a cube of nine is minimal which consists pip file and a hole faster in a share file we describe the logic How this the object is being built. This is simply a Select request or a dataframe in the pays-park, which we return in Yamla. We describe 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=2065)~~  some meta-information of our object, a description of the plate, a description of the fields and some expectations from the composition of the fields. the expectation is tests, for example, we expect that the field is unique or we expect  that the field does not contain cash or we expect that the field is a number in such a range there are a lot of extensions in dbt of all kinds of tests But in any case it looks like Expectation and by describing such bricks using the dbt Build or dbt campile command we set up the dbt project and already generate queries  after Ginger templates that will fly into our target DBMS and here the big advantage of dbt In my opinion is that all the difficulties of inserting all the difficulties of datal Lineage all the difficulties of data testing are easily hidden within itself so easily that the team of analysts who may not be immersed in the data engineering and the backend team, which is generally far from all this, both of these teams can easily understand how to again, a loud statement that can be 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=2149)~~  hung on the wall. Zhenya tell the problem of nine to you, you’re trying to sell it. And I’m trying to persuade our managers, the same ones who are listening  don’t buy, don’t use the DVD, maybe it’s not worth buying at all, on the other hand, but at least don’t use it, but firstly, analytics, analytics, we assume that they even mentioned it. It’s obvious that they are building ours and television processes there, etc. As I understand it, they use dbt  and they build models in 9, but if yesterday an analyst could just take his knowledge of SQL and go write that he needs to create new objects in the database and so on, then today he cannot do this; he needs some other magic jinja to Study and more  and some kind of dbt and understand how it’s all connected there, that is, well, firstly, I think that this is a big minus, I will be such an opponent of the dbt idea, the second point to what you said is that you want to object to data tests - this is very cool, but the problem is nine  the fact is that all the tests in 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=2221)~~  dbt work on data that you have already loaded, you loaded the data into your burial place and then you run the tests and say Oh, we expected that there would be no catch in this column, we have them, what next? But we have already loaded them and  dbt can’t show us how to do these tests before we upload them somewhere because I will answer these questions Let’s do the first part if the analyst is not ready to take it Study dbt and Jinja which is actually simple Well, not complicated And even if you don’t want to to delve into Jinja, you just need to understand its principle and use its most basic Well, teamplates are the first question If an analyst, if analysts don’t want to develop, then the question is: Why do we need analysts who don’t want to dive in? This is the first thing tomorrow, you’ll say goal, you studied analytics because it’s necessary  for our project We are building good storage facilities, fashionable new ones with shining eyes, that’s all analysts with shining eyes, they have another problem, they will sit and 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=2296)~~  study these goals and instead of loading the data normally and storing it, we are talking about finding out what Ju is  and show it to Novaya and before it actually doesn’t mean learning a new approach to work. You’re just two small tools with each of which you can get acquainted. Well, in an hour. Let’s just do the second one. What about ginji? hour and analysts; I can’t figure out the direct line. vrflow is written by Dags, naturally, also with Jinja. Naturally, they will be happy if they write the names, but it will be necessary to just write sqlge, it seems to me that you need to fire your analysts, this is a we’ll write on the walls, I’ll tell you more, there are simply projects on which analysts  They also don’t know SQL, only they use Excel, also don’t know SQL, only they use Excel, they are definitely analysts, they are excellent, they give business and strength to the levels based on the data they can get using Excel, we say analysts seems to me that any sane person can simply take any person in two weeks if 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=2382)~~  in two weeks  I personally won’t be able to tell him what SQL is I don’t know I’ll leave the cleanup I’ll go somewhere to a Tibetan monastery and I’ll be there for 10 years thinking about the second part of your question This is not a problem with the letter T, it’s a problem with the letter L, that is, that’s what you need  data has arrived that does not satisfy the contract that you, well, which you agreed upon, means in the first part the problem is not in the dbt part, wait a question arises, we say that tests in dbt are cool, they allow you to find a lot of problems with the data, or it’s convenient to find these problems to look for if we are talking about  that the data is already good and satisfies the contract and came this way, why do we need tests then, but do we still need this, plus or not, that the two tables get along normally with each other, that they do not have any data transfer, that it is possible from one source is equal to the second  Figures from another source, but We can find out this using D5 only when 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=2461)~~  we have already loaded this data. Of course, this data at the moment when you load it means loading and checking what’s wrong with the data Everything is ok, it’s not  implies that verifying data from different sources, for example, we are looking at where our columns went, is everything loaded, is it possible, how does the total number of today’s lines relate to the quantity regarding the tests, I can say how we did in the taxi, we had three types of tests First  tests that compare what we all received from the source in terms of quantity and composition, that is, we are identical to what is in the source, the second test is that the input to the storage is given coincides with the output, for example, there was a million trips, the output is also a million trips and the third type of tests is that  our outputs coincide with each other. That is, for example, two reports that make cuts with a similar number of trips or the amount of money there should also coincide, respectively, inside the storage there are almost 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=2542)~~  no tests, we check that the input is what we received, the input coincides with the output, the output coincides with each other three set of tests OK Okay, let's move on We've convinced you for now We thought you were going to tell me that dbt has some kind of underlying problem, it doesn't work well, it does n't hold the load and confuses the models. I'm not yet trying to understand why use a tool if it doesn't solve existing problems, and here are the problems it solves  Zhenya says he adds tests to tests does not show that the main problems you have will be solved by HR of a good company by firing me, I agree. This solution can solve many problems, at least for me, Okay D5 adds an additional level, you can say abstraction, enriches Plane sequal with something  additional jinge and something else everything is fine Why do we need to integrate it now you just said that dbt has models We can use ref macros to combine models with each other dbt will build it for us The graph will build essentially Dag which we need to execute why here 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=2632)~~  airflow in terms of integration, well, the maximum I can understand is that you can run dbt, how much deeper do you need to play the question that we faced about a year ago when we were implementing dbt, we not only have no money, we have no desire to pay for nine Cloud that runs dbt here  We also have the nine models described Zhenya If you all didn’t buy unfortunately, there, that is, we pay yes from the engineers so that they chip in on the tools, unfortunately, unfortunately, there is too much price spread between these and they are constantly increasing  prices let's return to the word here we have we have dbt in dbt you can run the model through the tyrant type We have airflow which can also SQL with jinja can run the same thing for now Let's combine airflow and dbt and  this idea is on one side of the surface and we spied it in astronomy astronomer is the Flow manager If I’m not mistaken they offer the following next integration at the time of compilation dbt spit out a json manifest a giant Jason of files in which there is a 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=2728)~~  dependency between different models in fact this is the Graph that we want to get  VR Flow Let's take this file in airflow and generate our dags based on it then wait a minute, is it possible or can I clarify at this point And you are telling how to do dbt Air Flow integration And I want to understand Why do it Why dbt itself builds degradation all Dak dependencies  knows what needs to be launched after what and everything that on the airflow side, in theory we need to do launch nine runs of the entire project. The answer to this question is quite simple from my point of view airflow allows you to integrate with almost anything if you take any tool from the mods of the Mont Carl stack David Det  anything mods of the Mont Carl stack David Det  anything will most likely be a Flow connector go away every quarter, a new killer comes out every quarter it doesn’t come out and every next quarter they don’t die Air Flow is still alive and can still be Now let’s possibly take advantage of the possibilities for integration With anything 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=2813)~~  just put it on  dbt To understand Which model was formed when Which test failed, how to restart How to forget that is, we use all the possibilities but Okay That is, if I understand the idea correctly instead of giving everything to dbt management itself What was created inside our dbt project and debit  knows what it generates in these manifests and other files; instead, we take it all apart and manage each individual model from the airflow side, and we need this in order to conveniently see it all VI and, if necessary, restart from some point in the wrong place turning to yourself knowing a little debut, the next question arises. If something crashes in 9, then dbt stores the state of the last launch, you can save it somewhere and ask 9 to launch the ask 9 to launch the broken models after we have done something, fixed something, and this can  again do dbt itself, that is, the whole idea for now is to see Everything, not only T, not only more precisely, and part of LT and see all the models separately and build Lineage 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=2907)~~  correctly, I understood correctly, we can also integrate all the capabilities of nine for restarting models into airflow or reuse them for in this regard, we leave the brain or the compilation of dag graphs to dbt but ua and directly transfer the launch via cron to airflow between them, such a symbiosis arises and of course there are disadvantages here in the sense that we create additional integration or an additional library that I  I really hope we will start sourcing by the end of the year. On the one hand, on the other hand, we are using the advantages of Air Flow in terms of integration, a convenient UI and the ability to add your own dugs that are not a sequel to either dbt and we are using the capabilities of dbt Okay, before I start again - It’s a nasty question to ask. Please tell me what you did for integration. So you started with the airflow meter and the astronomer. I think they have their own operators for airflow that they wrote and I’ve already seen Open Source for a couple of things that essentially 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=2988)~~  also parse manifests and  they build it like this inside Air Flow Why do we need our own at the time when we studied this issue? This was about a year ago except for an astronomer and enough of this Well, not for the knee Well, I had nothing written in the code, there was nothing we couldn’t find a suitable tool for us I  I would say so, we couldn’t find anything except this integration, which is described in astronomy and dbt itself refers, in general, to this article number How to integrate correctly debuted, we went a little further than the astronomer itself, we don’t generate one big Duck that loads weight 9 with us  on the one hand, there is a Data inter-approach that all our data domains share, and on the other hand, each one is triggered by its own frequency; in general, each model can be considered with its own frequency, and then we have a map of the product between all domains and all possible typings  We have them regulated 15 minutes hourly daily weekly and monthly by monthly no one uses a product map between domains and 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=3067)~~  this sharing is our dag they start with different frequencies and they sounds good Where do you store dbt in general projects 9 models this  everything lies with Air Flow or somewhere on the side it lies in Github such one single dbt project this is a separate question for research was when we tried to implement dbt a year ago there are different approaches brought from project 9 you can make one centralized one this is very much what is recommended there  dbt there are other options You can, for example, do a layer-by-layer breakdown of this individual storefront separately, you can do it in a microservice and It would seem that this is an ideal approach for us, you can do it mixed, but there were problems and they are actually still not solved if we do a microservice approach, for example, each domain is  a separate project, before version 1.6 we had to name models differently in different domains. That is, if one domain uses another domain or one nine-project imports another dbt-project, the model names should not 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=3151)~~  intersect within these models, which is extremely difficult to actually do  if we have a disparate organization once Homeland github projects end-to-end testing celebrating projects it's difficult actually the problem is no longer the problem with the naming fixed in version 1.6 It came out to divide three ago and we should have had it in the news release The problem was that  9 projects cannot import each other cyclically That is, I am a domain I want to import information from domain two domain 2 wants to change import What cyclic dependency does it destroy dbt and in my opinion this is still an unsolved problem we can not use refs as recommended to use dbt  sars, that is, not to build a full Lineage, but we are losing most of what Lineage 9 provides all around. As a result, we have one monolithic project, it lies on GitHub, different domain teams live in this monorep, they simply put them in separate folders and, accordingly, our Seya process when we release  collecting dbt collecting the manifest Jason and all the files that 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=3226)~~  are in the target and fed into our libraries that generate dougs in Lowe she just reads Now the most interesting the most interesting has begun Okay That is, you have a separate monolithic dbt project because sharing under the exchange is not very convenient  it has its own problems, well this nine project lies in the line Turnip from Air Flow correctly Yes, everything is correct, dbt lives on its own, dbt lives on its own, you have generated a manifest, you have generated a manifest perfectly using the utility, there are libraries, they disassembled it, they built VR Flow ducky, they haven’t built anything yet  it’s not clear how to divide Well, for me personally, it’s still not clear how to divide specific models into dugs using the name. A specific model is divided into dugs through not tags, but through the 9th organization of the project in nine projects, the models are in the Models folder, we added a hierarchy inside, first the data domain then the layer  data then the model itself, and this is at the assembly or compilation stage 9 of the project, the 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=3308)~~  Jason manifest sprouts where exactly the model is located, understanding in which domain this model is located next, depending on in which domain this we launched it Even we have worked out the models, let’s imagine they worked out all the ideal situations well and everything is green, excellent data is in place, the users are happy tomorrow someone went and added or changed some model that we had in the 9th project and the manifest was generated differently. This means that Dak will be generated for the same manifesto another with a different set of tasks VR Flow What to do with it Yesterday there were five tasks in launch yesterday And today there are three how can I launch yesterday’s task and I don’t see it today Flow it works out such changes quite well That is, if we have new ones  tasks from the moment they appeared they will be launched if you need to run them for yesterday the day before yesterday there is a backfill that is present you can run without problems to backfill the data for this longing which was not there before now it has appeared 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=3393)~~  Yes, but the problem is that the model is gone now it is not in the manifest  and airflow doesn’t see it doesn’t generate this task in this dag How can I restart it from yesterday if you deleted the model then of course it’s not there But you don’t need to restart it even then if you moved the model it needs to be restarted from yesterday I deleted it today because yesterday’s data is crooked went to bed I want to restart yesterday's model, we have such an opportunity, we do not delete the model, we respectively. Then you can restart for the range of Dates when it worked for you And when it stopped working You want to turn it off but leave me this description leave  code and so on It will no longer work for these dates and it’s not difficult to do it through brunchperators for the group, it’s rule of good manners not to delete models If you want to restart them for yesterday if you deleted the model and in principle does not exist That is, if you deleted the model I would  along the way, we need to delete the 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=3468)~~  Okay, the model remains, but the model is now being built. Dan is building, transforming the data and performing its functions differently. Yesterday we converted this field to a numeric type. And from today we started loading it into the text field. I want to restart it yesterday and the state of this model today is no longer the same Yes, here D5 right now offers a version of the nerves of not models, but we haven’t used it since version 1.5 appeared if we talk about the solution that we have dbt will try to convert the field from a number to text at the moment when we converted the number to  in the text, the field is a more general design, then the Russians will work and we have all the fields, a bad example was Zhen, let me be different, it’s better not to focus on it for a long time, the model has changed in such a way that the field you have remains The same output field in the table as it was numeric like this  and it remains, but the way we collect it has changed Select, which in our model is nine times broken, it has changed Why 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=3545)~~  why am I doing all this And why but the business logic has changed But if it has changed today, it would be good to rewrite it for everything, leave it For all previous A  Yesterday it worked correctly And how is it correct yesterday Well, what kind of situation is that if a little more specific Well, let’s say, but it could be for example Show Let’s say we counted there in one way before January, after January in another way, so it turns out that we need to use the same branch operator and versioning  we are doing it right now in the following way: if before January we calculated in one way from January in another way and okay, we changed the calculation, the only thing is that if we forgot the film for the period Before January, then we will recalculate it in a new way that there should not be another option. We can with using macro jingle, right in the select in nine, write that if the date is less than the length of the number, then calculate this way, if it is greater, then calculate it differently. Honestly, we don’t 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=3609)~~  need analysts, but in fact, For me personally, after hearing what you just said about that  what we can do in jinge, depending on what kind of logical Date VR Flow is running to the limit now, and with the help of jinge we can put just this, this is good for me. How would such a bun be obtained from dbt if you implement it? on the project, that is, I can use this to regulate how I will load this data and, accordingly, backfill and everything else. The only thing I don’t like about Oh, yes, I forgot to say I sold it, I sold another part of our debut listeners now too The only thing I don’t like  the fact that everything is in files but I don’t like it And in airflow too, here’s how to make a model I need to write I need to create a file I don’t know what to do with it excellent So we talked And what alternatives are there Just curious This is a good one  question a year ago when we were studying dbt I didn’t see any alternatives, in fact there is an alternative inside Yandex, the dmp platform, which I haven’t ever talked about on the basis of which we 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=3694)~~  did khan hama there and so on, that’s the dumpish platform that is in Yandex, it’s super similar to  dbt but the difference is that the dmp platform is developed by 30 people and the debit is developed by 100-150 people at least not counting everyone else, so the speeds are different because of this dbt is cool I think we should take 9 in the sense that it’s not a treasure trove of money for you  let's leave it on nespress And if I'm not familiar with the money platform, can you tell me in a nutshell what it is? You can watch Vladimir Verstoy's reports You can watch Vladimir Verstoy's reports in different places On the same Smart date that was online today if we went yes we need a time machine to find out if briefly tmp  this is the same dbt, but instead of Yamniks, py-files are used, that is, instead of Yamal, a description of the documentation for a specific tablet or object is used py-files with heirs, here is Green pumptable with a description of the fields, the concept is the same, you describe the tablet, you describe the loader  somehow the 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=3783)~~  tablet is formed, the rest of the complexity is how the data is inserted, how it does not depend on each other, how they move, they all go inside the Dimpe framework, for me these are super similar things, so when we transferred taxi teams from taxi to cleanup, the transition was quite easy because there were nine  very similar to what we did before in dump, why didn’t I do the same thing in cleanup why didn’t I do the same thing in cleanup [music] dbt was better, it turned out to be simpler or I wanted to feel the new tool, it’s clean, it’s the same startup, that is, it’s not clean  has all the power of the big Yandex to create its own products so and so If we had thousands of 2000 developers Why not write your own Or is there not your own Yes there is Brix to write when you this is a rather complex question happening in the world different, so you can’t use the Yandex infrastructure, you need to use the infrastructure that is generally available in the world, so we don’t have those tools that weren’t sorted in one gulp. By the way, drink, for 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=3869)~~  example, we sorted it recently, but later than we moved to the general infrastructure. So, something like this, listen, it’s really interesting  I still don’t know how Alexa sold me dbt, sold sold the last question that one of the hosts of the coffee date podcast likes to ask at the end of episodes, the question is, what if I’m the only listener of our podcast And I decided to figure out dbt and how to do it integrate with airflow And in general, here's what to delve into all this, what needs to be done, what to in fact, you can listen to our podcast and if this was not enough, then the dbt documentation is actually super cool dbt documentation is one of probably the few documentation that I could recommend as italon documentation there  a lot is described It is relevant it explains what you need there are tutorials directly documentation dbt is almost my ideal in addition to documentation 9 you can watch a report on smart type about the integration of BTR Flow which was told by a handicraft worker from the debt platform Evgeniy 
 - ~~[▶](https://www.youtube.com/watch?v=YZhaZCLresQ&t=3959)~~  Ermakov the name is a very good report he did it himself told I advise everyone in the user group dbt, in fact, the Russian-speaking group is quite large. There you can ask for advice, ask around, read articles on Hambra, and in fact, medium foreign sources, a lot of companies use dbt, they are implementing them. Therefore, there is a lot of information on 9. implementing them. Therefore, there is a lot of information on 9. Excellent, you know what to do guys, well, we are on  Today we’ll probably wrap it up Thank you very much I hope they are very tricky questions everyone and we’ll hear from you in a week bye