---
title: 'Ideal document length and special character handling · Issue #540 · MaartenGr/BERTopic
  · GitHub'
date: 2023-09-25
src_link: https://www.notion.so/Ideal-document-length-and-special-character-handling-Issue-540-MaartenGr-BERTopic-GitHub-985a48da06fe4cecb955958231b6e389
src_date: '2023-09-25 17:55:00'
gold_link: https://github.com/MaartenGr/BERTopic/issues/540
gold_link_hash: cdb4063baa5c51f48820bda3deeb3ca0
tags:
- '#host_github_com'
---

 Thanks so much for this great tool! It is such a huge step-up from LDA topic modeling.  Thank you for your kind words!  1.) I was wondering fo there are any other downsides to working with long documents apart from the fact that it is highly computationally expensive? And if so, what would be the ideal length for a document?  The main downside with working with longer documents is that information will be ignored if the documents are too long. Some models only take in a maximum of 384 tokens and anything longer than that will be cut off. The base model, `all-MiniLM-L6-v2` works quite well if either sentences or paragraphs are being fed to the model. So converting long documents to sentences and/or paragraphs is preferred.  2.) I am concerned that if I split up the longer documents into sentences I will obtain less coherent topics or will lose some meaning. Is this the case?  I actually think you will get much more coherent topics by splitting them up into sentences. Imagine that you pass in a long document that you are convinced contains several different topics. Since BERTopic follows essentially a clustering approach, it will assign only a single topic to that document which should contain several topics. By splitting it up into sentences, we can assume that each sentence contains a single topic so a document, containing multiple sentences, may contain multiple topics. As a result, the topic representation is likely to be more accurate.  3.) What about emojis? Does BERTopic consider them or should one 'demojize' the text first (i.e., replace unicodes with their verbal meanings?  Interesting question, the base model, `all-MiniLM-L6-v2`, is trained on Wikipedia data which contains little emojis. So I do not think the model will have encoded much of the semantic meaning. You could replace them with their verbal meanings but you will be missing some of the contexts in which the emoji were found. Emojis can also be highly contextual, so you might be encoding them incorrectly by replacing them with their verbal meaning. I would start by not doing anything with the text first and see what the results look like. I should note that BERTopic does very slight preprocessing if `language="english"` and if you have not set an `embedding_model`:  [BERTopic/bertopic/\_bertopic.py](https://github.com/MaartenGr/BERTopic/blob/407fd4fdf2e05e80019c1c217972bf3314a41040/bertopic/_bertopic.py#L1887)    Line 1887  in  [407fd4f](/MaartenGr/BERTopic/commit/407fd4fdf2e05e80019c1c217972bf3314a41040)   | cleaned\_documents = [re.sub(r'[^A-Za-z0-9 ]+', '', doc) for doc in cleaned\_documents] | | --- |  It might be worthwhile to set `embedding_model=SentenceTransformer("all-MiniLM-L6-v2")` in order to prevent the preprocessing steps above so that emoji will be put in the topic representation. |