---
aliases:
- https://github.com/MaartenGr/BERTopic/issues/897#issuecomment-1374387291
title: 'ValueError: empty vocabulary; perhaps the documents only contain stop word
  · Issue #897 · MaartenGr/BERTopic · GitHub'
date: 2023-09-25
src_link: https://www.notion.so/ValueError-empty-vocabulary-perhaps-the-documents-only-contain-stop-word-Issue-897-MaartenGr--7e4d25bb99b14522b5d2a86cfe8e6943
src_date: '2023-09-25 18:16:00'
gold_link: https://github.com/MaartenGr/BERTopic/issues/897#issuecomment-1374387291
gold_link_hash: 27f1521eb86751d898c9423b6d4ec71c
tags:
- '#host_github_com'
---

 Hi! Seems like I have the same issue. **Here is my code**:  ``` from bertopic import BERTopic import pandas as pd from tqdm import tqdm import glob, pandas as pd, numpy as np, re from nltk.corpus import stopwords from nltk.tokenize import word_tokenize from pandas.core.common import random_state  sws = stopwords.words("russian")  df2 = pd.read_csv('my_data.csv')  from nltk import word_tokenize, WordNetLemmatizer  stopwordSet = set(stopwords.words("russian"))   lemma = WordNetLemmatizer()  def cleanup_sentences(sentence):     text = re.sub('[^а-яА-Я]'," ", str(sentence)) # Removing non a-z characters     text = text.lower() # Lowering all text     text = word_tokenize(text, language="russian") # Splitting each word into an element of a list     text = [lemma.lemmatize(word) for word in text if(word) not in stopwordSet] # Lemmatizing words and removing stopwords     text = " ".join(text) # Putting words back into a single string.     return text   df2['text_cleaned'] = df2['text'].apply(cleanup_sentences)  docs3 = df2['text_cleaned']  docs3.reset_index(inplace = True,drop = True)  topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs3)    ```  **And the full error message:** ValueError Traceback (most recent call last)  Input In [12], in <cell line: 2>() 1 topic\_model = BERTopic() ----> 2 topics, probs = topic\_model.fit\_transform(docs3)File ~\anaconda3\lib\site-packages\bertopic\_bertopic.py:355, in BERTopic.fit\_transform(self, documents, embeddings, y)  352 documents = self.\_sort\_mappings\_by\_frequency(documents) 354 # Extract topics by calculating c-TF-IDF --> 355 self.\_extract\_topics(documents) 357 # Reduce topics 358 if self.nr\_topics:File ~\anaconda3\lib\site-packages\bertopic\_bertopic.py:2389, in BERTopic.*extract\_topics(self, documents)*, words = self.*2380 """ Extract topics from the clusters using a class-based TF-IDF**2381**2382 Arguments:**(...)**2386 c\_tf\_idf: The resulting matrix giving a value (importance score) for each word per topic**2387 """**2388 documents\_per\_topic = documents.groupby(['Topic'], as\_index=False).agg({'Document': ' '.join})**-> 2389 self.c\_tf\_idf**c\_tf\_idf(documents\_per\_topic)* = self.\_extract\_words\_per\_topic(words)*2390 self.topic\_representations*  2391 self.\_create\_topic\_vectors()File ~\anaconda3\lib\site-packages\bertopic\_bertopic.py:2519, in BERTopic.\_c\_tf\_idf(self, documents\_per\_topic, fit, partial\_fit)  2517 X = self.vectorizer\_model.partial\_fit(documents).update\_bow(documents) 2518 elif fit: -> 2519 self.vectorizer\_model.fit(documents) 2520 X = self.vectorizer\_model.transform(documents) 2521 else:File ~\anaconda3\lib\site-packages\sklearn\feature\_extraction\text.py:1283, in CountVectorizer.fit(self, raw\_documents, y)  1267 """Learn a vocabulary dictionary of all tokens in the raw documents. 1268 1269 Parameters (...) 1280 Fitted vectorizer. 1281 """ 1282 self.\_warn\_for\_unused\_params() -> 1283 self.fit\_transform(raw\_documents) 1284 return selfFile ~\anaconda3\lib\site-packages\sklearn\feature\_extraction\text.py:1330, in CountVectorizer.fit\_transform(self, raw\_documents, y)  1322 warnings.warn( 1323 "Upper case characters found in" 1324 " vocabulary while 'lowercase'" 1325 " is True. These entries will not" 1326 " be matched with any documents" 1327 ) 1328 break -> 1330 vocabulary, X = self.*count\_vocab(raw\_documents, self.fixed\_vocabulary*) 1332 if self.binary: 1333 X.data.fill(1)File ~\anaconda3\lib\site-packages\sklearn\feature\_extraction\text.py:1220, in CountVectorizer.\_count\_vocab(self, raw\_documents, fixed\_vocab)  1218 vocabulary = dict(vocabulary) 1219 if not vocabulary: -> 1220 raise ValueError( 1221 "empty vocabulary; perhaps the documents only contain stop words" 1222 ) 1224 if indptr[-1] > np.iinfo(np.int32).max: # = 2\*\*31 - 1 1225 if \_IS\_32BIT:ValueError: empty vocabulary; perhaps the documents only contain stop words |